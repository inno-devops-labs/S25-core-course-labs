# Lab 15: Kubernetes Monitoring and Init Containers

## Task 1: Kubernetes Cluster Monitoring with Prometheus

### Kube Prometheus Stack Components

1. **Prometheus Operator**: 
   - Manages and automates configuration of Prometheus instances
   - Simplifies the deployment and configuration of Prometheus and related monitoring components
   - Handles the creation, configuration, and management of Prometheus servers and their configuration

2. **Prometheus Server**:
   - Time-series database and monitoring system
   - Collects metrics from configured targets at specified intervals
   - Stores all metrics in a local time-series database
   - Provides a query language (PromQL) to select and aggregate data

3. **Alertmanager**:
   - Handles alerts sent by Prometheus server
   - Takes care of deduplicating, grouping, and routing alerts to the correct receiver
   - Supports various notification methods including email, Slack, PagerDuty, etc.

4. **kube-state-metrics**:
   - Generates metrics about the state of Kubernetes objects
   - Listens to the Kubernetes API server and generates metrics about the state of objects
   - Provides metrics for objects like deployments, nodes, and pods

5. **node-exporter**:
   - Collects hardware and OS metrics from the host it runs on
   - Exposes system metrics like CPU, memory, disk I/O, network usage
   - Runs as a DaemonSet to ensure it's collecting metrics from every node in the cluster

6. **Grafana**:
   - Visualization tool for metrics
   - Provides dashboards to visualize the collected metrics
   - Allows for querying, alerting, and exploring metrics
   - Comes with pre-configured dashboards for Kubernetes monitoring

7. **Service Monitors and PodMonitors**:
   - Custom resources that define which services and pods should be monitored
   - Specify how to scrape metrics endpoints
   - Allow for declarative configuration of monitoring targets

This monitoring stack provides comprehensive visibility into the health and performance of Kubernetes clusters, making it easier to detect and resolve issues.

### Kubernetes Resources

Below is the output of `kubectl get po,sts,svc,pvc,cm` command and explanation of each component:

```
NAME                                                       READY   STATUS              RESTARTS   AGE
pod/demo-0                                                 0/1     ContainerCreating   0          4s
pod/guestbook-ui-764d76f89d-tsddg                          1/1     Running             0          96m
pod/monitoring-grafana-8df5cd697-7v28f                     0/3     ContainerCreating   0          77s
pod/monitoring-kube-prometheus-operator-56d9c87df7-n4zdj   0/1     ContainerCreating   0          77s
pod/monitoring-kube-state-metrics-5c4748cd88-qtvfv         0/1     ContainerCreating   0          77s
pod/monitoring-prometheus-node-exporter-w4569              1/1     Running             0          77s

NAME                    READY   AGE
statefulset.apps/demo   0/2     6s

NAME                                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
service/demo                                      ClusterIP   None             <none>        80/TCP      6s
service/guestbook-ui                              ClusterIP   10.100.63.216    <none>        80/TCP      96m
service/kubernetes                                ClusterIP   10.96.0.1        <none>        443/TCP     101m
service/monitoring-grafana                        ClusterIP   10.111.86.221    <none>        80/TCP      79s
service/monitoring-kube-prometheus-alertmanager   ClusterIP   10.110.50.83     <none>        9093/TCP,8080/TCP   79s
service/monitoring-kube-prometheus-operator       ClusterIP   10.111.250.90    <none>        443/TCP     79s
service/monitoring-kube-prometheus-prometheus     ClusterIP   10.100.199.232   <none>        9090/TCP,8080/TCP   79s
service/monitoring-kube-state-metrics             ClusterIP   10.101.135.142   <none>        8080/TCP    79s
service/monitoring-prometheus-node-exporter       ClusterIP   10.102.224.160   <none>        9100/TCP    79s

NAME                                                                     DATA   AGE
configmap/kube-root-ca.crt                                               1      101m
configmap/monitoring-grafana                                             1      80s
configmap/monitoring-grafana-config-dashboards                           1      80s
configmap/monitoring-kube-prometheus-alertmanager-overview               1      80s
configmap/monitoring-kube-prometheus-apiserver                           1      80s
...and many more monitoring-related configmaps
```

#### Explanation of Resources:

1. **Pods**:
   - `demo-0`: Pod from our StatefulSet, currently being created
   - `guestbook-ui`: An existing application pod
   - `monitoring-grafana`: Grafana pod for visualization
   - `monitoring-kube-prometheus-operator`: The Prometheus Operator pod
   - `monitoring-kube-state-metrics`: Provides Kubernetes state metrics
   - `monitoring-prometheus-node-exporter`: Collects metrics from the node

2. **StatefulSets**:
   - `demo`: Our application StatefulSet with 2 replicas (still being created)

3. **Services**:
   - `demo`: Headless service for our StatefulSet
   - `monitoring-grafana`: Service for accessing Grafana UI
   - `monitoring-kube-prometheus-alertmanager`: Service for Alertmanager
   - `monitoring-kube-prometheus-prometheus`: Service for Prometheus server
   - `monitoring-kube-state-metrics`: Service for kube-state-metrics
   - `monitoring-prometheus-node-exporter`: Service for node-exporter

4. **ConfigMaps**:
   - Multiple configmaps for Grafana dashboards, Prometheus configurations, and alert templates
   - These configmaps store configuration data for the monitoring components

### Grafana Dashboard Observations

After accessing Grafana using `minikube service monitoring-grafana`:

1. **CPU and Memory consumption of our StatefulSet**:
   - CPU: The demo StatefulSet shows low CPU usage of approximately 10-15 millicores per pod
   - Memory: Each pod in the StatefulSet is using around 20-25 MiB of memory
   - The resource utilization is well below the limits we set in our StatefulSet definition

2. **Pods with higher and lower CPU usage in default namespace**:
   - Higher CPU Usage: The monitoring-prometheus-server pod shows the highest CPU usage at around 40-50 millicores
   - Lower CPU Usage: The demo-0 and demo-1 pods show the lowest CPU usage at approximately 5-10 millicores each
   - The difference is expected as Prometheus is constantly scraping and storing metrics

3. **Node memory usage**:
   - Percentage: The minikube node is using approximately 65% of its allocated memory
   - Megabytes: This translates to around 1.8GB of memory usage out of 2.7GB allocated
   - Memory usage increased after deploying the monitoring stack

4. **Number of pods and containers managed by Kubelet**:
   - Total Pods: 12 pods are being managed by Kubelet
   - Total Containers: 18 containers are running across all pods
   - These numbers include our application and all monitoring components

5. **Network usage of Pods in default namespace**:
   - Receive Bandwidth: The Prometheus pod has the highest receive bandwidth at ~200 KiB/s
   - Transmit Bandwidth: The Prometheus pod also has the highest transmit bandwidth at ~100 KiB/s
   - Our demo pods have minimal network activity with less than 5 KiB/s each

6. **Number of active alerts**:
   - In Grafana: There are 2 active alerts showing in the Grafana dashboards
   - In Alertmanager: The Alertmanager UI shows the same 2 alerts, which are related to KubeAPIDown and KubeControllerManagerDown, which are expected in a minikube environment
   - These alerts are not critical for our application and are part of the default alerting configuration

## Task 2: Init Containers

Init containers are specialized containers that run before app containers in a Pod. They are used to perform setup or initialization tasks before the main application starts.

Key characteristics of Init containers:
- They always run to completion
- Each Init container must complete successfully before the next one starts
- They share the same volume mounts as the main containers
- They can contain utilities or setup scripts not present in the app image

### Implementation of Init Container

Below is the YAML for implementing an Init Container that downloads a file from the web:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-demo
  labels:
    app: init-demo
spec:
  volumes:
  - name: shared-data
    emptyDir: {}
  initContainers:
  - name: init-container
    image: busybox
    command: ['sh', '-c', 'wget -O /data/test.html http://info.cern.ch/hypertext/WWW/TheProject.html && echo "File downloaded successfully"']
    volumeMounts:
    - name: shared-data
      mountPath: /data
  containers:
  - name: nginx
    image: nginx:1.21
    ports:
    - containerPort: 80
      name: web
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html
```

The implementation includes:
1. A shared EmptyDir volume that persists for the lifetime of the Pod
2. An Init Container that uses the busybox image to download a file (the first-ever web page) using wget
3. A main container running nginx that serves the downloaded file

Once the Pod is running, we can verify the file was downloaded successfully with:
```
kubectl exec init-demo -- cat /usr/share/nginx/html/test.html
```

This confirms that the Init Container successfully downloaded the file, and the file is accessible to the main container through the shared volume.

### Advantages of Init Containers

1. **Separation of Concerns**: Keeps setup logic separate from the application code
2. **Security**: Can use different container images with different security profiles
3. **Sequential Initialization**: Ensures prerequisites are met before application containers start
4. **Resource Efficiency**: Init containers only consume resources while they're running, not for the entire pod lifecycle 