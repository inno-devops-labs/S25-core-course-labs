# Lab 14: Kubernetes StatefulSet Exploration

## StatefulSet Implementation

I've implemented a StatefulSet for the Python application with the following key features:

1. **Persistent Storage**: Each pod has its own persistent volume for storing visit counts
2. **Headless Service**: Created for DNS-based pod discovery
3. **Health Monitoring**: Added liveness and readiness probes
4. **Parallel Pod Management**: Configured for parallel pod creation and termination

## Command Outputs

### Kubernetes Resources

```bash
$ kubectl get po,sts,svc,pvc

NAME                                        READY   STATUS    RESTARTS       AGE
pod/nodejs-app-84d8456744-r4pkg             1/1     Running   1 (111m ago)   26h
pod/python-app-0                            2/2     Running   0              3m57s
pod/python-app-1                            2/2     Running   0              3m5s
pod/python-app-2                            2/2     Running   0              107s
pod/vault-0                                 1/1     Running   2 (111m ago)   5d
pod/vault-agent-injector-669f58d9b5-pl2q9   1/1     Running   2 (112m ago)   5d

NAME                          READY   AGE
statefulset.apps/python-app   3/3     4m18s
statefulset.apps/vault        1/1     5d

NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/kubernetes                 ClusterIP   10.96.0.1        <none>        443/TCP             5d
service/nodejs-app                 NodePort    10.111.145.109   <none>        3000:30921/TCP      26h
service/python-app                 ClusterIP   None             <none>        5000/TCP            4m18s
service/python-app-external        NodePort    10.101.110.68    <none>        5000:30012/TCP      4m18s
service/vault                      ClusterIP   10.101.6.255     <none>        8200/TCP,8201/TCP   5d
service/vault-agent-injector-svc   ClusterIP   10.97.93.191     <none>        443/TCP             5d
service/vault-internal             ClusterIP   None             <none>        8200/TCP,8201/TCP   5d

NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/data-python-app-0   Bound    pvc-780fb294-8da2-4b3a-a196-9d917ffcf2d9   1Gi        RWO            standard       5m11s
persistentvolumeclaim/data-python-app-1   Bound    pvc-8fdc4215-d619-42a8-84da-111a9f6e9931   1Gi        RWO            standard       3m5s
persistentvolumeclaim/data-python-app-2   Bound    pvc-fedbfbf3-477a-40a6-9bc5-0ffbfb90fe2f   1Gi        RWO            standard       3m5s
persistentvolumeclaim/data-vault-0        Bound    pvc-0524cdbc-83da-494b-8729-41f43e2b1db1   10Gi       RWO            standard       5d
```

### Accessing the Service

```bash
$ minikube service python-app-external

|-----------|---------------------|-------------|---------------------------|
| NAMESPACE |        NAME         | TARGET PORT |            URL            |
|-----------|---------------------|-------------|---------------------------|
| default   | python-app-external | http/5000   | http://192.168.49.2:30012 |
|-----------|---------------------|-------------|---------------------------|
üèÉ  Starting tunnel for service python-app-external.
|-----------|---------------------|-------------|------------------------|
| NAMESPACE |        NAME         | TARGET PORT |          URL           |
|-----------|---------------------|-------------|------------------------|
| default   | python-app-external |             | http://127.0.0.1:51003 |
|-----------|---------------------|-------------|------------------------|
üéâ  Opening service default/python-app-external in default browser...
‚ùó  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.
^C‚úã  Stopping tunnel for service python-app-external.
```

### Pod File Contents

```bash
$ kubectl exec pod/python-app-0 -- cat /data/visits
4

$ kubectl exec pod/python-app-1 -- cat /data/visits
8

$ kubectl exec pod/python-app-2 -- cat /data/visits
4
```

## Differences Explanation

The visit counts are different across pods because each StatefulSet pod maintains its own state. When a user accesses the service, the request is routed to one of the pods, and that specific pod increments its own visit counter. This demonstrates how StatefulSets maintain individual state for each replica.

## Persistent Storage Validation

After deleting a pod:

```bash
$ kubectl delete pod python-app-0
pod "python-app-0" deleted

$ kubectl get pvc
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-python-app-0   Bound    pvc-780fb294-8da2-4b3a-a196-9d917ffcf2d9   1Gi        RWO            standard       17m
data-python-app-1   Bound    pvc-8fdc4215-d619-42a8-84da-111a9f6e9931   1Gi        RWO            standard       15m
data-python-app-2   Bound    pvc-fedbfbf3-477a-40a6-9bc5-0ffbfb90fe2f   1Gi        RWO            standard       15m
data-vault-0        Bound    pvc-0524cdbc-83da-494b-8729-41f43e2b1db1   10Gi       RWO            standard       5d

$ kubectl exec python-app-0 -- cat /data/visits
4
```

The data persists because the PVC remains intact even when the pod is deleted. When the pod is recreated, it reattaches to the same PVC, preserving the state.

## Headless Service Access

```bash
$ kubectl exec python-app-0 -- nslookup python-app-1.python-app.default.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53


Name:	python-app-1.python-app.default.svc.cluster.local
Address: 10.244.0.121
```

The DNS resolution works because the headless service creates DNS entries for each pod in the format `<pod-name>.<service-name>`. This allows direct communication between pods using stable network identities.

## Monitoring & Alerts

I've added liveness and readiness probes to the StatefulSet:

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
readinessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
```

### How Probes Ensure Pod Health

- **Liveness Probe**: Checks if the application is running. If it fails, Kubernetes restarts the pod.
- **Readiness Probe**: Determines if the pod is ready to receive traffic. If it fails, the pod is removed from service endpoints.

### Why Probes are Critical for Stateful Apps

Probes are especially important for stateful applications because:

1. **Data Integrity**: Ensuring proper shutdown prevents data corruption
2. **Consistent State**: Readiness probes prevent traffic to pods that aren't ready to handle requests
3. **Graceful Recovery**: Allows the application to properly initialize before receiving traffic
4. **Preventing Cascading Failures**: Isolates unhealthy pods to prevent system-wide issues

## Ordering Guarantee and Parallel Operations

### Why Ordering Guarantees Are Unnecessary for This App

Ordering guarantees are unnecessary for our application because:

1. **Independent State**: Each pod maintains its own visit counter independently
2. **No Inter-Pod Dependencies**: Pods don't rely on each other to function
3. **No Initialization Order Requirements**: Pods can start in any order without affecting functionality
4. **No Master-Slave Relationship**: There's no primary/replica relationship between pods

### Implementation of Parallel Pod Management

I've implemented parallel pod management by setting the `podManagementPolicy` to `Parallel` in the StatefulSet configuration:

```yaml
spec:
  podManagementPolicy: Parallel
```

This instructs the StatefulSet controller to launch or terminate all pods in parallel rather than sequentially, which improves deployment and scaling speed for our application since pods don't need to wait for others to be ready.

## Bonus Task: Update Strategies

### Exploring Update Strategies

StatefulSets support two update strategies: `OnDelete` and `RollingUpdate`.

#### OnDelete Update Strategy

With the `OnDelete` strategy, the StatefulSet controller will not automatically update the pods when the StatefulSet's pod template is updated. Instead, users must manually delete pods to cause the controller to create new pods that use the updated template.

```yaml
spec:
  updateStrategy:
    type: OnDelete
```

**Use cases for OnDelete**:

- When you need complete control over the update process
- When you want to update pods one by one at your own pace
- When you need to perform manual verification between pod updates
- For critical stateful applications where automatic updates might be risky

#### RollingUpdate Strategy

With the `RollingUpdate` strategy, the StatefulSet controller will automatically update pods one at a time, in reverse ordinal order, when the pod template is updated.

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 1
```

When a partition is specified, all pods with an ordinal greater than or equal to the partition will be updated when the StatefulSet's pod template is updated. This allows for canary deployments, where you can test the update on a subset of pods before rolling it out to all pods.

**Use cases for RollingUpdate**:

- When you want automated updates with minimal disruption
- When you need to test updates on a subset of pods (using partition)
- For applications that can handle rolling updates without data inconsistency

### Comparison with Deployment Update Strategies

Unlike Deployments, StatefulSets maintain a unique identity for each pod, which is crucial for stateful applications. The key differences in update strategies are:

1. **Pod Identity**: StatefulSets maintain pod identity during updates, while Deployments create entirely new pods
2. **Update Order**: StatefulSets update in reverse ordinal order (highest to lowest), while Deployments can update in any order
3. **Partition Support**: StatefulSets support partitioned updates, which Deployments don't have
4. **Rollback Behavior**: StatefulSet rollbacks must consider persistent storage, while Deployments can rollback more freely

For our application, the RollingUpdate strategy with a partition value is ideal for testing new versions on a subset of pods before rolling out to all pods, ensuring we can catch any issues before they affect all users.

### Practical Demonstration of Update Strategies

To demonstrate the RollingUpdate strategy with a partition, I performed the following steps:

1. First, I scaled our StatefulSet to 3 replicas:

```bash
kubectl scale statefulset python-app --replicas=3
```

2. Then, I configured the StatefulSet to use the RollingUpdate strategy with a partition value of 2:

```bash
kubectl patch statefulset python-app -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":2}}}}'
```

3. Next, I made a change to the pod template by updating an environment variable name:

```bash
kubectl patch statefulset python-app --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/env/0/name", "value": "PYTHON_APP_PASSWORD_NEW"}]'
```

4. I observed that only the pod with ordinal 2 (python-app-2) was updated, while pods with ordinals 0 and 1 remained unchanged:

```bash
$ kubectl get pod python-app-2 -o jsonpath='{.spec.containers[0].env[0].name}'
PYTHON_APP_PASSWORD_NEW
```

This demonstrates how the partition parameter in the RollingUpdate strategy allows for canary deployments, where you can test changes on a subset of pods before rolling them out to the entire StatefulSet. If the changes work as expected, you can gradually decrease the partition value to update more pods until all pods are running the new configuration.
