# Kubernetes StatefulSet

## Implement StatefulSet in Helm Chart

I have renamed `deployment.yml` to `statefulset.yml` and configured values to work with StatefulSet.
Then I deployed my app using helm.

```bash
PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> helm install moscow-time-py ./moscow-time-py
NAME: moscow-time-py
LAST DEPLOYED: Wed Mar 12 22:51:11 2025
NAMESPACE: default  
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=moscow-time-py,app.kubernetes.io/instance=moscow-time-py" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
```

## StatefulSet Exploration and Optimization

### StatefulSet Resources

```bash
PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> kubectl get po,sts,svc,pvc
NAME                   READY   STATUS    RESTARTS   AGE
pod/moscow-time-py-0   1/1     Running   0          63s
pod/moscow-time-py-1   1/1     Running   0          63s

NAME                              READY   AGE
statefulset.apps/moscow-time-py   2/2     63s

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP    3m55s
service/moscow-time-py   ClusterIP   10.106.161.180   <none>        5000/TCP   63s

NAME                                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/py-visits-data-moscow-time-py-0   Bound    pvc-70179ee1-e140-432d-8623-c9451baba06b   512Mi      RWO            standard       <unset>   
              63s
persistentvolumeclaim/py-visits-data-moscow-time-py-1   Bound    pvc-32164d07-d1b8-452d-a895-082dfbbbfc2e   512Mi      RWO            standard       <unset>   
              63s
```

### Persistent Storage Validation

#### Checking Visit Counts Per Pod

```bash
PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> kubectl exec pod/moscow-time-py-0 -- cat data/visits 
104
PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> kubectl exec pod/moscow-time-py-1 -- cat data/visits
97
```

Each pod maintained its own visit count due to separate PVCs. So when I accessed the app, different pods with separate volumes with files of counters were used due to load balancing.

#### Deleting a Pod and Verifying Data Persistence

```bash
PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> kubectl delete pod/moscow-time-py-0
pod "moscow-time-py-0" deleted
PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> kubectl exec pod/moscow-time-py-0 -- cat data/visits
124
PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> kubectl get pvc
NAME                              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
py-visits-data-moscow-time-py-0   Bound    pvc-70179ee1-e140-432d-8623-c9451baba06b   512Mi      RWO            standard       <unset>                 16m     
py-visits-data-moscow-time-py-1   Bound    pvc-32164d07-d1b8-452d-a895-082dfbbbfc2e   512Mi      RWO            standard       <unset>                 16m     
```

The PVC persisted even after deleting the pod, ensuring data durability.

### Headless Service & DNS Resolution

```bash
PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> kubectl exec moscow-time-py-0 -- nslookup moscow-time-py-1.moscow-time-py.default.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10:53

Name:   moscow-time-py-1.moscow-time-py.default.svc.cluster.local
Address: 10.244.0.8


PS E:\Documents\Innop\C3\S2\devops\S25-core-course-labs\k8s> kubectl exec moscow-time-py-1 -- nslookup moscow-time-py-0.moscow-time-py.default.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10:53

Name:   moscow-time-py-0.moscow-time-py.default.svc.cluster.local
Address: 10.244.0.9
```

Pods were able to resolve each other's DNS names, demonstrating stable network identities.

### Monitoring & Alerts

Liveness and Readiness Probes were already added from the previous labs.

```yaml
livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http
```

- **Liveness probes** check if the pod is still alive (by using assigned endpoint and receiving or not the corresponding response of success) and can restart the container if necessary.
- **Readiness probes** ensure that traffic is only sent to healthy pods.
- These probes are critical for stateful apps as they prevent traffic from being routed to failed pods, ensuring stability and consistency.

### Ordering Guarantee and Parallel Operations

For my app, ordering guarantees were unnecessary because:

- Each pod operates independently with its own PVC.
- Thereâ€™s no dependency between Pods during startup or shutdown.

Parallel work of pods was implemented via the following config:

`podManagementPolicy: Parallel`

This allowed me to launch or terminate all Pods in parallel.

## Bonus Task: Update Strategies

Bonus app was analogically configured to work as a stateful set.
Also rolling updates were added to the bonus app.

### Update Strategies Implemented

- **RollingUpdate**:

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 1
```

**OnDelete**:
The OnDelete update strategy for a StatefulSet ensures that pods are not automatically updated when the StatefulSet definition changes. Instead, the user must manually delete and recreate each pod to apply changes.

**Use Cases**:

- When you need full control over the update process.
- For stateful applications where automatic updates could cause data inconsistency or corruption.
- When updates require careful coordination, such as databases that need manual migration or replication adjustments.

**RollingUpdate**:
The RollingUpdate strategy automatically updates pods in a StatefulSet one by one, ensuring there is no downtime. Kubernetes replaces old pods with new ones gradually while maintaining their identity (same hostname, persistent storage, and network identity).

**Use Cases**:

- Ideal for applications that can tolerate incremental updates without data loss.
- Suitable for distributed databases, message brokers, and other stateful services that support online upgrades.
- Ideal for zero-downtime deployments.

Compared to Deployments, StatefulSets ensure ordered updates and persistent storage, making them suitable for databases and stateful apps. StatefulSets focus on maintaining pod identity.While Deployments are best for stateless workloads where pods can be freely replaced, StatefulSets ensure each pod retains its state, making OnDelete and RollingUpdate essential for safe updates.
