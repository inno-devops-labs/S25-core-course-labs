# Lab 14

## Task 2

### `kubectl get po,sts,svc,pvc` output

```bash
$ kubectl get po,sts,svc,pvc
NAME               READY   STATUS    RESTARTS   AGE
pod/app-python-0   1/1     Running   0          12m
pod/app-python-1   1/1     Running   0          12m

NAME                          READY   AGE
statefulset.apps/app-python   2/2     16m

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/app-python   ClusterIP   10.106.35.30     <none>        8081/TCP   12m
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP    50m

NAME                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/app-python-data-app-python-0   Bound    pvc-7b52d56f-4b76-4d59-9d4e-2e359c5a8af4   1Gi        RWO            standard       <unset>                 12m
persistentvolumeclaim/app-python-data-app-python-1   Bound    pvc-51d80cb8-dfe4-4c5a-ad0d-7a921e3fcca2   1Gi        RWO            standard       <unset>                 12m
```

### Visits file

```bash
$ kubectl exec pod/app-python-0 -- cat visits/visits.txt
12

$ kubectl exec pod/app-python-1 -- cat visits/visits.txt
1
```

The values are different because each pod has its own persistent volume that contains a visits file. 
These persistent volumes are not synchronized with each other.

### Persistent storage validation

```bash
$ kubectl delete pod app-python-0
pod "app-python-0" deleted
```

```bash
$ kubectl get pvc
NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
app-python-data-app-python-0   Bound    pvc-7b52d56f-4b76-4d59-9d4e-2e359c5a8af4   1Gi        RWO            standard       <unset>                 39m
app-python-data-app-python-1   Bound    pvc-51d80cb8-dfe4-4c5a-ad0d-7a921e3fcca2   1Gi        RWO            standard       <unset>                 39m
```

```bash
$ kubectl exec pod/app-python-0 -- cat visits/visits.txt
14
```

### Headless Service Access

```bash
$ kubectl exec app-python-0 -- nslookup app-python-1.app-python.default.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10:53

Name:   app-python-1.app-python.default.svc.cluster.local
Address: 10.244.0.8
```

```bash
$ kubectl exec app-python-1 -- nslookup app-python-0.app-python.default.svc.cluster.local
Server:         10.96.0.10
Address:        10.96.0.10:53

Name:   app-python-0.app-python.default.svc.cluster.local
Address: 10.244.0.10
```

### Liveness and readiness probes

I have liveness and readiness probes already defined in `values.yaml`.

### How probes ensure pod health and why they are essential for stateful applications

Both the liveness and readiness probes use an HTTP GET request to the root ("/") endpoint of the application 
to verify its functioning. If the expected OK response is not received, then:

- Liveness Check – Causes Kubernetes to restart the pod automatically  
- Readiness Check – Removes the pod from the service until it is ready to handle traffic again

Probes play a crucial role by ensuring that pods only begin handling traffic when 
they are fully capable of managing their state. They also guarantee that critical operations such as scaling 
and updates happen only when the pods are truly prepared.

### Why ordering guarantees are Unnecessary for my app

1) Every pod keeps its own visit counter, meaning they operate independently without relying on shared data.
2) There is no primary-secondary setup, so one pod doesn't need to be fully operational before another can start.

## Task Bonus

```bash
$ kubectl get po,sts,svc,pvc
NAME               READY   STATUS    RESTARTS   AGE
pod/app-python-0   1/1     Running   0          67m
pod/app-python-1   1/1     Running   0          69m

NAME                          READY   AGE
statefulset.apps/app-python   2/2     108m

NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/app-python   ClusterIP   10.106.35.30     <none>        8000/TCP   108m
service/kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP    133m

NAME                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/app-python-data-app-python-0   Bound    pvc-b39f350d-1aa3-4ea8-b368-08cb838882dd   1Gi        RWO            standard       <unset>                 108m
persistentvolumeclaim/app-python-data-app-python-1   Bound    pvc-0439cc40-bade-4a6f-90d3-770dbf328222   1Gi        RWO            standard       <unset>                 108m
```

### Update Strategies Comparison

#### OnDelete

Updates only happen when pods are manually removed.

Use cases:

1) When you prefer updates to take place at a specific moment of your choosing
2) When you want to verify changes before applying them
3) For critical systems where automatic updates could be dangerous

#### RollingUpdate

Automatically updates pods sequentially. Only those pods with an ordinal number equal to or higher 
than the specified partition value will be updated.

Use cases:

1) When you want to ensure continuous availability during updates
2) When automated updates are acceptable for your application
3) When applying the partition parameter for canary testing

#### Comparison with deployment update strategies

1. Update process  
   - A StatefulSet handles updates sequentially, updating one pod at a time in a predetermined order.  
   - In contrast, a Deployment is capable of updating several pods concurrently.

2. Pod identity management  
   - StatefulSets ensure that each pod retains its unique identity and steady network address even throughout updates.  
   - Deployments, on the other hand, replace pods entirely, resulting in new identities every time they are updated.

3. Rollback characteristics  
   - When rolling back, StatefulSets keep the original pod identities and persistent volume claim (PVC) associations intact.  
   - Conversely, Deployments perform a rollback by creating new pods, discarding the previous pod instances.
