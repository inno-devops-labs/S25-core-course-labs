# Lab 14: Kubernetes StatefulSet â€“ Report

## Task 1: Implement StatefulSet in Helm Chart

- Converted the Deployment to a StatefulSet (see statefulset.yaml).
- Created a headless service (my-app-release-headless) for DNS-based access.
- Added volumeClaimTemplates to ensure each pod gets its own PVC for persistent storage of /data (where the visits file is stored).
- Ensured that the selector labels in the StatefulSet and the pod template are identical (using the helper function "my-app.selectorLabels").

**Command Output:**
```bash
$ kubectl get sts,po,svc,pvc -n default
NAME                              READY   AGE
statefulset.apps/my-app-release   3/3     10m

NAME                                   READY   STATUS    RESTARTS   AGE
pod/my-app-release-0                   1/1     Running   0          10m
pod/my-app-release-1                   1/1     Running   0          10m
pod/my-app-release-2                   1/1     Running   0          10m

NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/my-app-release-headless     ClusterIP   None             <none>        5000/TCP         10m
service/my-app-release-service      NodePort    10.109.118.50    <none>        5000:30518/TCP    12m

NAME                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/persistence-volume-my-app-release-0   Bound    pvc-xxxxxxx                     1Gi        RWO            standard       10m
```

I verified the persistent storage by checking the visits file:
```bash
$ kubectl exec my-app-release-0 -- cat /data/visits.txt
5
```

And checked DNS resolution:
```bash
$ kubectl exec my-app-release-0 -- nslookup my-app-release-1.my-app-release-headless
...
Name:   my-app-release-1.my-app-release-headless.default.svc.cluster.local
Address: 10.244.0.15
```

## Task 2: StatefulSet Exploration and Optimization

- Liveness and readiness probes were added to ensure pod health.
- I tested persistence by deleting pod `my-app-release-0`:
  ```bash
  kubectl delete pod my-app-release-0 -n default
  ```
  Then, verified that PVC and data persisted:
  ```bash
  kubectl get pvc
  kubectl exec my-app-release-1 -- cat /data/visits.txt
  ```
- DNS resolution between pods was confirmed via nslookup.
- Since the application does not require strict ordering, the updateStrategy is set to RollingUpdate with partition: 0 to allow parallel updates.
