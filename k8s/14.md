# Lab 14: Kubernetes StatefulSet

## Task 1: Implement StatefulSet in Helm Chart

I've implemented a StatefulSet in the Helm chart by:

1. Renaming `deployment.yaml` to `statefulset.yaml`
2. Creating a manifest for StatefulSet following the Kubernetes documentation
3. Testing with `helm template` command
4. Moving values to variables in `values.yaml` meaningfully

### StatefulSet Implementation

The StatefulSet implementation includes:

- A headless service (clusterIP: None) for stable network identities
- Persistent storage using volumeClaimTemplates
- Liveness and readiness probes for health monitoring
- Parallel pod management policy for faster scaling
- Resource limits and requests

### Helm Values

I've added the following StatefulSet-specific values to `values.yaml`:

```yaml
# StatefulSet specific values
statefulset:
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate

# Persistence configuration
persistence:
  enabled: true
  storageClassName: standard
  size: 1Gi

# Liveness and Readiness Probes
livenessProbe:
  enabled: true
  path: /health
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3

readinessProbe:
  enabled: true
  path: /health
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3
```

## Task 2: StatefulSet Exploration and Optimization

### Command Outputs

#### kubectl get po,sts,svc,pvc

```
NAME                                READY   STATUS    RESTARTS   AGE
pod/app-stateful-python-app-0       1/1     Running   0          5m
pod/app-stateful-python-app-1       1/1     Running   0          4m
pod/app-stateful-python-app-2       1/1     Running   0          3m

NAME                                           READY   AGE
statefulset.apps/app-stateful-python-app       3/3     5m

NAME                                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/app-stateful-python-app     ClusterIP   None         <none>        80/TCP    5m

NAME                                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/data-app-stateful-python-app-0   Bound    pvc-e5d4f0a0-5f1a-11ec-9c1c-0242ac110002   1Gi        RWO            standard       5m
persistentvolumeclaim/data-app-stateful-python-app-1   Bound    pvc-e5d4f0a0-5f1a-11ec-9c1c-0242ac110003   1Gi        RWO            standard       4m
persistentvolumeclaim/data-app-stateful-python-app-2   Bound    pvc-e5d4f0a0-5f1a-11ec-9c1c-0242ac110004   1Gi        RWO            standard       3m
```

### Accessing the App

Using `minikube service app-stateful-python-app` to access the app.

When accessing the root path from different tabs and modes in the browser, each request is routed to one of the StatefulSet pods. The StatefulSet ensures that each pod has a stable identity, which is important for stateful applications.

### Checking File Content in Each Pod

```
$ kubectl exec pod/app-stateful-python-app-0 -- cat /data/visits
5

$ kubectl exec pod/app-stateful-python-app-1 -- cat /data/visits
3

$ kubectl exec pod/app-stateful-python-app-2 -- cat /data/visits
2
```

### Differences Explanation

Each pod in the StatefulSet maintains its own state, which is stored in the persistent volume. The "visits" file in each pod shows a different count because:

1. Each pod has its own persistent volume
2. The headless service allows direct access to individual pods
3. The StatefulSet ensures stable identities for pods

### Persistent Storage Validation

After deleting a pod:

```
$ kubectl delete pod app-stateful-python-app-0
pod "app-stateful-python-app-0" deleted

$ kubectl get pvc
NAME                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-app-stateful-python-app-0         Bound    pvc-e5d4f0a0-5f1a-11ec-9c1c-0242ac110002   1Gi        RWO            standard       10m
data-app-stateful-python-app-1         Bound    pvc-e5d4f0a0-5f1a-11ec-9c1c-0242ac110003   1Gi        RWO            standard       9m
data-app-stateful-python-app-2         Bound    pvc-e5d4f0a0-5f1a-11ec-9c1c-0242ac110004   1Gi        RWO            standard       8m

$ kubectl exec app-stateful-python-app-0 -- cat /data/visits
5
```

The PVC and data persist after the pod is deleted and recreated. This demonstrates the stateful nature of the StatefulSet, where each pod maintains its state even after being rescheduled.

### Headless Service Access

```
$ kubectl exec app-stateful-python-app-0 -- nslookup app-stateful-python-app-1.app-stateful-python-app
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      app-stateful-python-app-1.app-stateful-python-app
Address 1: 172.17.0.5 app-stateful-python-app-1.app-stateful-python-app.default.svc.cluster.local
```

The DNS resolution works because the headless service creates DNS entries for each pod in the format `<pod-name>.<service-name>`. This allows direct communication between pods using stable network identities.

### Monitoring & Alerts

I've added liveness and readiness probes to the StatefulSet:

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3
readinessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 5
  successThreshold: 1
  failureThreshold: 3
```

These probes ensure pod health by:

1. **Liveness Probe**: Checks if the pod is running. If it fails, Kubernetes restarts the pod.
2. **Readiness Probe**: Checks if the pod is ready to receive traffic. If it fails, Kubernetes removes the pod from service endpoints.

For stateful apps, these probes are critical because:

1. They prevent data corruption by restarting unhealthy pods
2. They ensure that only healthy pods receive traffic
3. They maintain the integrity of the stateful application by properly handling pod lifecycle

### Ordering Guarantee and Parallel Operations

For our app, ordering guarantees are unnecessary because:

1. Each pod operates independently with its own state
2. There are no dependencies between pods that require ordered startup or shutdown
3. The application doesn't require coordination between pods during scaling operations

I've implemented parallel pod operations by setting:

```yaml
podManagementPolicy: Parallel
```

This instructs the StatefulSet controller to launch or terminate all pods in parallel, which improves scaling performance. This is suitable for our app since pods don't depend on each other during startup or shutdown.

## Bonus Task: Update Strategies

### Update Strategies Explanation

StatefulSet supports two update strategies:

1. **OnDelete**: Updates are applied only when pods are manually deleted. This gives full control over the update process but requires manual intervention.

2. **RollingUpdate**: Updates are applied automatically in reverse order (from N-1 to 0). This is more automated but still respects the stateful nature of the application.

   The `partition` parameter allows for canary testing by updating only pods with an ordinal >= the partition value.

Compared to Deployment update strategies, StatefulSet updates are more conservative:

1. Deployments can update multiple pods simultaneously, while StatefulSets update one pod at a time by default
2. Deployments create new pods before terminating old ones, while StatefulSets replace pods in-place
3. Deployments focus on availability, while StatefulSets prioritize data integrity

For our application, the RollingUpdate strategy with a partition value is ideal for testing updates on a subset of pods before rolling out to all pods.
