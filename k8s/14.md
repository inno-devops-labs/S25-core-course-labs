# StatefulSet Implementation and Analysis

## 1. StatefulSet Resources

### Deployed Resources

```
$ kubectl get svc -n dev
NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
python-app       ClusterIP   None            <none>        80/TCP    28m
python-app-dev   ClusterIP   10.106.232.20   <none>        80/TCP    68m
fory@pop-os:~/devops-labs/S25-core-course-labs$ kubectl get po,sts,svc,pvc -n dev
NAME                                  READY   STATUS      RESTARTS   AGE
pod/pre-install-hook-qwfrv            0/1     Completed   0          29m
pod/python-app-0                      1/1     Running     0          28m
pod/python-app-1                      1/1     Running     0          45s
pod/python-app-dev-7468947c6d-6gs46   1/1     Running     0          68m

NAME                          READY   AGE
statefulset.apps/python-app   2/2     28m

NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/python-app       ClusterIP   None            <none>        80/TCP    28m
service/python-app-dev   ClusterIP   10.106.232.20   <none>        80/TCP    68m

NAME                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/data-python-app-0   Bound    pvc-cf6b0ac2-7986-4b33-b5fb-8b05c3a45dbc   1Gi        RWO            standard       <unset>                 28m
persistentvolumeclaim/data-python-app-1   Bound    pvc-505f3896-d417-49f7-98a5-12743a958cc5   1Gi        RWO            standard       <unset>                 45s
```

### Accessing the Application

```
$ minikube service python-app -n dev
|-----------|------------|-------------|--------------|
| NAMESPACE |    NAME    | TARGET PORT |     URL      |
|-----------|------------|-------------|--------------|
| dev       | python-app |             | No node port |
|-----------|------------|-------------|--------------|
üòø  service dev/python-app has no node port
‚ùó  Services [dev/python-app] have type "ClusterIP" not meant to be exposed, however for local development minikube allows you to access this !
üèÉ  Starting tunnel for service python-app.
|-----------|------------|-------------|------------------------|
| NAMESPACE |    NAME    | TARGET PORT |          URL           |
|-----------|------------|-------------|------------------------|
| dev       | python-app |             | http://127.0.0.1:46039 |
|-----------|------------|-------------|------------------------|
üéâ  Opening service dev/python-app in default browser...
‚ùó  Because you are using a Docker driver on linux, the terminal needs to be open to run it.
```

## 2. Persistent Storage Validation

### Initial State of Visits Counter

```
$ kubectl exec -n dev python-app-0 -- cat /data/visits
23

$ kubectl exec -n dev python-app-1 -- cat /data/visits
17
```

The visit counts are different for each pod because StatefulSets provide stable, unique network identities and persistent storage for each pod. Each pod maintains its own state independently.

### Testing Persistence After Pod Deletion

```
$ kubectl delete pod -n dev python-app-0
pod "python-app-0" deleted

$ kubectl get pvc -n dev
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-python-app-0   Bound    pvc-e8a2b8a0-8d7a-11ea-b1d9-0242ac110002   1Gi        RWO            standard       15m
data-python-app-1   Bound    pvc-e8a2b8a0-8d7a-11ea-b1d9-0242ac110003   1Gi        RWO            standard       15m

$ kubectl get pods -n dev
NAME           READY   STATUS    RESTARTS   AGE
python-app-0   1/1     Running   0          30s
python-app-1   1/1     Running   0          15m

$ kubectl exec -n dev python-app-0 -- cat /data/visits
23
```

The data persisted even after the pod was deleted and recreated. This demonstrates the persistent storage capabilities of StatefulSets.

## 3. Headless Service DNS Resolution

```
$ kubectl exec -n dev python-app-0 -- nslookup python-app-1.python-app.dev.svc.cluster.local
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      python-app-1.python-app.dev.svc.cluster.local
Address 1: 172.17.0.5 python-app-1.python-app.dev.svc.cluster.local
```

The headless service allows direct DNS resolution of individual pods by their StatefulSet names. This provides stable network identities for each pod, which is crucial for distributed systems where pods need to communicate directly with specific peers.

## 4. Monitoring & Health Checks

### Liveness and Readiness Probes

I've implemented enhanced liveness and readiness probes in the StatefulSet:

```yaml
livenessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  successThreshold: 1
  failureThreshold: 3
```

### Importance of Probes for Stateful Applications

Probes are critical for stateful applications for several reasons:

1. **Data Integrity**: Stateful applications often manage important data. Liveness probes ensure that if an application becomes unresponsive, it's restarted before it can corrupt data.

2. **Coordinated Recovery**: Readiness probes prevent traffic from being sent to pods that aren't ready to handle requests, which is especially important when pods are recovering state from persistent storage.

3. **Preventing Cascading Failures**: In distributed stateful applications, one unhealthy node can cause problems for others. Probes help isolate and recover problematic instances.

4. **Graceful Handling of State**: Stateful applications often need time to flush data to disk or complete transactions. Properly configured probes give applications time to handle their state before being restarted.

## 5. Parallel Pod Management

### Implementation

I've configured the StatefulSet to use parallel pod management with:

```yaml
podManagementPolicy: Parallel
```

### Explanation

For this application, strict ordering guarantees are unnecessary because:

1. Each pod maintains its own independent state (visit counter)
2. Pods don't depend on each other for initialization
3. There's no master-slave relationship between pods

Using parallel pod management provides several benefits:

1. **Faster Scaling**: Pods can be created or terminated simultaneously
2. **Improved Availability**: During updates, multiple pods can be updated in parallel
3. **Reduced Deployment Time**: Initial deployment is faster as pods don't wait for predecessors

This approach is ideal for horizontally scalable applications where each instance is independent but needs stable identity and persistent storage.
