# Kubernetes StatefulSet report

## State of cluster after chart application

```sh
tedor49@tedor49:~/S25-core-course-labs/k8s$ kubectl get po,sts,svc,pvc
NAME                    READY   STATUS    RESTARTS   AGE
pod/python-msk-time-0   1/1     Running   0          8m56s
pod/python-msk-time-1   1/1     Running   0          16m

NAME                               READY   AGE
statefulset.apps/python-msk-time   2/2     16m

NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/kubernetes        ClusterIP   10.96.0.1        <none>        443/TCP    62m
service/python-msk-time   ClusterIP   10.101.218.159   <none>        8000/TCP   16m

NAME                                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/python-msk-time-pvc-python-msk-time-0   Bound    pvc-59a48abf-11d6-486e-ae80-e01120d4382f   100Mi      RWO            standard       <unset>                 42m
persistentvolumeclaim/python-msk-time-pvc-python-msk-time-1   Bound    pvc-6d486d67-50d3-4c36-8e75-dd225342c509   100Mi      RWO            standard       <unset>                 42m
```

```sh
tedor49@tedor49:~/S25-core-course-labs/k8s$ kubectl exec pod/python-msk-time-0 -- cat /data/visits ; echo
19
tedor49@tedor49:~/S25-core-course-labs/k8s$ kubectl exec pod/python-msk-time-1 -- cat /data/visits ; echo
21
```

The number of visits differ because each of the pods has its own volume where it saves the amount of visits to. The LoadBalancer chooses one of the two pods randomly each time we access an instance, so the amount of visits differs. The amounts are rather high because the probes keep calling our app so it keeps increasing the visit count.

## Deleting a pod

```sh
tedor49@tedor49:~/S25-core-course-labs/k8s$ kubectl exec pod/python-msk-time-0 -- cat /data/visits ; echo
78
tedor49@tedor49:~/S25-core-course-labs/k8s$ kubectl delete pod python-msk-time-0
pod "python-msk-time-0" deleted
tedor49@tedor49:~/S25-core-course-labs/k8s$ kubectl exec pod/python-msk-time-0 -- cat /data/visits ; echo
84
```

## DNS lookup

```sh
tedor49@tedor49:~/S25-core-course-labs/k8s$ kubectl exec python-msk-time-0 -- nslookup python-msk-time-1.python-msk-time.default.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53

Name:	python-msk-time-1.python-msk-time.default.svc.cluster.local
Address: 10.244.0.24


```

```sh
tedor49@tedor49:~/S25-core-course-labs/k8s$ kubectl exec python-msk-time-1 -- nslookup python-msk-time-0.python-msk-time.default.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53


Name:	python-msk-time-0.python-msk-time.default.svc.cluster.local
Address: 10.244.0.25

```

## Monitoring & Alerts

Probes ensure pod health by repeatedly pinging it via some means (in this case it is an http request). If the pod does not answer the readliness probe, it is considered overloaded and requests are no longer sent to it. However, when a container does nto answer a liveness probe, it is considered dead and restarted. By ensuring the responsiveness of the container, we can ensure the workability of the entire system. They are critical for stateful apps because for them, we need to always ensure they work correctly to not mess up anything in the ordered system. 

## Ordering Guarantee and Parallel Operations

Ordering guarantees are needed when pods depend on each other, so some more important pods must start earlier than others. In our app, pods are just a method of load balancing, they do not depend on each other, so we do not need ordering guarantees. 

Parallel launching and terminating are implemented by just stating podManagementPolicy: Parallel inside the stateeefulset.yaml
