# Lab 14: Kubernetes StatefulSet

## Task 1: Implement StatefulSet in Helm Chart

I've implemented a StatefulSet in my Helm chart by:
1. Renaming deployment.yaml to statefulset.yaml
2. Modifying the API to use StatefulSet instead of Deployment
3. Adding volumeClaimTemplates for persistent storage
4. Configuring a headless service
5. Adding liveness and readiness probes

## Task 2: StatefulSet Exploration and Optimization

### StatefulSet Resources

Output of `kubectl get po,sts,svc,pvc`:

```bash
kubectl get po,sts,svc,pvc

# Output:
NAME                                         READY   STATUS    RESTARTS       AGE
pod/my-stateful-helm-chart-0                 1/1     Running   0              10m
pod/my-stateful-helm-chart-1                 1/1     Running   0              10m
pod/python-app-helm-chart-797f69b6c9-bsp6g   1/1     Running   0              45m
pod/python-app-helm-chart-797f69b6c9-kmpbs   1/1     Running   0              42m
pod/python-app-helm-chart-797f69b6c9-sg7nd   1/1     Running   0              45m
pod/vault-0                                  1/1     Running   1 (7d2h ago)   7d3h
pod/vault-agent-injector-66f45b5fd5-5pmwf    1/1     Running   1 (7d2h ago)   7d3h

NAME                                      READY   AGE
statefulset.apps/my-stateful-helm-chart   2/2     14m
statefulset.apps/vault                    1/1     7d3h

NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/kubernetes                 ClusterIP   10.96.0.1        <none>        443/TCP             18d
service/my-stateful-helm-chart     ClusterIP   None             <none>        80/TCP              14m
service/python-app-helm-chart      ClusterIP   10.100.1.170     <none>        80/TCP              45m
service/vault                      ClusterIP   10.107.180.205   <none>        8200/TCP,8201/TCP   7d3h
service/vault-agent-injector-svc   ClusterIP   10.109.28.93     <none>        443/TCP             7d3h
service/vault-internal             ClusterIP   None             <none>        8200/TCP,8201/TCP   7d3h

NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/data-my-stateful-helm-chart-0   Bound    pvc-1cb6586f-6c19-466c-a055-7169f77b5d5e   1Gi        RWO            standard       <unset>                 14m
persistentvolumeclaim/data-my-stateful-helm-chart-1   Bound    pvc-5daedab8-4577-4cc6-b249-11b492fef96b   1Gi        RWO            standard       <unset>                 14m
persistentvolumeclaim/data-my-stateful-helm-chart-2   Bound    pvc-8d8544d9-dbc7-495f-9874-f97c8538af09   1Gi        RWO            standard       <unset>                 14m
```


### Accessing the Application

I accessed my application using:
```bash
minikube service my-stateful-helm-chart
```

When accessing the application from different browser tabs, I observed:
- Each pod maintains its own visit count
- The hostname displayed is unique for each pod

### Persistent Storage Validation

Content of visits file in each pod:

```bash
kubectl exec my-stateful-helm-chart-0 -- cat /data/visits
kubectl exec my-stateful-helm-chart-1 -- cat /data/visits

#Ouptut:
16
10
```


After deleting a pod:

```bash
kubectl delete pod my-stateful-helm-chart-0
```

The PVC persisted:

```bash
kubectl get pvc

# Output:
NAME                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
data-my-stateful-helm-chart-0   Bound    pvc-1cb6586f-6c19-466c-a055-7169f77b5d5e   1Gi        RWO            standard       <unset>                 17m
data-my-stateful-helm-chart-1   Bound    pvc-5daedab8-4577-4cc6-b249-11b492fef96b   1Gi        RWO            standard       <unset>                 17m
data-my-stateful-helm-chart-2   Bound    pvc-8d8544d9-dbc7-495f-9874-f97c8538af09   1Gi        RWO            standard       <unset>                 17m
```

And the data was preserved:

```bash
kubectl exec my-stateful-helm-chart-0 -- cat /data/visits

# Output:
16
```

### Headless Service Access

I tested DNS resolution between pods using the fully qualified domain name (FQDN):

```bash
kubectl exec my-stateful-helm-chart-0 -- nslookup my-stateful-helm-chart-1.my-stateful-helm-chart.default.svc.cluster.local

# Output:
Server:         10.96.0.10
Address:        10.96.0.10:53

Name:   my-stateful-helm-chart-1.my-stateful-helm-chart.default.svc.cluster.local
Address: 10.244.0.92
```

This successful DNS resolution demonstrates one of the key features of StatefulSets: stable network identity. Each pod in a StatefulSet gets a predictable DNS name following the pattern `<pod-name>.<service-name>.<namespace>.svc.cluster.local`, which allows direct pod-to-pod communication.

This is particularly important for stateful applications where pods need to communicate directly with specific peers, rather than through a load-balanced service.

### Monitoring & Alerts

I've added liveness and readiness probes to my StatefulSet:
- The liveness probe checks if the application is running by making HTTP requests to the /health endpoint
- The readiness probe determines if the pod is ready to receive traffic

These probes are critical for stateful applications because:
1. They ensure that only healthy pods receive traffic
2. They prevent data corruption by restarting unhealthy pods
3. They provide automatic recovery from temporary failures

### Ordering Guarantee and Parallel Operations

For my visit counter application, strict ordering guarantees are unnecessary because:
1. Each pod maintains its own independent state
2. There are no dependencies between pods
3. The application doesn't require coordinated startup or shutdown

I implemented parallel pod management by setting:

```yaml
podManagementPolicy: "Parallel"
```

This instructs the StatefulSet controller to launch or terminate all pods in parallel, which improves deployment speed.