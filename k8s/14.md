# 14.md

# Task 1: Implement a StatefulSet in Helm Chart

1. **Renamed `deployment.yml` to `statefulset.yml`**  
   - Updated the manifest to `kind: StatefulSet`, specifying a `serviceName`, `volumeClaimTemplates`, and stable Pod identifiers.

2. **Dry-Run & Deployment**  
   - Command:
     ```bash
     helm install --dry-run --debug python-app python-app
     ```
   - After resolving any issues, deployed with:
     ```bash
     helm install python-app python-app
     ```
     **Output:**
     ```
     andrew@Andrews-MacBook-Pro k8s % helm install python-app python-app
     NAME: python-app
     LAST DEPLOYED: Sun Mar 16 19:46:41 2025
     NAMESPACE: default
     STATUS: deployed
     REVISION: 1
     NOTES:
     1. Get the application URL by running these commands:
       export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services python-app)
       export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
       echo http://$NODE_IP:$NODE_PORT
     ```

3. **Best Practices**  
   - Liveness/readiness probes parameterized in `values.yaml`.

So, everything was done like in a guide.

---

# Task 2: StatefulSet Exploration & Optimization

## 1. Resource Outputs

Below are the outputs showing our StatefulSet and associated resources (Pods, Services, PVCs). Replace the example lines with your actual console output.

```bash
kubectl get pod,sts,svc,pvc
```
**Output:**
```
andrew@Andrews-MacBook-Pro k8s % kubectl get pod,sts,svc,pvc
NAME                                        READY   STATUS    RESTARTS        AGE
pod/python-app-0                            1/1     Running   0               60s
pod/python-app-1                            1/1     Running   0               60s
pod/vault-0                                 1/1     Running   2 (47h ago)     7d19h
pod/vault-agent-injector-66f45b5fd5-rdncj   1/1     Running   2 (6d10h ago)   7d19h

NAME                          READY   AGE
statefulset.apps/python-app   2/2     60s
statefulset.apps/vault        1/1     7d19h

NAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/kubernetes                 ClusterIP   10.96.0.1       <none>        443/TCP             17d
service/python-app                 NodePort    10.101.61.167   <none>        5001:32248/TCP      60s
service/vault                      ClusterIP   10.104.110.68   <none>        8200/TCP,8201/TCP   7d19h
service/vault-agent-injector-svc   ClusterIP   10.110.10.101   <none>        443/TCP             7d19h
service/vault-internal             ClusterIP   None            <none>        8200/TCP,8201/TCP   7d19h

NAME                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/python-app-data-python-app-0   Bound    pvc-91127311-4f85-4506-bbdb-76c5d513038e   512Mi      RWO            standard       <unset>                 21m
persistentvolumeclaim/python-app-data-python-app-1   Bound    pvc-142c0a15-81b3-4255-bc3c-3cd87a099831   512Mi      RWO            standard       <unset>                 21m
```
## 2. Accessing the App
1. **Minikube Service**  
    ```bash
    minikube service python-app
    ```
    This command opens your browser (and prints a URL) to access the application’s root path.
2. **Browser Tests**
   - Open multiple tabs or use different browsers to ensure each Pod might respond differently if they store data locally.
   - If each Pod increments a local file (like /data/visits), you’ll see distinct counters.
   
## 3. Checking File Content in Each Pod 
Each Pod in the StatefulSet has its own PersistentVolumeClaim. Confirm it:
```
andrew@Andrews-MacBook-Pro k8s % kubectl exec python-app-0 -- cat /app/data/visits
8%                                                                                                                                               
andrew@Andrews-MacBook-Pro k8s % kubectl exec python-app-1 -- cat /app/data/visits
6% 
```
Each Pod has its own volume, so Pod 0 and Pod 1 maintain separate data. If Pod 0 has been accessed more often, its `visits` count will be higher than Pod 1’s.

## 4. Persistent Storage Validation
1. **Delete a Pod**
   ```bash
   kubectl delete pod python-app-1
   ```
2. **Check the PVC**
   ```bash
   kubectl get pvc
   ```
   **Output:**
   ```
    andrew@Andrews-MacBook-Pro k8s % kubectl get pvc
    NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
    python-app-data-python-app-0   Bound    pvc-91127311-4f85-4506-bbdb-76c5d513038e   512Mi      RWO            standard       <unset>                 44m
    python-app-data-python-app-1   Bound    pvc-142c0a15-81b3-4255-bc3c-3cd87a099831   512Mi      RWO            standard       <unset>                 44m
   ```
   The claim remains Bound.
3. **New Pod Reuse**
   Once `python-app-1` restarts, check again:
   ```bash
   kubectl exec python-app-1 -- cat /app/data/visits
   ```
   Output:
   ```
    andrew@Andrews-MacBook-Pro k8s % kubectl exec python-app-1 -- cat /app/data/visits
    6%   
   ```
   The data persists, confirming that the volume is reattached to the new Pod!

## 5. Headless Service & DNS
1. **Headless Service**  
   Our chart uses `serviceName: python-app` in the StatefulSet.
2. **DNS Resolution**
   - Each Pod is reachable:
     ```bash
     kubectl exec python-app-0 -- nslookup python-app-1.python-app.default.svc.cluster.local
     kubectl exec python-app-1 -- nslookup python-app-0.python-app.default.svc.cluster.local
     ```
     **Output:**
     ```
     andrew@Andrews-MacBook-Pro k8s % kubectl exec python-app-0 -- nslookup python-app-1.python-app.default.svc.cluster.local
     Server:		10.96.0.10
     Address:	10.96.0.10:53
    
    
     Name:	python-app-1.python-app.default.svc.cluster.local
     Address: 10.244.0.201
     andrew@Andrews-MacBook-Pro k8s % kubectl exec python-app-1 -- nslookup python-app-0.python-app.default.svc.cluster.local
     Server:		10.96.0.10
     Address:	10.96.0.10:53
    
    
     Name:	python-app-0.python-app.default.svc.cluster.local
     Address: 10.244.0.200
     ```
## 6. Monitoring & Alerts  
Added _livenessProbe_ and _readinessProbe_ in `values.yaml`  
### How Probes Ensure Pod Health

Kubernetes uses two main types of HTTP-based probes—**liveness** and **readiness**—to monitor and maintain Pod health:

1. **Liveness Probe**  
   - Periodically checks if the container is still alive (for example, by hitting a `/health` endpoint).  
   - If the container is unresponsive or fails the probe, Kubernetes automatically restarts it.  
   - This prevents Pods from remaining in a “hung” or “crashed” state indefinitely.

2. **Readiness Probe**  
   - Checks if the application is “ready” to serve requests.  
   - Until a Pod passes this check, it won’t receive traffic from Services.  
   - For a **stateful** app, the Pod might need to reattach its volume, load data, or perform initialization. Only when this is done does the Pod become “Ready.”

#### Why They’re Critical for Stateful Apps

- **Data Consistency**: A Pod must be fully operational and have its persistent volume correctly mounted before handling requests. The readiness probe ensures the Pod doesn’t serve traffic prematurely.  
- **Automatic Recovery**: If a stateful container locks up (for instance, due to a database error or memory leak), the liveness probe triggers a restart, preventing indefinite downtime.  
- **Minimal Downtime**: In a multi-Pod stateful environment, you can keep the rest of the replicas serving traffic while a failing Pod restarts or recovers, thanks to these probes.

## 7. Ordering Guarantee vs. Parallel Operations

By default, a StatefulSet enforces an **ordered** rollout: Pod 0 starts, then Pod 1, etc. Similarly, it terminates them in reverse order. This is crucial if your application requires strict sequencing (for example, a database cluster that must bootstrap in a particular order).

However, **if out application doesn’t rely on Pod 0 finishing before Pod 1**, so we can enable:

```yaml
podManagementPolicy: Parallel
```
**Why Ordering Guarantees Are Unnecessary for My App**

- My app doesn’t require a specialized handshake or leader-election process that demands Pod 0 be up before Pod 1.
- Each Pod is self-sufficient, simply mounting its own volume and running independently.
- Parallel operations reduce deployment time by creating or terminating Pods simultaneously.

## Bonus Task

### Updating Golang Application
**Output:**
```
andrew@Andrews-MacBook-Pro k8s % helm install golang-app golang-app                                                     
NAME: golang-app
LAST DEPLOYED: Sun Mar 16 20:36:55 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services golang-app)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
andrew@Andrews-MacBook-Pro k8s % kubectl get pod,sts,svc,pvc
NAME                                        READY   STATUS    RESTARTS        AGE
pod/golang-app-0                            1/1     Running   0               3m2s
pod/python-app-0                            1/1     Running   0               53m
pod/python-app-1                            1/1     Running   0               49m
pod/vault-0                                 1/1     Running   2 (2d ago)      7d20h
pod/vault-agent-injector-66f45b5fd5-rdncj   1/1     Running   2 (6d10h ago)   7d20h

NAME                          READY   AGE
statefulset.apps/golang-app   1/1     3m2s
statefulset.apps/python-app   2/2     53m
statefulset.apps/vault        1/1     7d20h

NAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/golang-app                 NodePort    10.109.245.6    <none>        8080:30660/TCP      3m2s
service/kubernetes                 ClusterIP   10.96.0.1       <none>        443/TCP             17d
service/python-app                 NodePort    10.101.61.167   <none>        5001:32248/TCP      53m
service/vault                      ClusterIP   10.104.110.68   <none>        8200/TCP,8201/TCP   7d20h
service/vault-agent-injector-svc   ClusterIP   10.110.10.101   <none>        443/TCP             7d20h
service/vault-internal             ClusterIP   None            <none>        8200/TCP,8201/TCP   7d20h

NAME                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/golang-app-data-golang-app-0   Bound    pvc-e6500232-9bc5-4b33-8658-03b5e0a4bff2   512Mi      RWO            standard       <unset>                 3m2s
persistentvolumeclaim/python-app-data-golang-app-0   Bound    pvc-479c48b3-edfb-418f-81ca-4afab015b30e   512Mi      RWO            standard       <unset>                 10m
persistentvolumeclaim/python-app-data-python-app-0   Bound    pvc-91127311-4f85-4506-bbdb-76c5d513038e   512Mi      RWO            standard       <unset>                 74m
persistentvolumeclaim/python-app-data-python-app-1   Bound    pvc-142c0a15-81b3-4255-bc3c-3cd87a099831   512Mi      RWO            standard 
```

### Explore Update Strategies

In **StatefulSet**, we  can choose between:

1. **OnDelete**  
   - The controller **never** automatically updates or replaces Pods.  
   - You manually delete each Pod to trigger an update.  
   - Useful if your app can’t handle automatic restarts or must do a carefully orchestrated process.

2. **RollingUpdate**  
   - Allows the controller to update Pods automatically, one by one (or in parallel if you set `podManagementPolicy: Parallel`).  
   - The `partition` field can define how many Pods are “partitioned” away from updates, letting you do partial canaries.  
This means one Pod remains on the old version while the others update, letting you test changes on a subset of Pods (a canary). Once you’re confident, you reduce partition to 0, allowing all Pods to roll.
## Compare with Deployment Update Strategies

### Deployment Update Strategies
- **RollingUpdate**: Gradually updates Pods behind a ReplicaSet, creating new Pods and phasing out old ones. This is the default strategy for Deployments, minimizing downtime by ensuring some Pods remain available throughout the update.
- **Recreate**: Terminates all existing Pods before creating new ones. This can cause downtime, but is simpler for apps that cannot handle multiple versions running simultaneously.

### StatefulSet Update Strategies
- **OnDelete**: The controller never automatically updates Pods. You must manually delete each Pod to trigger an update. This is often used for apps requiring careful, manual orchestration (for example, complex databases that need specialized migration steps).
- **RollingUpdate**: Allows the StatefulSet controller to automatically update Pods, but does so in a strictly ordered or parallel manner (depending on `podManagementPolicy`). Each Pod’s unique identity and stable storage remain intact. 
  - **Partition** can define a subset of Pods that remain on the old version while others update, effectively enabling a canary-like approach.

### Key Differences
- **Stable Identity**: StatefulSets assign persistent pod names (like `golang-app-0`, `golang-app-1`), whereas Deployments rely on ephemeral ReplicaSet Pod names.
- **Volume Management**: StatefulSets typically use volumeClaimTemplates to attach unique PersistentVolumeClaims to each Pod, whereas Deployments commonly share ephemeral volumes or rely on external volumes.
- **Update Behavior**:  
  - Deployments can seamlessly roll out updates with `RollingUpdate` or do a `Recreate`.  
  - StatefulSets can do a more controlled approach: either manual (`OnDelete`) or an automated `RollingUpdate` that respects each Pod’s identity and volume.

In short, **Deployments** are simpler for stateless apps where Pod identity is unimportant, while **StatefulSets** offer stable network IDs and persistent volumes, crucial for stateful applications. Both can use rolling updates, but the **StatefulSet** ensures a more carefully controlled rollout to maintain data consistency and Pod uniqueness. 

