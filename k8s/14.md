# Lab 14: Kubernetes StatefulSet

## Overview of StatefulSet Implementation

Implementation and exploration of Kubernetes StatefulSet for the Moscow Time application

## StatefulSet and Related Resources

```bash
$ kubectl get po,sts,svc,pvc
NAME                                              READY   STATUS    RESTARTS   AGE
pod/my-statefulset-moscow-time-app-0              1/1     Running   0          25m
pod/my-statefulset-moscow-time-app-1              1/1     Running   0          25m
pod/my-statefulset-moscow-time-app-2              1/1     Running   0          25m
pod/my-statefulset-moscow-time-app-3              1/1     Running   0          25m

NAME                                              READY   AGE
statefulset.apps/my-statefulset-moscow-time-app   4/4     25m

NAME                                      TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/kubernetes                        ClusterIP      10.96.0.1        <none>        443/TCP          5h30m
service/my-statefulset-moscow-time-app    LoadBalancer   10.109.236.206   <pending>     8000:32220/TCP   25m
service/my-statefulset-moscow-time-app-headless   ClusterIP      None     <none>        8000/TCP         25m

NAME                                                          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/data-my-statefulset-moscow-time-app-0   Bound    pvc-27d2103d-5de6-446f-8518-9f31f71cad05   1Gi        RWO            standard       25m
persistentvolumeclaim/data-my-statefulset-moscow-time-app-1   Bound    pvc-feebc845-0713-45c7-9f88-cb3f0473fc83   1Gi        RWO            standard       25m
persistentvolumeclaim/data-my-statefulset-moscow-time-app-2   Bound    pvc-abe24732-017f-4770-be50-62ffffe82196   1Gi        RWO            standard       25m
persistentvolumeclaim/data-my-statefulset-moscow-time-app-3   Bound    pvc-f7d61e53-9c10-45ab-8d08-3f15d37d8628   1Gi        RWO            standard       25m
```

## Visit Counter Analysis

Each pod in the StatefulSet maintains its own independent visit counter stored in persistent storage:

```bash
$ kubectl exec my-statefulset-moscow-time-app-0 -- cat /app/app/visits/visits_count.txt
44%
$ kubectl exec my-statefulset-moscow-time-app-1 -- cat /app/app/visits/visits_count.txt
42%
$ kubectl exec my-statefulset-moscow-time-app-2 -- cat /app/app/visits/visits_count.txt
67%
$ kubectl exec my-statefulset-moscow-time-app-3 -- cat /app/app/visits/visits_count.txt
40%
```

**Explanation of differences**: The visit counters vary between pods because each pod receives a different portion of the traffic through the LoadBalancer. Some pods may receive more requests based on the load balancing algorithm and user behavior patterns.

## Persistent Storage Validation

To verify data persistence: deleted pod 0 and monitored its recreation and data state:

```bash
$ kubectl delete pod my-statefulset-moscow-time-app-0
pod "my-statefulset-moscow-time-app-0" deleted

$ kubectl exec my-statefulset-moscow-time-app-0 -- cat /app/app/visits/visits_count.txt
47%
```

The visit counter changed from 44% to 47%, confirming that:
1. Data persisted after pod deletion
2. The StatefulSet controller automatically recreated the pod with the same name
3. The same PersistentVolumeClaim was reattached to the new pod
4. The counter continued to increment with new visits

## Headless Service DNS Resolution

To test DNS resolution for the StatefulSet: used an interactive debug pod:

```bash
$ kubectl run -it --rm debug --image=busybox -- sh
/ # nslookup my-statefulset-moscow-time-app-headless
Server:         10.96.0.10
Address:        10.96.0.10:53
** server can't find my-statefulset-moscow-time-app-headless.cluster.local: NXDOMAIN
** server can't find my-statefulset-moscow-time-app-headless.cluster.local: NXDOMAIN
** server can't find my-statefulset-moscow-time-app-headless.svc.cluster.local: NXDOMAIN
Name:   my-statefulset-moscow-time-app-headless.default.svc.cluster.local
Address: 10.244.0.90
Name:   my-statefulset-moscow-time-app-headless.default.svc.cluster.local
Address: 10.244.0.88
Name:   my-statefulset-moscow-time-app-headless.default.svc.cluster.local
Address: 10.244.0.89
Name:   my-statefulset-moscow-time-app-headless.default.svc.cluster.local
Address: 10.244.0.87
** server can't find my-statefulset-moscow-time-app-headless.svc.cluster.local: NXDOMAIN
```

The results show that the Headless Service correctly resolves to four different IP addresses (one for each pod in the StatefulSet), demonstrating how Kubernetes creates DNS entries for stateful applications

In Kubernetes, StatefulSets work in conjunction with Headless Services to provide stable network identities. Each pod gets a predictable DNS name in the format:
`<pod-name>.<service-name>.<namespace>.svc.cluster.local`

While individual pod lookups in this test didn't resolve directly, the Headless Service itself correctly returned all pod IP addresses, which is the expected behavior for service discovery

## Monitoring & Liveness/Readiness Probes

Probes are crucial for stateful applications for several reasons:

1. **Liveness Probe** - Ensures container restart if it becomes unresponsive. For stateful apps, this is critical as "zombie" containers can lead to data loss or corruption. Probes check the HTTP endpoint with the following configuration:
```yaml
livenessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 15
  periodSeconds: 20
```

2. **Readiness Probe** - Ensures traffic is only sent to pods that are ready to process requests. For stateful applications, this is essential to prevent data inconsistency from partially initialized pods. Configuration:
```yaml
readinessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 5
  periodSeconds: 10
```

These probes help:
- Prevent data corruption during uncontrolled terminations
- Ensure proper initialization before processing requests
- Allow for safe recovery after failures

## Parallel Operations Implementation

For Moscow Time application, ordering guarantees are unnecessary because:
1. Each pod independently handles its portion of traffic
2. Pods don't depend on each other for data consistency or replication
3. There's no initialization sequence that requires ordered startup

To enable parallel pod operations: implemented the `podManagementPolicy: Parallel` parameter in the StatefulSet spec:

```yaml
spec:
  serviceName: my-statefulset-moscow-time-app-headless
  podManagementPolicy: Parallel
  replicas: 4
```

This instructs the StatefulSet controller to launch and terminate pods in parallel rather than sequentially (the default behavior). This speeds up scaling operations and recovery from failures, as there's no need to wait for each previous pod to be fully started or stopped