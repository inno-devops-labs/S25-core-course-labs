# Lab 14

## Task 1: Implement StatefulSet in Helm Chart

For implementing StatefulSet for my `moscow-time-python` Helm Chart I:

- Create `templates/statefulset.yaml`
- Update `values.yaml`

Verify success:

```bash
[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# helm install --dry-run --debug moscow-python moscow-time-python/
install.go:224: 2025-03-08 10:48:05.373860447 +0300 MSK m=+0.031983270 [debug] Original chart version: ""
install.go:241: 2025-03-08 10:48:05.373909469 +0300 MSK m=+0.032032272 [debug] CHART PATH: /home/raleksan/projects/S25-core-course-labs/k8s/moscow-time-python

NAME: moscow-python
LAST DEPLOYED: Sat Mar  8 10:48:05 2025
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: IfNotPresent
  repository: raleksan/app_python
  tag: v0.1
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations:
  vault.hashicorp.com/agent-inject: "true"
  vault.hashicorp.com/agent-inject-secret-database-config.txt: secret/my-secret
  vault.hashicorp.com/agent-inject-template-database-config.txt: |
    {{- with secret "secret/my-secret" -}}
    postgresql://{{ .Data.username }}:{{ .Data.password }}@postgres:5432/wizard
    {{- end }}
  vault.hashicorp.com/role: internal-app
podLabels: {}
podSecurityContext: {}
raleksan-library:
  affinity: {}
  autoscaling:
    enabled: false
    maxReplicas: 100
    minReplicas: 1
    targetCPUUtilizationPercentage: 80
  fullnameOverride: ""
  global: {}
  image:
    pullPolicy: IfNotPresent
    repository: nginx
    tag: ""
  imagePullSecrets: []
  ingress:
    annotations: {}
    className: ""
    enabled: false
    hosts:
    - host: chart-example.local
      paths:
      - path: /
        pathType: ImplementationSpecific
    tls: []
  livenessProbe:
    httpGet:
      path: /
      port: http
  nameOverride: ""
  nodeSelector: {}
  podAnnotations: {}
  podLabels: {}
  podSecurityContext: {}
  readinessProbe:
    httpGet:
      path: /
      port: http
  replicaCount: 1
  resources: {}
  securityContext: {}
  service:
    port: 80
    type: ClusterIP
  serviceAccount:
    annotations: {}
    automount: true
    create: true
    name: ""
  tolerations: []
  volumeMounts: []
  volumes: []
replicaCount: 3
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 128Mi
securityContext: {}
service:
  port: 8000
  type: ClusterIP
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: internal-app
tolerations: []
vault:
  enabled: true
  role: internal-app
  secretPath: secret/my-secret
vaultAgent:
  image: hashicorp/vault:1.14.0
volumeMounts: []
volumes: []

HOOKS:
---
# Source: moscow-time-python/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "moscow-python-moscow-time-python-test-connection"
  labels:
    helm.sh/chart: moscow-time-python-0.1.0
    app.kubernetes.io/name: moscow-time-python
    app.kubernetes.io/instance: moscow-python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['moscow-python-moscow-time-python:8000']
  restartPolicy: Never
---
# Source: moscow-time-python/templates/post-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: post-install-hook
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: post-install-job
          image: busybox
          command: ["sh", "-c", "echo Post-install hook running; sleep 20"]
---
# Source: moscow-time-python/templates/pre-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pre-install-hook
  annotations:
    "helm.sh/hook": pre-install
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: pre-install-job
          image: busybox
          command: ["sh", "-c", "echo Pre-install hook running; sleep 20"]
MANIFEST:
---
# Source: moscow-time-python/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: internal-app
  labels:
    helm.sh/chart: moscow-time-python-0.1.0
    app.kubernetes.io/name: moscow-time-python
    app.kubernetes.io/instance: moscow-python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: moscow-time-python/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: moscow-python-config
data:
  config.json: |
    
    {
        "name": "Aleksandr",
        "surname": "Ryabov",
        "nickname": "Raleksan",
        "meme_pack": "https://gist.github.com/Raleksan/2069dac15b4aa2de5daedba183536e69"
    }
---
# Source: moscow-time-python/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: moscow-python-moscow-time-python
  labels:
    helm.sh/chart: moscow-time-python-0.1.0
    app.kubernetes.io/name: moscow-time-python
    app.kubernetes.io/instance: moscow-python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: moscow-time-python
    app.kubernetes.io/instance: moscow-python
---
# Source: moscow-time-python/templates/deployment.yaml.old
apiVersion: apps/v1
kind: Deployment
metadata:
  name: moscow-python
  labels:
    
    app.kubernetes.io/name: moscow-time-python
    app.kubernetes.io/instance: moscow-python
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: moscow-time-python
      app.kubernetes.io/instance: moscow-python
  template:
    metadata:
      labels:
        app.kubernetes.io/name: moscow-time-python
        app.kubernetes.io/instance: moscow-python
    spec:
      serviceAccountName: "internal-app"
      containers:
        - name: moscow-time-python
          image: "raleksan/app_python:v0.1"
          ports:
            - containerPort: 8000
          volumeMounts:
            - name: config-volume
              mountPath: /config/config.json
              subPath: config.json
          envFrom:
            - configMapRef:
                name: moscow-python-config
        - name: vault-agent
          image: hashicorp/vault:1.14.0
          volumeMounts:
            - name: vault-secret
              mountPath: /vault/secrets
      volumes:
        - name: vault-secret
          emptyDir: {}
        - name: config-volume
          configMap:
            name: moscow-python-config
---
# Source: moscow-time-python/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: moscow-python
  labels:
    
    app.kubernetes.io/name: moscow-time-python
    app.kubernetes.io/instance: moscow-python
spec:
  serviceName: "moscow-python-headless"
  replicas: 3
  selector:
    matchLabels:
      
      app.kubernetes.io/name: moscow-time-python
      app.kubernetes.io/instance: moscow-python
  template:
    metadata:
      labels:
        
        app.kubernetes.io/name: moscow-time-python
        app.kubernetes.io/instance: moscow-python
    spec:
      serviceAccountName: internal-app
      containers:
        - name: moscow-time-python
          image: "raleksan/app_python:v0.1"
          ports:
            - containerPort: 8000
          volumeMounts:
            - name: config-volume
              mountPath: /config/config.json
              subPath: config.json
          envFrom:
            - configMapRef:
                name: moscow-python-config
        - name: vault-agent
          image: hashicorp/vault:1.14.0
          volumeMounts:
            - name: vault-secret
              mountPath: /vault/secrets
      volumes:
        - name: vault-secret
          emptyDir: {}
        - name: config-volume
          configMap:
            name: moscow-python-config

NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=moscow-time-python,app.kubernetes.io/instance=moscow-python" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
```

## Task 2: StatefulSet Exploration and Optimization

### 1. Research and Documentation

```bash
[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl get po,sts,svc,pvc
NAME                                 READY   STATUS    RESTARTS   AGE
pod/moscow-python-0                  2/2     Running   0          31s
pod/moscow-python-1                  2/2     Running   0          30s
pod/moscow-python-2                  2/2     Running   0          29s
pod/moscow-python-55cb95c96f-qkl69   2/2     Running   0          31s

NAME                             READY   AGE
statefulset.apps/moscow-python   3/3     31s

NAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes                         ClusterIP   10.96.0.1       <none>        443/TCP    7d13h
service/moscow-python-moscow-time-python   ClusterIP   10.101.56.106   <none>        8000/TCP   31s
```

After multiple attempts to reach service, one can see following data in the visits.txt in pods.

```bash
[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl exec moscow-python-0 -- cat visits.txt
3

[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl exec moscow-python-0 -- cat visits.txt
4

[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl exec moscow-python-0 -- cat visits.txt
2

```

This occurs because each of the pods has its own volume where it stores its own visits.txt file. And the service load balancer routes requests to different pods at different requests. So, reading from these files gives us different unsychronized results.

### 2. Persistent Storage Validation

As one can see from the experiment below, after pod deletion, the PVC still remains alive. An the data retreived from the pod will be equal data before the deletion.

```bash
[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl exec moscow-python-0 -- cat visits.txt
3

[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl delete pod moscow-python-0
pod "moscow-python-0" deleted

[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl get po,sts,svc,pvc
NAME                                 READY   STATUS    RESTARTS   AGE
pod/moscow-python-0                  2/2     Running   0          2s
pod/moscow-python-1                  2/2     Running   0          14m
pod/moscow-python-2                  2/2     Running   0          14m
pod/moscow-python-55cb95c96f-zs64w   2/2     Running   0          14m

NAME                             READY   AGE
statefulset.apps/moscow-python   3/3     14m

NAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kubernetes                         ClusterIP   10.96.0.1       <none>        443/TCP    7d14h
service/moscow-python-moscow-time-python   ClusterIP   10.106.55.225   <none>        8000/TCP   14m

[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl exec moscow-python-0 -- cat visits.txt
3
```

### 3. Headless Service Access

For some reason DNS resolving works with porblems:

```bash
[root@laptop:/home/raleksan/projects/S25-core-course-labs/k8s]# kubectl exec moscow-python-0 -- nslookup moscow-python-1.moscow-python
Defaulted container "moscow-time-python" out of: moscow-time-python, vault-agent
Server:         10.96.0.10
Address:        10.96.0.10:53

** server can't find moscow-python-1.moscow-python: NXDOMAIN

** server can't find moscow-python-1.moscow-python: NXDOMAIN

command terminated with exit code 1
```

### 4. Monitoring & Alerts

- To add probes I updated `values.yaml` to add `livenessProbe`.

- How liveness/readiness probes work:

To ensure pod health probes continiously checks state of the pod. The liveness probe may trigger a restart of the pod if the it fails to respond,
while the readiness probe ensures traffic is only sent to pods that are ready.

- Importance for Stateful Apps:

They are imoportant, because they ensures that each replica is working correctly, which is especially critical for application deployment.

### 5. Ordering Guarantee and Parallel Operations

- Explain "ordering guaranties":

For my application I do not need "ordering guarantees" because I do not have any complex infrastructure or dependent services.

Ordering guarantees are needed in clusters / StatefulSets with master-replicas.

- A way for parrallel launch of pods:

It can be done with `podManagementPolicy: Parallel`.

## Bonus Task: Update Strategies

### 1. Apply StatefulSet to Bonus App

For implementing StatefulSet for my `moscow-time-rust` Helm Chart I:

- Created `templates/statefulset.yaml`
- Updated `values.yaml`

### 2. Explore Update Strategies

- `RollingUpdate`:

Updates pods incrementally, allowing testing for a subset (`canary deployment`) before rolling out to all pods. Helps to minimize downtime and allow rollback if necessary.

- `OnDelete` vs. `RollingUpdate`:
  - `OnDelete`: Pods are updated only when manually deleted, giving full control but requiring manual actions.
    - `RollingUpdate`: Automatically replaces pods one at a time, which is ideal for maintaining service availability.

- Comparison with `Deployments`:

`Deployments` are typically used for stateless apps and also support rolling updates. `StatefulSets` add the benefit of stable identities and persistent storage, which is essential for stateful apps.
