# Lab 14: Kubernetes StatefulSet 

## Overview

In this lab, we convert our application’s Helm chart from a Deployment to a StatefulSet to manage stateful workloads with guarantees about ordering and uniqueness. We also verify persistence, headless service DNS resolution, add liveness/readiness probes, and implement update strategies.

---

## Step 1: Implementing the StatefulSet in the Helm Chart

### 1.2 Update Helm Chart for a StatefulSet

1. **Rename and Update Manifest:**  
   In our existing Helm chart for our Python and Node JS app, we renamed the `deployment.yaml` file to `statefulset.yaml`. Modified its content to conform to a StatefulSet manifest. This YAML below is for our python application


   ```yaml
    apiVersion: apps/v1
    kind: StatefulSet
    metadata:
    name: {{ include "python-app.fullname" . }}
    labels:
        {{- include "python-app.labels" . | nindent 4 }}
    spec:
    serviceName: {{ include "python-app.fullname" . }}
    podManagementPolicy: {{ .Values.podManagementPolicy }}
    {{- if not .Values.autoscaling.enabled }}
    replicas: {{ .Values.replicaCount }}
    {{- end }}
    updateStrategy:
        type: RollingUpdate
        rollingUpdate:
        partition: 1
    selector:
        matchLabels:
        {{- include "python-app.selectorLabels" . | nindent 6 }}
    volumeClaimTemplates:
    - metadata:
        name: {{ .Values.volumeClaimTemplates.name }}
        spec:
        accessModes: {{ .Values.volumeClaimTemplates.accessModes }}
        resources:
            requests:
            storage: {{ .Values.volumeClaimTemplates.storage }}
    template:
        metadata:
        {{- with .Values.podAnnotations }}
        annotations:
            {{- toYaml . | nindent 8 }}
        {{- end }}
        labels:
            {{- include "python-app.labels" . | nindent 8 }}
            {{- with .Values.podLabels }}
            {{- toYaml . | nindent 8 }}
            {{- end }}
        spec:
        {{- with .Values.imagePullSecrets }}
        imagePullSecrets:
            {{- toYaml . | nindent 8 }}
        {{- end }}
        serviceAccountName: {{ include "python-app.serviceAccountName" . }}
        {{- with .Values.podSecurityContext }}
        securityContext:
            {{- toYaml . | nindent 8 }}
        {{- end }}
        containers:
            - name: {{ .Chart.Name }}
            {{- with .Values.securityContext }}
            securityContext:
                {{- toYaml . | nindent 12 }}
            {{- end }}
            image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
            imagePullPolicy: {{ .Values.image.pullPolicy }}
            ports:
                - name: http
                containerPort: {{ .Values.service.port }}
                protocol: TCP
            env:
                - name: MY_PASS
                valueFrom:
                    secretKeyRef:
                    name: {{ include "python-app.fullname" . }}-secret
                    key: MY_PASS
                {{- include "python-app.envVars" . | nindent 12 }}
            {{- with .Values.livenessProbe }}
            livenessProbe:
                {{- toYaml . | nindent 12 }}
            {{- end }}
            {{- with .Values.readinessProbe }}
            readinessProbe:
                {{- toYaml . | nindent 12 }}
            {{- end }}
            {{- with .Values.resources }}
            resources:
                {{- toYaml . | nindent 12 }}
            {{- end }}
            volumeMounts:
                {{- if .Values.persistence.enabled }}
                - name: visits-data
                mountPath: /app/visits
                subPath: visits
                {{- end }}
                - name: config-volume
                mountPath: /app/config
                {{- with .Values.volumeMounts }}
                {{- toYaml . | nindent 12 }}
                {{- end }}
        volumes:
            {{- if .Values.persistence.enabled }}
            - name: visits-data
            persistentVolumeClaim:
                claimName: {{ include "python-app.fullname" . }}-visits-pvc
            {{- end }}
            - name: config-volume
            configMap:
                name: {{ include "python-app.fullname" . }}-config
            {{- with .Values.volumes }}
            {{- toYaml . | nindent 8 }}
            {{- end }}
        {{- with .Values.nodeSelector }}
        nodeSelector:
            {{- toYaml . | nindent 8 }}
        {{- end }}
        {{- with .Values.affinity }}
        affinity:
            {{- toYaml . | nindent 8 }}
        {{- end }}
        {{- with .Values.tolerations }}
        tolerations:
            {{- toYaml . | nindent 8 }}
        {{- end }}

   ```

2. **Headless Service:**  
   Created a headless Serviceto manage the StatefulSet’s network identities. In the chart’s `templates` folder with a file named `headless-service.yaml`:

   ```yaml
    apiVersion: v1
    kind: Service
    metadata:
    name: {{ include "python-app.fullname" . }}-headless
    labels:
        {{- include "python-app.labels" . | nindent 4 }}
    spec:
    clusterIP: None
    ports:
        - port: {{ .Values.service.port }}
        targetPort: http
        protocol: TCP
        name: http
    selector:
        {{- include "python-app.selectorLabels" . | nindent 4 }} 
   ```

3. **Update `values.yaml`:**  
   We updated that `values.yaml` variable for `replicaCount`, and enabled parallel operations for both my python and node app. :

   ```yaml
   replicaCount: 3
   image:
     repository: sedoxxx/python-webapp
     tag: "latest"
   ```

   ```yaml
   podManagementPolicy: Parallel
   ```

4. **Test the Chart Imperatively:**  
   Run a dry-run to check for errors:
   ```bash
   helm install --dry-run --debug python-app-stateful ./python-app
   ```
   Output : 
   ```yaml
    NAME: python-app
    LAST DEPLOYED: Sun Mar 16 22:11:24 2025
    NAMESPACE: default
    STATUS: pending-install
    REVISION: 1
    USER-SUPPLIED VALUES:
    {}

    COMPUTED VALUES:
    affinity: {}
    autoscaling:
    enabled: false
    maxReplicas: 100
    minReplicas: 1
    targetCPUUtilizationPercentage: 80
    fullnameOverride: ""
    image:
    pullPolicy: Always
    repository: sedoxxx/python-webapp
    tag: latest
    imagePullSecrets: []
    ingress:
    annotations: {}
    className: ""
    enabled: false
    hosts:
    - host: chart-example.local
        paths:
        - path: /
        pathType: ImplementationSpecific
    tls: []
    livenessProbe:
    httpGet:
        path: /
        port: http
    nameOverride: ""
    nodeSelector: {}
    persistence:
    accessMode: ReadWriteOnce
    enabled: true
    size: 1Gi
    podAnnotations:
    vault.hashicorp.com/agent-inject: "true"
    vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
    vault.hashicorp.com/agent-inject-template-database-config.txt: |
        {{- with secret "internal/data/database/config" -}}
        export DB_USERNAME="{{ .Data.data.username }}"
        export DB_PASSWORD="{{ .Data.data.password }}"
        {{- end -}}
    vault.hashicorp.com/role: python-app
    podLabels: {}
    podManagementPolicy: Parallel
    podSecurityContext: {}
    readinessProbe:
    httpGet:
        path: /
        port: http
    replicaCount: 4
    resources:
    limits:
        cpu: 500m
        memory: 512Mi
    requests:
        cpu: 250m
        memory: 256Mi
    secret:
    myPass: MySuperSecretPassword
    securityContext: {}
    service:
    port: 5000
    type: NodePort
    serviceAccount:
    annotations: {}
    automount: true
    create: true
    name: ""
    tolerations: []
    volumeClaimTemplates:
    accessModes:
    - ReadWriteOnce
    name: visits-volume
    storage: 128Mi
    volumeMounts:
    - mountPath: /app/config
    name: config-volume
    readOnly: true
    - mountPath: /app/visits
    name: visits-volume
    volumes:
    - configMap:
        items:
        - key: config.json
        path: config.json
        name: python-app-config
    name: config-volume
    - emptyDir: {}
    name: visits-volume

    HOOKS:
    ---
    # Source: python-app/templates/post-install.yaml
    apiVersion: v1
    kind: Pod
    metadata:
    name: python-app-post-install-hook
    annotations:
        "helm.sh/hook": post-install
        "helm.sh/hook-delete-policy": hook-succeeded
    spec:
    containers:
    - name: post-install
        image: busybox
        command: ['sh', '-c', 'echo Starting post-install hook && sleep 20 && echo Post-install hook completed']
    restartPolicy: Never
    ---
    # Source: python-app/templates/pre-install.yaml
    apiVersion: v1
    kind: Pod
    metadata:
    name: python-app-pre-install-hook
    annotations:
        "helm.sh/hook": pre-install
        "helm.sh/hook-delete-policy": hook-succeeded

    spec:
    containers:
    - name: pre-install
        image: busybox
        command: ['sh', '-c', 'echo Starting pre-install hook && sleep 20 && echo Pre-install hook completed']
    restartPolicy: Never
    ---
    # Source: python-app/templates/tests/test-connection.yaml
    apiVersion: v1
    kind: Pod
    metadata:
    name: "python-app-test-connection"
    labels:
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    annotations:
        "helm.sh/hook": test
    spec:
    containers:
        - name: wget
        image: busybox
        command: ['wget']
        args: ['python-app:5000']
    restartPolicy: Never
    MANIFEST:
    ---
    # Source: python-app/templates/serviceaccount.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
    name: python-app
    labels:
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    automountServiceAccountToken: true
    ---
    # Source: python-app/templates/secrets.yaml
    apiVersion: v1
    kind: Secret
    metadata:
    name: python-app-secret
    labels:
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    type: Opaque
    data:
    MY_PASS: "TXlTdXBlclNlY3JldFBhc3N3b3Jk"
    ---
    # Source: python-app/templates/service.yaml
    apiVersion: v1
    kind: Service
    metadata:
    name: python-app
    labels:
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
    type: NodePort
    ports:
        - port: 5000
        targetPort: http
        protocol: TCP
        name: http
    selector:
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
    ---
    # Source: python-app/templates/statefulset.yaml
    apiVersion: apps/v1
    kind: StatefulSet
    metadata:
    name: python-app
    labels:
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
    serviceName: python-app
    podManagementPolicy: Parallel
    replicas: 4
    updateStrategy:
        type: RollingUpdate
        rollingUpdate:
        partition: 1
    selector:
        matchLabels:
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
    volumeClaimTemplates:
    - metadata:
        name: visits-volume
        spec:
        accessModes: [ReadWriteOnce]
        resources:
            requests:
            storage: 128Mi
    template:
        metadata:
        annotations:
            vault.hashicorp.com/agent-inject: "true"
            vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
            vault.hashicorp.com/agent-inject-template-database-config.txt: |
            {{- with secret "internal/data/database/config" -}}
            export DB_USERNAME="{{ .Data.data.username }}"
            export DB_PASSWORD="{{ .Data.data.password }}"
            {{- end -}}
            vault.hashicorp.com/role: python-app
        labels:
            helm.sh/chart: python-app-0.1.0
            app.kubernetes.io/name: python-app
            app.kubernetes.io/instance: python-app
            app.kubernetes.io/version: "1.16.0"
            app.kubernetes.io/managed-by: Helm
        spec:
        serviceAccountName: python-app
        containers:
            - name: python-app
            image: "sedoxxx/python-webapp:latest"
            imagePullPolicy: Always
            ports:
                - name: http
                containerPort: 5000
                protocol: TCP
            env:
                - name: MY_PASS
                valueFrom:
                    secretKeyRef:
                    name: python-app-secret
                    key: MY_PASS
                - name: ENVIRONMENT
                value: "production"
                - name: APP_VERSION
                value: "v1.0.0"
            livenessProbe:
                httpGet:
                path: /
                port: http
            readinessProbe:
                httpGet:
                path: /
                port: http
            resources:
                limits:
                cpu: 500m
                memory: 512Mi
                requests:
                cpu: 250m
                memory: 256Mi
            volumeMounts:
                - name: visits-data
                mountPath: /app/visits
                subPath: visits
                - name: config-volume
                mountPath: /app/config
                - mountPath: /app/config
                name: config-volume
                readOnly: true
                - mountPath: /app/visits
                name: visits-volume
        volumes:
            - name: visits-data
            persistentVolumeClaim:
                claimName: python-app-visits-pvc
            - name: config-volume
            configMap:
                name: python-app-config
            - configMap:
                items:
                - key: config.json
                path: config.json
                name: python-app-config
            name: config-volume
            - emptyDir: {}
            name: visits-volume

    NOTES:
    1. Get the application URL by running these commands:
    export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services python-app)
    export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
    echo http://$NODE_IP:$NODE_PORT
   ```

---

## Step 2: StatefulSet Exploration and Optimization

### 2.1 Verification of Persistence and DNS Resolution

1. **Deploy the StatefulSet:**  
   Install the chart:
   ```bash
   helm upgrade --install python-app-stateful ./python-app

   Release "python-app-stateful" does not exist. Installing it now.
    NAME: python-app-stateful
    LAST DEPLOYED: Sun Mar 16 22:36:34 2025
    NAMESPACE: default
    STATUS: deployed
    REVISION: 1
    NOTES:
    1. Get the application URL by running these commands:
    export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services python-app-stateful)
    export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
    echo http://$NODE_IP:$NODE_PORT
   ```
2. **Check Resources:**  
   Retrieve pods, StatefulSet, Services, and PersistentVolumeClaims:
   ```bash
    $ kubectl get pods,sts,svc,pvc
    NAME                                        READY   STATUS     RESTARTS   AGE
    pod/helm-node-app-65bc8f65cc-fxbn2          1/1     Running    0          164m    
    pod/python-app-stateful-0                   1/1     Running  0          8m   
    pod/python-app-stateful-1                   1/1     Running   0          8m
    pod/python-app-stateful-2                   1/1     Running   0          8m
    pod/python-app-stateful-3                   1/1     Running   0          8m
    pod/python-app-release-6b68cfdf7f-9kws7     0/2     Init:0/1   0          164m   
    pod/python-app-release-7f76548594-jfhjn     1/1     Running    0          164m   
    pod/vault-0                                 1/1     Running    0          164m   
    pod/vault-agent-injector-66f45b5fd5-7p9hb   1/1     Running    0          164m   


    NAME                                   READY   AGE
    statefulset.apps/python-app-stateful   4/4     8m
    statefulset.apps/vault                 1/1     13d

    NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
    service/helm-node-app              NodePort    10.96.54.66      <none>        3000:32469/TCP      14d
    service/helm-python-app            NodePort    10.100.68.39     <none>        5000:32521/TCP      14d
    service/kubernetes                 ClusterIP   10.96.0.1        <none>        443/TCP             14d
    service/python-app                 NodePort    10.97.125.214    <none>        5000:31535/TCP      13d
    service/python-app-release         NodePort    10.101.130.252   <none>        5000:32345/TCP      13d
    service/python-app-stateful        NodePort    10.108.84.171    <none>        5000:32436/TCP      8m
    service/vault                      ClusterIP   10.100.105.214   <none>        8200/TCP,8201/TCP   13d
    service/vault-agent-injector-svc   ClusterIP   10.107.27.76     <none>        443/TCP             13d
    service/vault-internal             ClusterIP   None             <none>        8200/TCP,8201/TCP   13d

    NAME                                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
    persistentvolumeclaim/visits-volume-python-app-stateful-0   Bound    pvc-ef5ea0e5-c132-43b1-ad4a-168f1eee6a02   128Mi      RWO            standard       <unset>                 8m
    persistentvolumeclaim/visits-volume-python-app-stateful-1   Bound    pvc-4c7418e3-ba65-40ba-b92f-9evd2eb19686   128Mi      RWO            standard       <unset>                 8m
    persistentvolumeclaim/visits-volume-python-app-stateful-2   Bound    pvc-e9d7e5f6-3ef6-4b26-b6d2-05edaee45fb9   128Mi      RWO            standard       <unset>                 8m
    persistentvolumeclaim/visits-volume-python-app-stateful-3   Bound    pvc-2775175e-ccc4-4edc-99a1-9debdab8aaa5   128Mi      RWO            standard       <unset>                 8m
   ```

   ```bash
    $ minikube service python-app-stateful
    |-----------|---------------------|-------------|---------------------------|
    | NAMESPACE |        NAME         | TARGET PORT |            URL            |    
    |-----------|---------------------|-------------|---------------------------|    
    | default   | python-app-stateful | http/5000   | http://192.168.49.2:32436 |    
    |-----------|---------------------|-------------|---------------------------| 
   ```
   ```bash
   $ kubectl exec python-app-stateful-0 exec -- cat visits
   34
   ```
3. **Persistent Storage Validation:**  
   - Delete one pod:
     ```bash
     kubectl delete pod python-app-stateful-0
     deleted
     ```
   - Verify that the corresponding PVC still exists:
     ```bash
     kubectl get pvc
     kubectl exec python-app-stateful-0 exec -- cat visits
     36
     ```

4. **Headless Service DNS Resolution:**  
   Test DNS resolution among pods:
   ```bash
    kubectl exec python-app-stateful-0 -- getent hosts python-app-stateful-1.python-app-stateful
    10.243.1.55     python-app-stateful-1.python-app-stateful.default.svc.cluster.local
   ```

### 2.2 Monitoring and Probes

1. **Liveness/Readiness Probes:**  
   The StatefulSet manifest includes liveness and readiness probes to ensure pod health.  
    ```yaml
    livenessProbe:
    httpGet:
        path: /
        port: http
    readinessProbe:
    httpGet:
        path: /
        port: http
    ```
    - `Liveness Probe`: This probe verifies if your application is still operational and healthy. If the probe fails, Kubernetes will automatically restart the pod to ensure continuity.

    - `Readiness Probe`: This probe assesses whether the pod is prepared to manage incoming traffic. If the probe fails, Kubernetes will halt traffic to that pod, even if it remains operational.

2. **Ordering and Parallel Operations:**  

   - To launch pods in parallel, update the StatefulSet’s update strategy by setting:
     ```yaml
     updateStrategy:
       type: RollingUpdate
       rollingUpdate:
         partition: 0
     ```
    Ordering guarantees are not required for my application since each pod functions independently, utilizing its own Persistent Volume Claim (PVC).

    - There are no dependencies between pods during startup or shutdown, meaning the sequence in which pods are started or stopped does not impact the application's functionality.
    - The system is designed to manage parallel operations across pods without any complications.

    For parallel operations, the `podManagementPolicy: Parallel` configuration was implemented. This directs the StatefulSet controller to launch or terminate all pods simultaneously, rather than sequentially, thereby optimizing the deployment and scaling processes.

---

## Step 3: Bonus Task – Update Strategies for the Bonus App

1. **Convert Node Js App to StatefulSet:**  
    We already modifief its Helm chart to use a StatefulSet (similar to the steps above for our python app).
2. **Implement Rolling Updates and Canary Testing:**  
   Update the StatefulSet manifest with an update strategy:
   ```yaml
   updateStrategy:
     type: RollingUpdate
     rollingUpdate:
       partition: 1
   ```
- `OnDelete`:  
  This update strategy requires manual deletion of pods to apply changes. It is suitable for stateful applications that need controlled and deliberate updates, such as databases, where careful management is essential.

- `RollingUpdate`:  
  This strategy updates pods sequentially, ensuring that the identity of each pod is maintained and avoiding downtime. It is ideal for stateful applications that require zero-downtime updates, such as distributed databases, where continuous availability is critical.


---



