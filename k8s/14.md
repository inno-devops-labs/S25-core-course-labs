# StatefulSet lab

## StatefulSet in the cluster
```bash
NAME                                                      READY   STATUS      RESTARTS        AGE
pod/time-moscow-post-install-hook                         0/1     Completed   0               55m
pod/time-moscow-pre-install-hook                          0/1     Completed   0               56m
pod/time-moscow-set-0                                     2/2     Running     0               10m
pod/time-moscow-set-1                                     2/2     Running     0               10m
pod/time-moscow-set-2                                     2/2     Running     0               10m
pod/time-moscow-set-post-install-hook                     0/1     Completed   0               10m
pod/time-moscow-set-pre-install-hook                      0/1     Completed   0               11m
pod/typescript-app-propositional-logic-5977d9cdfc-5w8xp   1/1     Running     0               3h37m
pod/typescript-app-propositional-logic-5977d9cdfc-dqskm   1/1     Running     0               3h37m
pod/typescript-app-propositional-logic-5977d9cdfc-m5lt8   1/1     Running     0               3h37m
pod/vault-0                                               1/1     Running     1 (6d19h ago)   7d7h
pod/vault-agent-injector-84b987db6f-zzgxf                 1/1     Running     1 (6d19h ago)   7d7h

NAME                               READY   AGE
statefulset.apps/time-moscow-set   3/3     10m
statefulset.apps/vault             1/1     7d7h

NAME                                         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/kubernetes                           ClusterIP      10.43.0.1       <none>        443/TCP             18d
service/propositional-logic                  ClusterIP      10.43.73.181    <none>        8001/TCP            6d20h
service/propositional-logic-app-service      LoadBalancer   10.43.195.163   <pending>     8000:32447/TCP      18d
service/time-moscow-service                  LoadBalancer   10.43.54.220    192.168.5.1   8000:31544/TCP      18d
service/time-moscow-set                      ClusterIP      None            <none>        8000/TCP            10m
service/typescript-app-propositional-logic   ClusterIP      10.43.101.54    <none>        8001/TCP            3h37m
service/vault                                ClusterIP      10.43.129.9     <none>        8200/TCP,8201/TCP   7d7h
service/vault-agent-injector-svc             ClusterIP      10.43.121.21    <none>        443/TCP             7d7h
service/vault-internal                       ClusterIP      None            <none>        8200/TCP,8201/TCP   7d7h

NAME                                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/data-time-moscow-set-0   Bound    pvc-c1662177-97d3-4331-ac64-02d8839a2a17   5Mi        RWO            local-path     <unset>                 46m
persistentvolumeclaim/data-time-moscow-set-1   Bound    pvc-64d09b8c-aaba-43fb-90cd-5d58acbdf4e8   5Mi        RWO            local-path     <unset>                 10m
persistentvolumeclaim/data-time-moscow-set-2   Bound    pvc-7c4fdfdf-dc37-4548-8541-c09f2450d6f7   5Mi        RWO            local-path     <unset>                 10m
```

## Data volume across the replicas

```bash
> kubectl exec pod/time-moscow-set-0 -- cat /app/data/visits
Defaulted container "time-moscow" out of: time-moscow, vault-agent, vault-agent-init (init)
38%                                                                                                                                         fallenchromium@Pavels-MacBook-Air k8s % kubectl exec pod/time-moscow-set-1 -- cat /app/data/visits
Defaulted container "time-moscow" out of: time-moscow, vault-agent, vault-agent-init (init)
28%                                                                                                                                         fallenchromium@Pavels-MacBook-Air k8s % kubectl exec pod/time-moscow-set-2 -- cat /app/data/visits
Defaulted container "time-moscow" out of: time-moscow, vault-agent, vault-agent-init (init)
15%
```

It's easy to see that the data is different across replicas. The reason is also simple: PV Claim is templated in Helm, so for each replica there's a separate PVC and thus a separate PV. It makes sense, because replicas are made for resiliency, if the data storage would've been shared, the failure of one replica would probably mean failure of all replicas.

## DNS resolution

```bash
> kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml

pod/dnsutils created
> kubectl get pods dnsutils

NAME       READY   STATUS              RESTARTS   AGE
dnsutils   0/1     ContainerCreating   0          6s
> kubectl exec -i -t dnsutils -- nslookup time-moscow-set-0.time-moscow-set
Server:		10.43.0.10
Address:	10.43.0.10#53

Name:	time-moscow-set-0.time-moscow-set.default.svc.cluster.local
Address: 10.42.0.107

> kubectl exec -i -t dnsutils -- nslookup time-moscow-set-1.time-moscow-set
Server:		10.43.0.10
Address:	10.43.0.10#53

Name:	time-moscow-set-1.time-moscow-set.default.svc.cluster.local
Address: 10.42.0.110

> kubectl exec -i -t dnsutils -- nslookup time-moscow-set-2.time-moscow-set
E0316 20:16:27.837192   20338 websocket.go:296] Unknown stream id 1, discarding message
                                                                                       Server:		10.43.0.10
Address:	10.43.0.10#53

Name:	time-moscow-set-2.time-moscow-set.default.svc.cluster.local
Address: 10.42.0.11
```

## Liveness and readiness probes

I already have them configured in the chart. My probes are querying the main page of the app, so basically if the probe is able to do it, the user can as well. Probing is critical for stateful apps because if you route the traffic to a stateful set, you need to consistently send the request to the same pod. If it's not in a healthy state, you cannot just kill and restart the pod, because the data might be lost. Redirecting the user's query to another replica (depending on the app architecture) might result in data inconsistency as well, so knowing the state of the pod is very important.

## Ordering guarantee

My app is not a clustered solution nor a microservices app, so it doesn't have to know about any other replica in the set, nor there are some special containers that have to start first (like a DB for example). `podManagementPolicy: Parallel` in spec is the way to instruct the StatefulSet controller to launch or terminate all Pods at the same time.

## Bonus Task

### Bonus app

I did not implement the statefulness in the bonus app so it doesn't really make any sense to convert it to a StatefulSet. I will skip this task.

### Update strategies

OnDelete - used for manual control over the update process. Set is replaced with a newer one when the old one is deleted.

RollingUpdate - used for automated updates. The new set is created, and the old one is deleted one by one.

Canaries - used for testing the new version of the app. A subset of pods is updated first, and if everything is fine, the rest of the pods are updated. `partition` in the spec is used to control the number of pods that are updated at the same time.

Deployment "rolling update" doesn't have any anchors to the order of updating the pods but rather resource constraints (e.g. `maxSurge` and `maxUnavailable`). The `Recreate` strategy is similar to the `OnDelete` strategy of the StatefulSet but drops all pods at once.
