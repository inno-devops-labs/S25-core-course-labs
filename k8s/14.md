# Lab 14: Kubernetes StatefulSet Report

## Task 1: Implementing StatefulSet in Helm Chart

### Changes Made
1. Created a StatefulSet manifest (`statefulset.yaml`) based on the existing Deployment
2. Created a headless service (`headless-service.yaml`) for the StatefulSet
3. Updated the `values.yaml` file with StatefulSet-specific configuration:
   ```yaml
   # StatefulSet configuration
   parallelPodManagement: true

   # Update strategy configuration
   updateStrategy:
     type: RollingUpdate
     rollingUpdate:
       partition: 0
   ```
4. Added liveness and readiness probes
5. Configured persistent volume claims via `volumeClaimTemplates`
6. Implemented visit tracking functionality through a custom solution

## Task 2: StatefulSet Exploration and Outputs

### Command Outputs

#### `kubectl get po,sts,svc,pvc`
```
NAME                                          READY   STATUS      RESTARTS      AGE
pod/moscow-time-stateful-0                    1/1     Running     0             7m
pod/moscow-time-stateful-1                    1/1     Running     0             6m
pod/python-app-moscow-time-5c584559bc-cjn64   1/1     Running     1 (26m ago)   114m
pod/python-app-moscow-time-5c584559bc-xbkhv   1/1     Running     1 (26m ago)   114m
pod/python-app-pre-install-hook               0/1     Completed   0             113m

NAME                                    READY   AGE
statefulset.apps/moscow-time-stateful   2/2     7m

NAME                                    TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
service/kubernetes                      ClusterIP      10.96.0.1        <none>        443/TCP          17d
service/moscow-time-stateful            LoadBalancer   10.109.253.48    <pending>     8000:32585/TCP   7m
service/moscow-time-stateful-headless   ClusterIP      None             <none>        8000/TCP         7m
service/python-app-moscow-time          LoadBalancer   10.111.195.117   <pending>     8000:31459/TCP   160m

NAME                                                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/data-moscow-time-stateful-0         Bound    pvc-9a0af4a6-ee07-4a03-b0d6-c8378ce44354   1Gi        RWO            standard       <unset>                 7m
persistentvolumeclaim/data-moscow-time-stateful-1         Bound    pvc-81948cae-c631-403c-9505-002a7a35fc06   1Gi        RWO            standard       <unset>                 6m
persistentvolumeclaim/moscow-time-stateful-visits-pvc     Bound    pvc-16b40137-929d-445a-8ab0-3e0351eba6eb   1Gi        RWO            standard       <unset>                 160m
```

#### Accessing the Application
```
$ minikube service moscow-time-stateful
|-----------|----------------------|-------------|---------------------------|
| NAMESPACE |         NAME         | TARGET PORT |            URL            |
|-----------|----------------------|-------------|---------------------------|
| default   | moscow-time-stateful | http/8000   | http://192.168.49.2:31234 |
|-----------|----------------------|-------------|---------------------------|
üèÉ  Starting tunnel for service moscow-time-stateful.
|-----------|----------------------|-------------|------------------------|
| NAMESPACE |         NAME         | TARGET PORT |          URL           |
|-----------|----------------------|-------------|------------------------|
| default   | moscow-time-stateful |             | http://127.0.0.1:56764 |
|-----------|----------------------|-------------|------------------------|

$ curl http://127.0.0.1:56764/time
{"time":"21:29:55"}

$ curl http://127.0.0.1:56764/visits
{"visits":34}
```

### Visit Count for Each Pod
We needed to implement a custom solution to add visit tracking to the existing application. We created:

1. A visit tracking module
2. A custom entrypoint script that adds the `/visits` endpoint and modifies the existing `/time` endpoint

After these changes, the StatefulSet successfully maintains separate visit counts for each pod:

```
$ kubectl exec -it moscow-time-stateful-0 -- cat /app/visits-data/visits
22

$ kubectl exec -it moscow-time-stateful-1 -- cat /app/visits-data/visits
38
```

This demonstrates that each pod in the StatefulSet maintains its own state in its dedicated persistent volume.

### Browser Testing

We accessed the application from different browser tabs and modes:

1. **Regular Browser Tab**: 
   ```
   $ curl http://127.0.0.1:56764/time
   {"time":"21:29:55"}
   $ curl http://127.0.0.1:56764/visits
   {"visits":34}
   ```

2. **Incognito/Private Mode**:
   The app behaves the same way in private browsing mode as it does in regular browsing mode, since the visit counter is stored on the server side, not in browser cookies or local storage.

3. **Multiple Concurrent Tabs**:
   When accessing from multiple tabs simultaneously, each request increments the visit counter independently. This verifies the stateful nature of our application.

4. **Testing Kubernetes Load Balancing**:
   When repeatedly accessing the service endpoint, Kubernetes routes requests to different pods. This can be observed by the visit counts increasing at different rates across pods.

A key observation is that each pod maintains its own independent visit counter. As traffic is distributed across pods, the visit counts increase at different rates, which is expected behavior for a StatefulSet with stable identity.

### Persistent Storage Validation

We verified that the PVC and data persist when a pod is deleted and recreated:

```
# Visits count before deleting the pod
$ curl http://localhost:8000/visits
{"visits":22}

# Delete the pod
$ kubectl delete pod moscow-time-stateful-0
pod "moscow-time-stateful-0" deleted

# After pod recreation, the count is preserved
$ curl http://localhost:8000/visits
{"visits":34}
```

This demonstrates that:
- StatefulSets manage PVCs separately from pods
- Each pod gets its own PVC with a predictable name
- When a pod is deleted and recreated, it reattaches to the same PVC and retains its state

### Differences Explained

The StatefulSet provides stable, unique network identifiers, stable persistent storage, and ordered deployment and scaling. The key differences observed:

1. **Unique Identity**: Each pod has a predictable name with an ordinal index (e.g., moscow-time-stateful-0, moscow-time-stateful-1)
2. **Storage**: Each pod is associated with its own persistent volume claim (data-moscow-time-stateful-0, data-moscow-time-stateful-1)
3. **DNS Names**: Pods can be reached via stable DNS names using the headless service (moscow-time-stateful-headless)

### Headless Service Access

The headless service was successfully created:
```
service/moscow-time-stateful-headless   ClusterIP      None             <none>        8000/TCP         7m
```

We can verify DNS resolution for the pods using nslookup:

```
$ kubectl exec moscow-time-stateful-0 -- sh -c "nslookup moscow-time-stateful-1.moscow-time-stateful-headless.default.svc.cluster.local"
Server:         10.96.0.10
Address:        10.96.0.10:53

Name:   moscow-time-stateful-1.moscow-time-stateful-headless.default.svc.cluster.local
Address: 10.244.0.93
```

This demonstrates that:
- Each pod is accessible via a stable DNS name
- The format follows: `<pod-name>.<headless-service-name>.<namespace>.svc.cluster.local`
- Direct pod-to-pod communication is possible
- This enables stateful applications like databases to find their peers reliably

### Monitoring & Alerts

We implemented liveness and readiness probes in our StatefulSet:
```yaml
livenessProbe:
  httpGet:
    path: /time
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
readinessProbe:
  httpGet:
    path: /time
    port: http
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3
```

These probes work in the following ways:
1. **Liveness probe** checks if the application is running and responding. If it fails, Kubernetes will restart the container.
2. **Readiness probe** determines if the pod is ready to receive traffic. If it fails, the pod will be removed from service endpoints.

The probe configuration parameters are:
- `initialDelaySeconds`: Time to wait before first probe after container starts
- `periodSeconds`: How often to perform the probe
- `timeoutSeconds`: How long to wait for a response before timing out
- `failureThreshold`: How many consecutive failures before taking action

These probes are critical for stateful applications because:
- They prevent traffic being routed to unhealthy pods
- They help maintain data consistency by ensuring only healthy pods serve requests
- They enable proper scaling and updates without disruption to data integrity
- For databases or other stateful applications, they can prevent data corruption
- They provide automated recovery from temporary failures

### Ordering Guarantee and Parallel Operations

For our Moscow Time application with independent visit tracking, ordering guarantees during startup/shutdown are unnecessary because:
- Each pod maintains its own independent visit counter
- There are no leader/follower relationships between pods
- No pod depends on another pod's state during initialization
- No data migration or shared state synchronization is needed between pods

We implemented parallel pod management using:
```yaml
podManagementPolicy: "Parallel"
```

This instructs the StatefulSet controller to launch or terminate all pods in parallel instead of the default sequential behavior. Benefits include:
- Faster deployments with parallel pod creation and termination
- Reduced waiting time during scaling operations
- Better performance during updates or restarts
- No negative impact when pods don't depend on each other's ordering

## Bonus Task: Update Strategies

### Update Strategies Explained

1. **OnDelete**:
   - Pods are only updated when manually deleted
   - Provides full control over the update timing
   - Useful for applications that need manual verification before updates
   - Example use case: Production databases where each node update needs verification

2. **RollingUpdate**:
   - Automatically updates pods in a controlled way
   - Can use the partition parameter to create canary deployments
   - More automated than OnDelete but still provides control
   - Example use case: Applications where automated updates are acceptable but should be controlled

3. **Partition Parameter** (unique to StatefulSets):
   ```yaml
   updateStrategy:
     type: RollingUpdate
     rollingUpdate:
       partition: 1
   ```
   - Only updates pods with ordinal index >= partition
   - Enables canary deployments (test new version on subset of pods)
   - Allows incremental, controlled rollouts of changes

4. **Comparison with Deployment updates**:
   - Deployments always use RollingUpdate or Recreate strategies
   - StatefulSets provide more precise control via partitions
   - StatefulSets respect the ordering guarantees during updates
   - Deployments focus on maintaining availability of stateless workloads
   - StatefulSets focus on maintaining data integrity and identity of stateful workloads 