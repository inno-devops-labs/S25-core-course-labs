# Lab 14: Kubernetes StatefulSet

---

## Task 1: Implement StatefulSet in Helm Chart

### **1.1 StatefulSet Helm Chart Implementation**
- Renamed `deployment.yaml` to `statefulset.yaml`.
- Implemented StatefulSet manifest based on Kubernetes best practices.
- Ran the following command to test the Helm chart:

  ```bash
  helm install --dry-run --debug python-app python-app
  ```
  
  ```
  nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ helm install --dry-run --debug python-app python-app
    install.go:225: 2025-03-16 21:51:39.099458309 +0300 MSK m=+0.028911741 [debug] Original chart version: ""
    install.go:242: 2025-03-16 21:51:39.099493509 +0300 MSK m=+0.028946522 [debug] CHART PATH: /home/nikolai/Documents/DevOpsLabs/S25-core-course-labs/k8s/python-app
    
    NAME: python-app
    LAST DEPLOYED: Sun Mar 16 21:51:39 2025
    NAMESPACE: default
    STATUS: pending-install
    REVISION: 1
    USER-SUPPLIED VALUES:
    {}
    
    COMPUTED VALUES:
    autoscaling:
      enabled: false
    environment:
      author: Nikoali01
      course: devops
      debug: false
    image:
      containerPort: 80
      pullPolicy: Always
      repository: nickwidbestie/region-time-api
      tag: latest
    ingress:
      enabled: false
    livenessProbe:
      httpGet:
        path: /time/moscow
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 10
    my-library:
      affinity: {}
      autoscaling:
        enabled: false
        maxReplicas: 100
        minReplicas: 1
        targetCPUUtilizationPercentage: 80
      fullnameOverride: ""
      global: {}
      image:
        pullPolicy: IfNotPresent
        repository: nginx
        tag: ""
      imagePullSecrets: []
      ingress:
        annotations: {}
        className: ""
        enabled: false
        hosts:
        - host: chart-example.local
          paths:
          - path: /
            pathType: ImplementationSpecific
        tls: []
      livenessProbe:
        httpGet:
          path: /
          port: http
      nameOverride: ""
      nodeSelector: {}
      podAnnotations: {}
      podLabels: {}
      podSecurityContext: {}
      readinessProbe:
        httpGet:
          path: /
          port: http
      replicaCount: 1
      resources: {}
      securityContext: {}
      service:
        port: 80
        type: ClusterIP
      serviceAccount:
        annotations: {}
        automount: true
        create: true
        name: ""
      tolerations: []
      volumeMounts: []
      volumes: []
    podManagementPolicy: Parallel
    readinessProbe:
      httpGet:
        path: /time/moscow
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 10
    replicaCount: 2
    resources:
      limits:
        cpu: 400m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 256Mi
    service:
      port: 80
      type: ClusterIP
    serviceAccount:
      annotations: {}
      automount: true
      create: true
      name: ""
    volumeClaimTemplates:
      name: python-app-data
      storage: 512Mi
    volumeMounts:
    - mountPath: /config.json
      name: config
      subPath: config.json
      - mountPath: /app/data
        name: python-app-data
      volumes:
      - configMap:
          name: python-app-config
        name: config
    
    HOOKS:
    ---
    # Source: python-app/templates/post-install-hook.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: "python-app-postinstall-hook"
      annotations:
        "helm.sh/hook": post-install
        "helm.sh/hook-delete-policy": hook-succeeded
    spec:
      containers:
      - name: post-install-container
        image: busybox
        imagePullPolicy: Always
        command: ['sh', '-c', 'echo The post-install hook is running && sleep 15' ]
      restartPolicy: Never
      terminationGracePeriodSeconds: 0
    ---
    # Source: python-app/templates/pre-install-hook.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: "python-app-preinstall-hook"
      annotations:
        "helm.sh/hook": pre-install
        "helm.sh/hook-delete-policy": hook-succeeded
    spec:
      containers:
      - name: pre-install-container
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c', 'echo The pre-install hook is running && sleep 20' ]
      restartPolicy: Never
      terminationGracePeriodSeconds: 0
    ---
    # Source: python-app/templates/tests/test-connection.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: "python-app-test-connection"
      labels:
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
      annotations:
        "helm.sh/hook": test
    spec:
      containers:
        - name: wget
          image: busybox
          command: ['wget']
          args: ['python-app:80']
      restartPolicy: Never
    MANIFEST:
    ---
    # Source: python-app/templates/serviceaccount.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: python-app
      labels:
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    automountServiceAccountToken: true
    ---
    # Source: python-app/templates/configmap.yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: python-app-config
    data:
      config.json: |
        
        {
          "stand": "dev",
          "isFalse": true
        }
    ---
    # Source: python-app/templates/service.yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: python-app
      labels:
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      type: ClusterIP
      ports:
        - port: 80
          targetPort: http
          protocol: TCP
          name: http
      selector:
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
    ---
    # Source: python-app/templates/statefulset.yaml
    apiVersion: apps/v1
    kind: StatefulSet
    metadata:
      name: python-app
      labels:
        app.kubernetes.io/name: "python-app"
        app.kubernetes.io/version: "0.1.0"
        app.kubernetes.io/instance: "python-app"
        app.kubernetes.io/part-of: "my-shared-library"
        helm.sh/chart: python-app-0.1.0
        app.kubernetes.io/name: python-app
        app.kubernetes.io/instance: python-app
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      podManagementPolicy: Parallel
      serviceName: python-app
      updateStrategy:
        type: RollingUpdate
        rollingUpdate:
          partition: 1
      replicas: 2
      selector:
        matchLabels:
          app.kubernetes.io/name: "python-app"
          app.kubernetes.io/version: "0.1.0"
          app.kubernetes.io/instance: "python-app"
          app.kubernetes.io/part-of: "my-shared-library"
          app.kubernetes.io/name: python-app
          app.kubernetes.io/instance: python-app
      volumeClaimTemplates:
        - metadata:
            name: python-app-data
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 512Mi
      template:
        metadata:
          labels:
            helm.sh/chart: python-app-0.1.0
            app.kubernetes.io/name: python-app
            app.kubernetes.io/instance: python-app
            app.kubernetes.io/version: "1.16.0"
            app.kubernetes.io/managed-by: Helm
            app.kubernetes.io/name: "python-app"
            app.kubernetes.io/version: "0.1.0"
            app.kubernetes.io/instance: "python-app"
            app.kubernetes.io/part-of: "my-shared-library"
        spec:
          serviceAccountName: python-app
          containers:
            - name: python-app
              image: "nickwidbestie/region-time-api:latest"
              imagePullPolicy: Always
              env:
                # Example of environment variables from secrets
                # - name: GPT_KEY
                #   valueFrom:
                #     secretKeyRef:
                #       name: credentials
                #       key: CHATGPT_KEY
                - name: APP_AUTHOR
                  value: "Nikoali01"
                - name: APP_DEBUG
                  value: "false"
                - name: APP_COURSE
                  value: "devops"
              envFrom:
                - configMapRef:
                    name: python-app-config
              ports:
                - name: http
                  containerPort: 80
                  protocol: TCP
              livenessProbe:
                httpGet:
                  path: /time/moscow
                  port: 80
                initialDelaySeconds: 5
                periodSeconds: 10
              readinessProbe:
                httpGet:
                  path: /time/moscow
                  port: 80
                initialDelaySeconds: 5
                periodSeconds: 10
              resources:
                limits:
                  cpu: 400m
                  memory: 256Mi
                requests:
                  cpu: 100m
                  memory: 256Mi
              volumeMounts:
                - mountPath: /config.json
                  name: config
                  subPath: config.json
                - mountPath: /app/data
                  name: python-app-data
          volumes:
            - configMap:
                name: python-app-config
              name: config
    
    NOTES:
    1. Get the application URL by running these commands:
      export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=python-app,app.kubernetes.io/instance=python-app" -o jsonpath="{.items[0].metadata.name}")
      export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
      echo "Visit http://127.0.0.1:8080 to use your application"
      kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
  ```
- Successfully resolved all issues in the Helm chart.
- Deployment completed without errors.

---

## **1.2 Helm Chart Best Practices**
- Extracted configuration parameters into `values.yaml`.
- Ensured proper use of Helm templates and overrides for maintainability.

---

## **Task 2: StatefulSet Exploration and Optimization**

### **2.1 Resource Verification**
Executed the following command to verify resources:

```bash
kubectl get po,sts,svc,pvc
```
```commandline
nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ kubectl get pod,sts,svc,pvc
NAME                                        READY   STATUS    RESTARTS       AGE
pod/java-java-app-7f879bcb8c-9wts4          1/1     Running   0              78m
pod/python-app-0                            1/1     Running   0              59s
pod/python-app-1                            1/1     Running   0              59s
pod/python-app-java-app-7fb79dcd87-cnkj5    1/1     Running   0              92m
pod/vault-0                                 1/1     Running   8 (128m ago)   7d5h
pod/vault-agent-injector-66f45b5fd5-g8nrx   1/1     Running   9 (128m ago)   7d5h

NAME                          READY   AGE
statefulset.apps/python-app   2/2     59s
statefulset.apps/vault        1/1     7d5h

NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/java-java-app              ClusterIP   10.105.31.19     <none>        8081/TCP            78m
service/kubernetes                 ClusterIP   10.96.0.1        <none>        443/TCP             14d
service/python-app                 ClusterIP   10.102.21.163    <none>        80/TCP              60s
service/python-app-java-app        ClusterIP   10.104.117.178   <none>        8081/TCP            92m
service/vault                      ClusterIP   10.108.135.221   <none>        8200/TCP,8201/TCP   7d5h
service/vault-agent-injector-svc   ClusterIP   10.97.40.227     <none>        443/TCP             7d5h
service/vault-internal             ClusterIP   None             <none>        8200/TCP,8201/TCP   7d5h

NAME                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/python-app-data-python-app-0   Bound    pvc-d377a0b9-3fc1-42e4-a76f-dda820cc549c   512Mi      RWO            standard       <unset>                 59s
persistentvolumeclaim/python-app-data-python-app-1   Bound    pvc-2501275c-b6b3-4a24-8170-766fcc3f5092   512Mi      RWO            standard       <unset>                 59s
```

## 2. Accessing the App
1. **Minikube Service**  
    ```bash
    minikube service python-app
    ```
2. **Browser Tests**
   - Open multiple tabs or use different browsers to ensure each Pod might respond differently if they store data locally.
   - If each Pod increments a local file (like /data/visits.txt), you’ll see distinct counters.

## 3. Checking File Content in Each Pod 
Each Pod in the StatefulSet has its own PersistentVolumeClaim. Confirm it:
```
nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ kubectl exec python-app-0 -- cat /app/data/visits.txt
16                                                                                                                                              
nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ kubectl exec python-app-1 -- cat /app/data/visits.txt
23
```
Each Pod has its own volume, so Pod 0 and Pod 1 maintain separate data. If Pod 0 has been accessed more often, its `visits` count will be higher than Pod 1’s.

## 4. Persistent Storage Validation
1. **Delete a Pod**
   ```bash
   kubectl delete pod python-app-1
   ```
2. **Check the PVC**
   ```bash
   kubectl get pvc
   ```
   **Output:**
```commandline
nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ kubectl delete pod python-app-1
pod "python-app-1" deleted
nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ kubectl get pvc
NAME                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
python-app-data-python-app-0   Bound    pvc-d377a0b9-3fc1-42e4-a76f-dda820cc549c   512Mi      RWO            standard       <unset>                 12m
python-app-data-python-app-1   Bound    pvc-2501275c-b6b3-4a24-8170-766fcc3f5092   512Mi      RWO            standard       <unset>                 12m
   ```

3. **New Pod Reuse**
   Once `python-app-1` restarts, check again:
   ```bash
   kubectl exec python-app-1 -- cat /app/data/visits.txt
   ```
   Output:
   ```
    nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ kubectl exec python-app-1 -- cat /app/data/visits.txt
    23  
   ```

## 5. Headless Service & DNS
1. **Headless Service**  
   Our chart uses `serviceName: python-app` in the StatefulSet.
2. **DNS Resolution**
   - Each Pod is reachable:
     ```bash
     kubectl exec python-app-0 -- nslookup python-app-1.python-app.default.svc.cluster.local
     kubectl exec python-app-1 -- nslookup python-app-0.python-app.default.svc.cluster.local
     ```
     **Output:**
   ```
   nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ kubectl exec python-app-0 -- nslookup python-app-1.python-app.default.svc.cluster.localal
    kubectl exec python-app-1 -- nslookup python-app-0.python-app.default.svc.cluster.local
    Server:		10.96.0.10
    Address:	10.96.0.10:53
    
    
    Name:	python-app-1.python-app.default.svc.cluster.local
    Address: 10.244.1.12
    
    Server:		10.96.0.10
    Address:	10.96.0.10:53
    
    Name:	python-app-0.python-app.default.svc.cluster.local
    Address: 10.244.1.10
    ```
## **6. Monitoring & Alerts**  
Implemented **livenessProbe** and **readinessProbe** configurations in `values.yaml` for better pod health monitoring.  

### **Role of Probes in Pod Health Maintenance**  

Kubernetes relies on two primary types of health checks—**liveness** and **readiness**—to monitor and sustain the proper functioning of pods:

1. **Liveness Probe**  
   - Regularly verifies whether the container is still active by, for instance, sending a request to a health endpoint.  
   - If the probe fails or the container becomes unresponsive, Kubernetes automatically restarts the container.  
   - This helps prevent scenarios where a pod is stuck in a broken state without intervention.

2. **Readiness Probe**  
   - Assesses whether the application inside the pod is prepared to accept traffic.  
   - Until the pod successfully passes the probe, it will not receive requests from the Service.  
   - This is particularly important for **stateful applications**, where a pod may need time to reattach storage, load necessary data, or complete setup processes before it can start handling requests.

### **Importance of Probes for Stateful Applications**  

- **Ensuring Data Integrity**: Readiness probes prevent traffic from being routed to a pod before it has successfully attached its persistent storage and initialized completely.  
- **Self-Healing Mechanism**: If an application crashes or becomes unresponsive due to issues such as memory leaks or database errors, the liveness probe detects this and triggers a restart, reducing downtime.  
- **Increased Reliability**: In a multi-pod environment, these probes allow the rest of the StatefulSet to function correctly while an unhealthy pod undergoes recovery, ensuring continuous availability of services.

---

## **7. Controlling Pod Ordering & Parallel Operations**  

By default, **StatefulSets follow a strict startup sequence**, meaning pods are launched one after the other in a specific order (e.g., `pod-0` starts first, followed by `pod-1`). Similarly, termination follows the reverse order. This sequencing is necessary for applications that rely on specific startup dependencies, such as a database cluster requiring initialization in a defined sequence.

However, **for applications that don’t require strict startup dependencies**, we can enable parallel pod operations using the following configuration:

```yaml
podManagementPolicy: Parallel
```

**Why Ordering Guarantees Are Unnecessary for My App**

- My app doesn’t require a specialized handshake or leader-election process that demands Pod 0 be up before Pod 1.
- Each Pod is self-sufficient, simply mounting its own volume and running independently.
- Parallel operations reduce deployment time by creating or terminating Pods simultaneously.

## Bonus Task

### Updating Java Application
**Output:**
```
nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ helm install java-app java-app
NAME: java-app
LAST DEPLOYED: Sun Mar 16 22:11:00 2025
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
1. Get the application URL by running these commands:
  export POD_NAME=$(kubectl get pods --namespace default -l "app.kubernetes.io/name=java-app,app.kubernetes.io/instance=java-app" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT
nikolai@nikolai-Yoga-Slim-7-Pro-14ACH5:~/Documents/DevOpsLabs/S25-core-course-labs/k8s$ kubectl get pod,sts,svc,pvc
NAME                                        READY   STATUS    RESTARTS       AGE
pod/java-app-0                              1/1     Running   0              46s
pod/java-java-app-7f879bcb8c-9wts4          1/1     Running   0              96m
pod/python-app-0                            1/1     Running   0              8m14s
pod/python-app-1                            1/1     Running   0              6m21s
pod/python-app-java-app-7fb79dcd87-cnkj5    1/1     Running   0              110m
pod/vault-0                                 1/1     Running   8 (146m ago)   7d5h
pod/vault-agent-injector-66f45b5fd5-g8nrx   1/1     Running   9 (146m ago)   7d5h

NAME                          READY   AGE
statefulset.apps/java-app     1/1     46s
statefulset.apps/python-app   2/2     8m14s
statefulset.apps/vault        1/1     7d5h

NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/java-app                   ClusterIP   10.98.180.22     <none>        8081/TCP            46s
service/java-java-app              ClusterIP   10.105.31.19     <none>        8081/TCP            96m
service/kubernetes                 ClusterIP   10.96.0.1        <none>        443/TCP             14d
service/python-app                 ClusterIP   10.111.111.241   <none>        80/TCP              8m14s
service/python-app-java-app        ClusterIP   10.104.117.178   <none>        8081/TCP            110m
service/vault                      ClusterIP   10.108.135.221   <none>        8200/TCP,8201/TCP   7d5h
service/vault-agent-injector-svc   ClusterIP   10.97.40.227     <none>        443/TCP             7d5h
service/vault-internal             ClusterIP   None             <none>        8200/TCP,8201/TCP   7d5h

NAME                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/java-app-data-java-app-0       Bound    pvc-b29ecde4-1b78-424c-a5e9-f8b127812445   512Mi      RWO            standard       <unset>                 46s
persistentvolumeclaim/python-app-data-python-app-0   Bound    pvc-d377a0b9-3fc1-42e4-a76f-dda820cc549c   512Mi      RWO            standard       <unset>                 18m
persistentvolumeclaim/python-app-data-python-app-1   Bound    pvc-2501275c-b6b3-4a24-8170-766fcc3f5092   512Mi      RWO            standard       <unset>                 18m
```

## **Exploring StatefulSet Update Strategies**

When managing updates for a **StatefulSet**, you can select between two primary strategies:

1. **OnDelete**  
   - The StatefulSet controller does **not** automatically upgrade or restart pods.  
   - Each pod must be removed by hand to trigger an update.  
   - Useful for applications requiring deliberate, step-by-step modifications, such as databases that demand specific migration steps.

2. **RollingUpdate**  
   - The StatefulSet controller updates pods automatically, respecting the order defined by the StatefulSet—unless you configure `podManagementPolicy: Parallel` to update pods simultaneously.  
   - The `partition` field allows partial, canary-style deployments. For instance, you can keep a certain number of pods running the old version while updating the rest. Once satisfied with the new version, you adjust the `partition` to zero and update all remaining pods.

---

## **Comparison to Deployment Update Strategies**

### **Deployment Approaches**
- **RollingUpdate**: The default method for Deployments, where new pods are created and old pods are gradually removed, ensuring limited downtime and maintaining availability.
- **Recreate**: All existing pods are terminated before creating new ones. This can simplify updates at the cost of temporary downtime.

### **StatefulSet Approaches**
- **OnDelete**: Updates happen only when a pod is manually deleted, allowing you to maintain total control over the update process.
- **RollingUpdate**: Allows an automatic, sequential or parallel rollout, but each pod retains a **stable identity** and any associated volumes remain attached.  
  - Leveraging **partition** lets you perform partial updates (canaries) before rolling changes out to every pod.

---

## **Key Distinctions**

- **Stable Pod Identity**:  
  - **StatefulSets** assign consistent pod names (e.g., `python-app-0`, `python-app-1`).  
  - **Deployments** rely on ReplicaSets with ephemeral pod names.

- **Volume Handling**:  
  - **StatefulSets** typically define `volumeClaimTemplates`, so each pod automatically gets a unique persistent volume.  
  - **Deployments** often handle stateless services or reference external volumes and do not, by default, attach unique volumes per pod.

- **Update Semantics**:  
  - **Deployments** can seamlessly roll out or fully recreate all pods for updates.  
  - **StatefulSets** support a managed approach: either manual (**OnDelete**) or orderly/parallel (**RollingUpdate**) while preserving pod identity and storage.

In summary, **Deployments** suit applications where pod identity and persistent data are not concerns. **StatefulSets**, however, deliver stable networking identities and persistent storage—vital features for stateful workloads—while still providing rolling updates or manual updates to safeguard data integrity.
