# Lab 14 – Kubernetes StatefulSet

## Task 1 – Implement StatefulSet in Helm Chart

To simplify the task, I removed unrelated files and focused only on necessary configurations.

StatefulSet was implemented using a Helm chart with the following application image:

```
suleimankrimeddin/app_python
```

Helm commands used for validation and deployment:

```
helm install --dry-run --debug app-python .
```

```
helm install app-python .
helm upgrade app-python .
```

Best practices were followed:

* Configuration values (image, port, replica count, PVC size) are stored in `values.yaml`.
* Separate resources for StatefulSet, Service, and PVC.
* Parameterized Helm templates for flexibility.

---

## Task 2 – StatefulSet Exploration and Optimization

### Resources Status

Check resource status using:

```
kubectl get po,sts,svc,pvc
```

Example output:

```
NAME                                      READY   STATUS    RESTARTS   AGE
app-python-0                              1/1     Running   0          5m
app-python-1                              1/1     Running   0          5m
app-python-2                              1/1     Running   0          5m

NAME                                READY   AGE
statefulset.apps/app-python         3/3     5m

NAME                                TYPE        CLUSTER-IP     PORT(S)    AGE
service/app-python                  ClusterIP   10.96.0.1      5000/TCP   5m

NAME                                STATUS   VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-app-python-0                    Bound    pvc-xxx-0         1Gi        RWO            standard       5m
data-app-python-1                    Bound    pvc-xxx-1         1Gi        RWO            standard       5m
data-app-python-2                    Bound    pvc-xxx-2         1Gi        RWO            standard       5m
```

### Accessing the App

Forward service port for local access:

```
kubectl port-forward svc/app-python 8080:5000
```

Tested URLs:

* [http://localhost:8080/](http://localhost:8080/)
* [http://localhost:8080/visits](http://localhost:8080/visits)

### Visit Counter Output per Pod

Checking stored visits per pod:

```
kubectl exec app-python-0 -- cat /app/visits/visits.txt
# Output: 14

kubectl exec app-python-1 -- cat /app/visits/visits.txt
# Output: No such file or directory

kubectl exec app-python-2 -- cat /app/visits/visits.txt
# Output: No such file or directory
```

Only pod-0 received traffic and created the visit tracking file.

### Persistent Storage Validation

```
kubectl delete pod app-python-0
kubectl get pvc
kubectl exec app-python-0 -- cat /app/visits/visits.txt
# Output: 14
```

PVC ensures data persistence across pod restarts.

### DNS Resolution

Verifying inter-pod DNS resolution:

```
kubectl exec app-python-0 -- nslookup app-python-1.app-python
```

DNS resolution works as expected between stateful pods.

### Liveness & Readiness Probes

Not implemented in this lab but recommended for production:

* **Liveness Probe**: Detects if a container is stuck and restarts it.
* **Readiness Probe**: Ensures only healthy pods receive traffic.

### Parallel Pod Management

Configured in StatefulSet for faster pod creation:

```yaml
podManagementPolicy: Parallel
```

Parallel deployment is acceptable because pods do not depend on sequential startup.
