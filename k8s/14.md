## Research and documentation
```bash
kubectl get po,sts,svc,pvc
```
```bash
NAME               READY   STATUS    RESTARTS   AGE
pod/python-app-0   1/1     Running   0          38s
pod/python-app-1   1/1     Running   0          38s
pod/python-app-2   1/1     Running   0          38s

NAME                          READY   AGE
statefulset.apps/python-app   3/3     38s

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   56m
service/python-app   ClusterIP   10.106.6.205   <none>        80/TCP    38s

NAME                                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/python-moskov-time-volume-mount-python-app-0   Bound    pvc-f02dc011-3af0-4dca-94f2-82a907ab0ce4   100Mi      RWO            standard       <unset>                 38s
persistentvolumeclaim/python-moskov-time-volume-mount-python-app-1   Bound    pvc-60b8d5d8-1d8a-4b1d-9fc7-3e5323401aa0   100Mi      RWO            standard       <unset>                 38s
persistentvolumeclaim/python-moskov-time-volume-mount-python-app-2   Bound    pvc-3766b8b2-e55d-4ebd-93cc-a963ad11dbbb   100Mi      RWO            standard       <unset>                 38s
```

```bash
minikube service python-app
```
```bash
|-----------|------------|-------------|--------------|
| NAMESPACE |    NAME    | TARGET PORT |     URL      |
|-----------|------------|-------------|--------------|
| default   | python-app |             | No node port |
|-----------|------------|-------------|--------------|
üòø  service default/python-app has no node port
‚ùóÔ∏è  Services [default/python-app] have type "ClusterIP" not meant to be exposed, however for local development minikube allows you to access this !
üèÉ  Starting tunnel for service python-app.
|-----------|------------|-------------|------------------------|
| NAMESPACE |    NAME    | TARGET PORT |          URL           |
|-----------|------------|-------------|------------------------|
| default   | python-app |             | http://127.0.0.1:34645 |
|-----------|------------|-------------|------------------------|
üéâ  Opening service default/python-app in default browser...
‚ùóÔ∏è  Because you are using a Docker driver on linux, the terminal needs to be open to run it.
Gtk-Message: 21:30:03.929: Not loading module "atk-bridge": The functionality is provided by GTK natively. Please try to not load it.
^C‚úã  Stopping tunnel for service python-app.
```

```bash
kubectl exec pod/python-app-0 -- cat /tmp/visits.txt
```
```bash
10
```

```bash
kubectl exec pod/python-app-1 -- cat /tmp/visits.txt
```
```bash
9
```

```bash
kubectl exec pod/python-app-2 -- cat /tmp/visits.txt
```
```bash
11
```

The results are different because the files for the pods are different, thus due to the load balancer doing a mighty fine
job, the number of requests is spread among the pods mostly equally (run several times, this is just the last one).

But, as seen from there, the /data/visits.txt as a location is consistent between the pods and consistent if we delete,
which is our next task.

## Persistent Storage Validation
```bash
kubectl delete pod python-app-0
```
```bash
pod "python-app-0" deleted
```

```bash
kubectl get pvc
```
```bash
NAME                                           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
python-moskov-time-volume-mount-python-app-0   Bound    pvc-f02dc011-3af0-4dca-94f2-82a907ab0ce4   100Mi      RWO            standard       <unset>                 21m
python-moskov-time-volume-mount-python-app-1   Bound    pvc-60b8d5d8-1d8a-4b1d-9fc7-3e5323401aa0   100Mi      RWO            standard       <unset>                 21m
python-moskov-time-volume-mount-python-app-2   Bound    pvc-3766b8b2-e55d-4ebd-93cc-a963ad11dbbb   100Mi      RWO            standard       <unset>                 21m
```

As seen the volume is still there.

```bash
kubectl exec python-app-0 -- cat /data/visits.txt
```
```bash
17
```

The number did not reset, so the data persists.

## Headless Service Access

```bash
kubectl exec python-app-0 -- nslookup python-app-1.python-app.default.svc.cluster.local
```
```bash
Server:    10.96.0.10
Address:  10.96.0.10:53


Name:  python-app-1.python-app.default.svc.cluster.local
Address: 10.244.0.9
```

```bash
kubectl exec python-app-1 -- nslookup python-app-0.python-app.default.svc.cluster.local
```
```bash
Server:    10.96.0.10
Address:  10.96.0.10:53


Name:  python-app-0.python-app.default.svc.cluster.local
Address: 10.244.0.11
```

This command proves that DNS works (even though this is a different command, but it works unlike the original).

## Monitoring & Alerts

Well, liveliness and readiness probes are just checks by the management system (kubelet) that check that the container is 
alive and ready for connections. These probes are actually why my number of visits is higher later: there were requests.

This is incredible, because if the container is down for some reason, it will be evaluated as down and promptly restored
to working order (by simply restarting usually). So, if a container is not doing any job, or failed, it will be updated.

Now, since the probes are different, we can communicate with the load balancing by manually disabling the handler for
readiness, meaning that the overloaded pod can work on it's task and the others will get their tasks in turn.

And for stateful apps it seems like the only way to check that states are still being alive, correct, that all parts are working.
This is simply something that is necessary for them, hence critical.

## Ordering Guarantee and Parallel Operations

If there are pods that need to communicate with each other (master-slave for example) then once of the pods must 
be started before the other. There isn't such a logic in my app, so it's unnecessary here.

To run all pods in parallel, I use `podManagementPolicy: Parallel`.

## Some part of extra

It's sad that I did not have the time to keep the PHP version for the bonus tasks because it would have been rather 
easy to do. I am sorry, I did not manage to make it work on one of the previous labs.

OnDelete is a strategy that creates a new pod from an updated template whenever the previous pod is manually deleted.

RollingUpdate is a strategy that updates all the pods with `rollingUpdate.partition` higher than the one that is gone now.