# StatefulSet

## Task 1

```bash
$ helm install --dry-run --debug app-python app-python/
install.go:225: 2025-03-15 10:16:49.085591144 +0300 MSK m=+0.752780656 [debug] Original chart version: ""
install.go:242: 2025-03-15 10:16:49.086015651 +0300 MSK m=+0.753205149 [debug] CHART PATH: /home/justcgh9/projects/pet/DevOps/S25-core-course-labs/k8s/app-python

NAME: app-python
LAST DEPLOYED: Sat Mar 15 10:16:49 2025
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
chart-library:
  global: {}
fullnameOverride: ""
image:
  repository: justcgh/moscow-time-app
  tag: latest
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
nameOverride: ""
nodeSelector: {}
podAnnotations:
  vault.hashicorp.com/agent-inject: "true"
  vault.hashicorp.com/agent-inject-template-database-config.txt: |
    {{- with secret "internal/data/database/config" -}}
    postgresql://{{ .Data.data.username }}:{{ .Data.data.password }}@postgres:5432/wizard
    {{- end -}}
  vault.hashicorp.com/role: app-python
podLabels: {}
podSecurityContext: {}
replicaCount: 3
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 128Mi
securityContext: {}
service:
  port: 8081
  type: NodePort
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: ""
tolerations: []
volumeClaimTemplates:
- metadata:
    name: app-python-volume
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 100M
volumeMounts:
- mountPath: /app/data/config.json
  name: app-python-config-volume
  subPath: config.json
- mountPath: /app/data
  name: app-python-volume
volumes:
- configMap:
    name: app-python-config
  name: app-python-config-volume

HOOKS:
---
# Source: app-python/templates/post-install-hook.yaml
apiVersion: v1
kind: Pod
metadata:
   name: postinstall-hook
   annotations:
       "helm.sh/hook": "post-install"
       "helm.sh/hook-delete-policy": hook-succeeded
spec:
  containers:
  - name: post-install-container
    image: busybox
    imagePullPolicy: Always
    command: ['sh', '-c', 'echo The post-install hook is running && sleep 15' ]
  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: app-python/templates/pre-install-hook.yaml
apiVersion: v1
kind: Pod
metadata:
   name: preinstall-hook
   annotations:
       "helm.sh/hook": "pre-install"
       "helm.sh/hook-delete-policy": hook-succeeded
spec:
  containers:
  - name: pre-install-container
    image: busybox
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'echo The pre-install hook is running && sleep 20' ]
  restartPolicy: Never
  terminationGracePeriodSeconds: 0
---
# Source: app-python/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "app-python-test-connection"
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: app-python
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['app-python:8081']
  restartPolicy: Never
MANIFEST:
---
# Source: app-python/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-python
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: app-python
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: app-python/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-python-config
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: app-python
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
data:
  config.json: |-
    {
        "foo": "bar"
    }
---
# Source: app-python/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: app-python
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: app-python
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 8081
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: app-python
---
# Source: app-python/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: app-python
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: app-python
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 3
  podManagementPolicy: "Parallel"
  selector:
    matchLabels:
      app.kubernetes.io/name: app-python
      app.kubernetes.io/instance: app-python
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/agent-inject-template-database-config.txt: |
          {{- with secret "internal/data/database/config" -}}
          postgresql://{{ .Data.data.username }}:{{ .Data.data.password }}@postgres:5432/wizard
          {{- end -}}
        vault.hashicorp.com/role: app-python
      labels:
        helm.sh/chart: app-python-0.1.0
        app.kubernetes.io/name: app-python
        app.kubernetes.io/instance: app-python
        app.kubernetes.io/version: "latest"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: app-python
      containers:
        - name: app-python
          image: "justcgh/moscow-time-app:latest"
          imagePullPolicy: 
          env:
          - name: HELLO
            value: "world"
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 128Mi
          volumeMounts:
            - mountPath: /app/data/config.json
              name: app-python-config-volume
              subPath: config.json
            - mountPath: /app/data
              name: app-python-volume
      volumes:
        - configMap:
            name: app-python-config
          name: app-python-config-volume

  volumeClaimTemplates:
    - metadata:
        name: app-python-volume
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 100M

NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services app-python)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
```

## Task 2

Here is the output of the required command. As you may see, all the instances are up and running (at this poing the service type is yet node port)

```bash
$ kubectl get po,sts,svc,pvc
NAME                                 READY   STATUS    RESTARTS   AGE
pod/app-python-0                     1/1     Running   0          56s
pod/app-python-1                     1/1     Running   0          56s
pod/app-python-2                     1/1     Running   0          56s

NAME                          READY   AGE
statefulset.apps/app-python   3/3     56s

NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/app-python            NodePort    10.111.82.108   <none>        8081:32689/TCP   56s
service/kubernetes            ClusterIP   10.96.0.1       <none>        443/TCP          5d10h

NAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/app-python-volume-app-python-0   Bound    pvc-a542236b-ed37-4e7f-8fd3-e0609e61cb49   100M       RWO            standard       <unset>                 39m
persistentvolumeclaim/app-python-volume-app-python-1   Bound    pvc-a37fbc47-cae9-44a3-a3f8-2628668b95f3   100M       RWO            standard       <unset>                 39m
persistentvolumeclaim/app-python-volume-app-python-2   Bound    pvc-72baaf48-27df-48f6-897d-3cf2d2d3701a   100M       RWO            standard       <unset>                 39m
```

---

As I used node port in this part, I have enabled the `ingress addon` and started the `minikube tunnel` in the background, then randomly accessed the service

```bash
$ kubectl exec pod/app-python-0 -- cat data/visits.txt
19

$ kubectl exec pod/app-python-1 -- cat data/visits.txt
25

$ kubectl exec pod/app-python-2 -- cat data/visits.txt
6
```

Each one of the pods running uses its own persistence volume. When we try to access the service, the load balancer sends us to one of the running pods, so the counter is only updated for that pod. If we want to see the same number everywhere, we need to add some sync.

```bash
$ kubectl delete pod/app-python-2 
pod "app-python-2" deleted

$ kubectl get pvc
NAME                             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
app-python-volume-app-python-0   Bound    pvc-407d7a83-1d90-4301-9f33-1534ef1fbcac   100M       RWO            standard       <unset>                 36m
app-python-volume-app-python-1   Bound    pvc-a8384166-2dea-4fce-9f2f-43ad2c5ce678   100M       RWO            standard       <unset>                 36m
app-python-volume-app-python-2   Bound    pvc-4409a45c-6a90-4c6b-97a9-58d9c266f5f4   100M       RWO            standard       <unset>                 36m

$ kubectl exec pod/app-python-2 -- cat data/visits.txt
6
```

As you may see the storage is persistent, and the value has not changed after deletion of the pod

---

The next part is **DNS lookup**. For this one, I already had to change the service type to `ClusterIP`, and then (since my image does not have `dnsutils` installed) I installed it manually for this test:

```bash
$ kubectl exec app-python-0 -- nslookup app-python-1.app-python.default.svc.cluster.local
;; Got recursion not available from 10.96.0.10
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   app-python-1.app-python.default.svc.cluster.local
Address: 10.244.5.61
```

---

I have added liveness and readiness probes into my stateful sets:

values.yaml:

```yaml
livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http
```

statefulset.yaml:

```yaml
{{- with .Values.livenessProbe }}
livenessProbe:
{{- toYaml . | nindent 12 }}
{{- end }}
{{- with .Values.readinessProbe }}
readinessProbe:
{{- toYaml . | nindent 12 }}
{{- end }}
```

Liveness probes check the health of a container by sending a request to the root (/) path and expecting a 200 status code in response. If the container fails to return this status, it is restarted.

In a StatefulSet, liveness probes play a crucial role in directing traffic only to functional pods. Routing requests to an unhealthy pod could result in data loss or corruption, potentially causing an inconsistent system state.

---

Ordering guarantees are not needed for my applications since each pod maintains its own visit count on its dedicated persistent volume without relying on the state of other pods.

To enable parallel operations in the StatefulSet, I included the following line in the StatefulSet specification:

```yaml
podManagementPolicy: Parallel
```

With parallel pod management enabled, pods can be created or removed independently without waiting for others, allowing for faster scaling and recovery.

## Bonus Task

```bash
$ kubectl get po,sts,svc,pvc
NAME                                 READY   STATUS    RESTARTS   AGE
pod/app-go-0                         1/1     Running   0          2m11s
pod/app-go-1                         1/1     Running   0          2m1s
pod/app-go-2                         1/1     Running   0          103s
pod/app-python-0                     1/1     Running   0          3h43m
pod/app-python-1                     1/1     Running   0          3h43m
pod/app-python-2                     1/1     Running   0          3h43m

NAME                          READY   AGE
statefulset.apps/app-go       3/3     2m11s
statefulset.apps/app-python   3/3     3h43m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
service/app-go       ClusterIP   None         <none>        8080/TCP   2m11s
service/app-python   ClusterIP   None         <none>        8081/TCP   3h43m
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP    6d3h

NAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/app-go-volume-app-go-0           Bound    pvc-84caaa4d-2e65-4666-bdb8-45cb8560301a   1Gi        RWO            standard       <unset>                 9m8s
persistentvolumeclaim/app-go-volume-app-go-1           Bound    pvc-4e3a6d92-a2b5-4408-8806-3c4d35eaf6ee   1Gi        RWO            standard       <unset>                 2m3s
persistentvolumeclaim/app-go-volume-app-go-2           Bound    pvc-b9a1c837-f9f2-423a-b58c-5b0c3b74e1e5   1Gi        RWO            standard       <unset>                 104s
persistentvolumeclaim/app-python-volume-app-python-0   Bound    pvc-407d7a83-1d90-4301-9f33-1534ef1fbcac   100M       RWO            standard       <unset>                 5h29m
persistentvolumeclaim/app-python-volume-app-python-1   Bound    pvc-a8384166-2dea-4fce-9f2f-43ad2c5ce678   100M       RWO            standard       <unset>                 5h29m
persistentvolumeclaim/app-python-volume-app-python-2   Bound    pvc-4409a45c-6a90-4c6b-97a9-58d9c266f5f4   100M       RWO            standard       <unset>                 5h29m
```

I modified the `statefulset.yaml` for both applications by adding the following lines to configure the update strategy:  

```yaml
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    partition: 1
```

### Update Strategies in a StatefulSet  

#### **OnDelete**  

- Pods are updated only when manually deleted.  

pod/go-app-app-go-6fbdf9b65d-7924v   1/1     Running   1          18h

##### **OnDelete Use Cases:**  

- When manual control over the update process is required.  
- In environments where automatic updates are restricted.  
- When ensuring that only specific pods receive updates at a given time.  

#### **RollingUpdate**  

- Pods are automatically updated one at a time.  
- The `partition` field determines which pods are updated simultaneously. If set, only pods with an ordinal greater than or equal to the partition value will be updated.  
- Maintains high availability during updates.  
- If a new pod fails, the old one remains running, preventing downtime.  

##### **RollingUpdate Use Cases:**  

- When uninterrupted availability is essential.  
- When automatic updates are preferred without manual intervention.  
- When limiting the number of simultaneous updates is necessary to minimize risk.
