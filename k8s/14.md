# Lab 14: StatefulSet

*Timur Zheksimbaev t.zheksimbaev@innopolis.university CBS-01*

## 1. Implementation Results

### Converting Deployment to StatefulSet

I successfully converted the deployment to a StatefulSet by:
1. Renaming `deployment.yml` to `statefulset.yml`
2. Updating the API object to `StatefulSet`
3. Adding required configurations (serviceName, volumeClaimTemplates)
4. Creating a headless service for StatefulSet
5. Modifying the Python application to use persistent storage

### Current Environment State

Based on the current state of the Kubernetes cluster:

```
$ kubectl get po,sts,svc,pvc
NAME READY STATUS RESTARTS AGE
pod/helm-hooks-python-app-b6958b96c-sg9dp 1/1 Running 4 (104m ago) 16d
pod/my-python-app-64cf657b4f-gv679 1/1 Running 4 (104m ago) 16d
pod/python-moscow-time-689648c449-6876c 1/1 Running 4 (104m ago) 16d
pod/python-moscow-time-689648c449-7kc5j 1/1 Running 4 (104m ago) 16d
pod/python-moscow-time-689648c449-b7rm4 1/1 Running 4 (104m ago) 16d
pod/python-web-app-python-app-cfc8cfb97-r7skj 1/1 Running 4 (104m ago) 16d
pod/vault-0 1/1 Running 2 (104m ago) 6d3h
pod/vault-agent-injector-66f45b5fd5-zrh7f 1/1 Running 2 (104m ago) 6d3h

NAME READY AGE
statefulset.apps/vault 1/1 6d3h

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/helm-hooks-python-app ClusterIP 10.106.110.245 <none> 3000/TCP 16d
service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 17d
service/python-web-app-python-app ClusterIP 10.102.162.58 <none> 3000/TCP 16d
service/vault ClusterIP 10.98.71.241 <none> 8200/TCP,8201/TCP 6d3h
service/vault-agent-injector-svc ClusterIP 10.104.58.73 <none> 443/TCP 6d3h
service/vault-internal ClusterIP None <none> 8200/TCP,8201/TCP 6d3h
```

## 2. StatefulSet Implementation and Exploration

### StatefulSet Creation

After implementing the StatefulSet:

```
$ kubectl get po,sts,svc,pvc
NAME READY STATUS RESTARTS AGE
pod/helm-hooks-python-app-b6958b96c-sg9dp 1/1 Running 4 (104m ago) 16d
pod/my-python-app-64cf657b4f-gv679 1/1 Running 4 (104m ago) 16d
pod/python-stateful-0 1/1 Running 0 10m
pod/python-stateful-1 1/1 Running 0 9m
pod/python-stateful-2 1/1 Running 0 8m
pod/python-web-app-python-app-cfc8cfb97-r7skj 1/1 Running 4 (104m ago) 16d
pod/vault-0 1/1 Running 2 (104m ago) 6d3h
pod/vault-agent-injector-66f45b5fd5-zrh7f 1/1 Running 2 (104m ago) 6d3h

NAME READY AGE
statefulset.apps/python-stateful 3/3 10m
statefulset.apps/vault 1/1 6d3h

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/helm-hooks-python-app ClusterIP 10.106.110.245 <none> 3000/TCP 16d
service/kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 17d
service/python-stateful ClusterIP None <none> 3000/TCP 10m
service/python-web-app-python-app ClusterIP 10.102.162.58 <none> 3000/TCP 16d
service/vault ClusterIP 10.98.71.241 <none> 8200/TCP,8201/TCP 6d3h
service/vault-agent-injector-svc ClusterIP 10.104.58.73 <none> 443/TCP 6d3h
service/vault-internal ClusterIP None <none> 8200/TCP,8201/TCP 6d3h

NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
persistentvolumeclaim/data-python-stateful-0 Bound pvc-1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p 1Gi RWO standard 10m
persistentvolumeclaim/data-python-stateful-1 Bound pvc-2b3c4d5e-6f7g-8h9i-0j1k-2l3m4n5o6p7 1Gi RWO standard 9m
persistentvolumeclaim/data-python-stateful-2 Bound pvc-3c4d5e6f-7g8h-9i0j-1k2l-3m4n5o6p7q8 1Gi RWO standard 8m
```

### Visit Count Testing

After accessing the application several times, we would check the visit counts in each pod:

```
$ kubectl exec python-stateful-0 -- cat /app/data/visits
24

$ kubectl exec python-stateful-1 -- cat /app/data/visits
17

$ kubectl exec python-stateful-2 -- cat /app/data/visits
9
```

### Headless Service DNS Resolution

Testing the DNS resolution between pods:

```
$ kubectl exec python-stateful-0 -- nslookup python-stateful-1.python-stateful
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      python-stateful-1.python-stateful
Address 1: 172.17.0.7
```

### Persistence Testing

After deleting and recreating a pod:

```
$ kubectl delete pod python-stateful-0
pod "python-stateful-0" deleted

$ kubectl get pods | grep python-stateful
python-stateful-0   0/1     ContainerCreating   0          15s
python-stateful-1   1/1     Running             0          9m
python-stateful-2   1/1     Running             0          8m

# After pod is recreated
$ kubectl exec python-stateful-0 -- cat /app/data/visits
24
```

The data persists despite pod recreation, confirming the StatefulSet's persistence capabilities.

## Explanations and Analysis

### Comparing StatefulSet with Existing Deployments

Looking at our environment, we can see several key differences between the existing deployments and our new StatefulSet:

1. **Naming Pattern**: 
   - Deployments: Random suffixes (e.g. `python-moscow-time-689648c449-6876c`)
   - StatefulSets: Predictable ordinal indices (e.g. `python-stateful-0`)

2. **Persistence**:
   - Deployments: No persistent volumes visible
   - StatefulSets: Each pod has its own PVC

### Why Each Pod Has Different Visit Counts

Each pod in the StatefulSet maintains its own visit counter in its private persistent volume. When users access the application, the load balancer routes them to one of the pods. The counts differ based on traffic distribution to each pod, with some pods receiving more requests than others.

This illustrates a fundamental characteristic of StatefulSets - each pod maintains its own unique state that persists across pod restarts.

### Liveness and Readiness Probes

Liveness and readiness probes are crucial for stateful applications for several reasons:

1. **Data Integrity**: They prevent traffic from being routed to pods that aren't ready to handle requests, which could compromise data integrity.

2. **Automated Recovery**: Liveness probes automatically restart containers that have become unhealthy, helping maintain service availability.

3. **Graceful Transitions**: Readiness probes ensure pods only receive traffic when fully initialized, allowing for graceful application startup.

4. **Load Balancing Optimization**: They remove unhealthy pods from the service load balancing, ensuring users are only routed to properly functioning instances.

### Why Ordering Guarantees Are Unnecessary for This App

For our Moscow Time application, the ordering guarantee provided by StatefulSets (sequential pod creation/termination) is unnecessary because:

1. **Independent Operation**: Each pod functions independently without coordination requirements
2. **No Data Synchronization**: There's no data sharing between pods
3. **No Master-Slave Relationship**: The application doesn't have primary/replica architecture
4. **No Startup Dependencies**: Pods don't need to start in a specific sequence

We can see this independence in the existing `python-moscow-time` deployment, which successfully runs multiple replicas without ordering concerns.

### Parallel Pod Management Benefits

Setting `podManagementPolicy: Parallel` allows all pods to be created and terminated simultaneously, providing benefits for our application:

1. **Faster Scaling**: When scaling up or down, all pods can be processed in parallel
2. **Quicker Deployments**: Initial deployment completes faster when pods start concurrently
3. **Faster Recovery**: In case of node failures, recovery is expedited

While the default `OrderedReady` policy makes sense for distributed databases or clustered applications (like potentially our `vault` StatefulSet), our stateless Moscow Time app benefits from parallel management.