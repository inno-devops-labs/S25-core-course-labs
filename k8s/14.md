# Stateful Sets

After StatefulSet setup we can see the following picture.
I've also temporarily disabled liveness and readiness probes because they were pointing to the
root page :)

## Data synchronization issues

```bash
➜  k8s git:(lab-14) ✗ kubectl get po,sts,svc,pvc
NAME                READY   STATUS    RESTARTS   AGE
pod/moscow-time-0   1/1     Running   0          30s
pod/moscow-time-1   1/1     Running   0          23s
pod/moscow-time-2   1/1     Running   0          15s

NAME                           READY   AGE
statefulset.apps/moscow-time   3/3     30s

NAME                  TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
service/kubernetes    ClusterIP   10.96.0.1     <none>        443/TCP    11d
service/moscow-time   ClusterIP   10.109.83.2   <none>        8000/TCP   30s

NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/moscow-time-pvc-moscow-time-0   Bound    pvc-e5f8941c-99f1-4f47-8dcc-8da232a1ffb1   100Mi      RWO            local-path     <unset>                 30s
persistentvolumeclaim/moscow-time-pvc-moscow-time-1   Bound    pvc-39baa617-4147-4571-b34c-938a9f685102   100Mi      RWO            local-path     <unset>                 23s
persistentvolumeclaim/moscow-time-pvc-moscow-time-2   Bound    pvc-50be5814-1144-4978-89de-68058085bba5   100Mi      RWO            local-path     <unset>                 15s
```

After multiple attempts to reach service, one can see following data in the `visits.txt` in pods.

```bash
➜  k8s git:(lab-14) ✗ kubectl exec moscow-time-0 -- cat /data/visits.txt
13
➜  k8s git:(lab-14) ✗ kubectl exec moscow-time-1 -- cat /data/visits.txt
1
➜  k8s git:(lab-14) ✗ kubectl exec moscow-time-2 -- cat /data/visits.txt
5
```

This occurs because each of the pods has its own volume where it stores its own `visits.txt` file.
And the service load balancer routes requests to different pods at different requests. So, reading
from these files gives us different unsychronized results.

## Pod deletion

As one can see from the experiment below, after pod deletion, the PVC still remains alive.
An the data retreived from the pod will be equal data before the deletion.

```bash
➜  k8s git:(lab-14) ✗ kubectl exec moscow-time-0 -- cat /data/visits.txt
13
➜  k8s git:(lab-14) ✗ kubectl delete pod moscow-time-0
pod "moscow-time-0" deleted
➜  k8s git:(lab-14) ✗ kubectl get po,sts,svc,pvc
NAME                READY   STATUS              RESTARTS   AGE
pod/moscow-time-0   0/1     ContainerCreating   0          3s
pod/moscow-time-1   1/1     Running             0          20m
pod/moscow-time-2   1/1     Running             0          20m

NAME                           READY   AGE
statefulset.apps/moscow-time   2/3     20m

NAME                  TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
service/kubernetes    ClusterIP   10.96.0.1     <none>        443/TCP    12d
service/moscow-time   ClusterIP   10.109.83.2   <none>        8000/TCP   20m

NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
persistentvolumeclaim/moscow-time-pvc-moscow-time-0   Bound    pvc-e5f8941c-99f1-4f47-8dcc-8da232a1ffb1   100Mi      RWO            local-path     <unset>                 20m
persistentvolumeclaim/moscow-time-pvc-moscow-time-1   Bound    pvc-39baa617-4147-4571-b34c-938a9f685102   100Mi      RWO            local-path     <unset>                 20m
persistentvolumeclaim/moscow-time-pvc-moscow-time-2   Bound    pvc-50be5814-1144-4978-89de-68058085bba5   100Mi      RWO            local-path     <unset>                 20m
➜  k8s git:(lab-14) ✗ kubectl exec moscow-time-0 -- cat /data/visits.txt
13%
```

## DNS resolution

For some reason that I did not troubleshoot yet, I am unable to use Partially Qualified Domain Name (PQDN)
to ping pods :( Only FQDN allows to do it. But nevertheless

```bash
➜  k8s git:(lab-14) ✗ kubectl exec moscow-time-0 -- nslookup moscow-time-1.moscow-time.default.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53


Name:	moscow-time-1.moscow-time.default.svc.cluster.local
Address: 10.244.1.185
➜  k8s git:(lab-14) ✗ kubectl exec -it moscow-time-0 --container moscow-time -- nslookup moscow-time-1.moscow-time.default.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53


Name:	moscow-time-1.moscow-time.default.svc.cluster.local
Address: 10.244.1.235
➜  k8s git:(lab-14) ✗ kubectl exec -it moscow-time-0 --container moscow-time -- nslookup moscow-time-2.moscow-time.default.svc.cluster.local
Server:		10.96.0.10
Address:	10.96.0.10:53


Name:	moscow-time-2.moscow-time.default.svc.cluster.local
Address: 10.244.1.236
```

But the PQDN does not work
```bash
➜  k8s git:(lab-14) ✗ kubectl exec moscow-time-0 -- nslookup moscow-time-1.moscow-time
Server:		10.96.0.10
Address:	10.96.0.10:53

** server can't find moscow-time-1.moscow-time: NXDOMAIN

** server can't find moscow-time-1.moscow-time: NXDOMAIN

command terminated with exit code 1
```

## Liveness and Readiness probes

Livenes and Readiness probes are just queries performed by kubelet (node agent) in order to ensure
that a container is **alive** and **ready to accept connections**. It can be (as in my case) just
http requests to some path of pod. Even the same path.

Why do we need these probes? First of all, to ensure that Kubernetes will launch a container that
is really doing some job and does not fail. E.g, in order to not fail rollout update.

Secondly, these probes allow some kind of tricks. E.g, we can manually disable a handler for readiness
probe and after this the container will be removed from service load balancing. It can be convenient if
the pod is actually overaloaded and need some time to perform requests.

## Ordering guarantees and parallel operations

Ordering guarantees are needed in clusters / StatefulSets with master-replicas. For instance, some pods
need to connect to master replica to get some info. Therefore, the master-pod should start earlier than others.

In my application I do not have any dependencies and can start all the pods in parallel.

It can be easily done with `podManagementPolicy: Parallel`

## Update strategies

`OnDelete` and `RollingUpdate` update strategies are available for StatefulSet.

The second one (RollingUpdate) is default - deletion and creation of a new pod.
If the `rollingUpdate.partition` (define as `x`) is specified, then pods with
ordinal greater than `x` will be updated. This can be called Canary Release :)

`OnDelete` will create a new pod from updated template when some of the previous
pods is manually deleted.
