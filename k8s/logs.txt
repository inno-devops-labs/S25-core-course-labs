
==> Audit <==
|---------|------------------|----------|--------------------|---------|---------------------|---------------------|
| Command |       Args       | Profile  |        User        | Version |     Start Time      |      End Time       |
|---------|------------------|----------|--------------------|---------|---------------------|---------------------|
| start   |                  | minikube | DESKTOP-85MKAD8\dm | v1.35.0 | 09 Mar 25 21:19 MSK | 09 Mar 25 21:26 MSK |
| service | app-python --all | minikube | DESKTOP-85MKAD8\dm | v1.35.0 | 09 Mar 25 21:36 MSK |                     |
| service | --all            | minikube | DESKTOP-85MKAD8\dm | v1.35.0 | 09 Mar 25 21:36 MSK |                     |
| start   |                  | minikube | DESKTOP-85MKAD8\dm | v1.35.0 | 09 Mar 25 21:37 MSK | 09 Mar 25 21:38 MSK |
| service | --all            | minikube | DESKTOP-85MKAD8\dm | v1.35.0 | 09 Mar 25 21:43 MSK |                     |
|---------|------------------|----------|--------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/09 21:37:17
Running on machine: DESKTOP-85MKAD8
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0309 21:37:17.345846   19704 out.go:345] Setting OutFile to fd 104 ...
I0309 21:37:17.363440   19704 out.go:358] Setting ErrFile to fd 108...
W0309 21:37:17.378524   19704 root.go:314] Error reading config file at C:\Users\dm\.minikube\config\config.json: open C:\Users\dm\.minikube\config\config.json: The system cannot find the file specified.
I0309 21:37:17.384482   19704 out.go:352] Setting JSON to false
I0309 21:37:17.391313   19704 start.go:129] hostinfo: {"hostname":"DESKTOP-85MKAD8","uptime":13584,"bootTime":1741531853,"procs":319,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3194 Build 26100.3194","kernelVersion":"10.0.26100.3194 Build 26100.3194","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"46eff744-89b0-4a61-b00a-a53ee42c16b0"}
W0309 21:37:17.391313   19704 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0309 21:37:17.392951   19704 out.go:177] üòÑ  minikube v1.35.0 –Ω–∞ Microsoft Windows 11 Pro 10.0.26100.3194 Build 26100.3194
I0309 21:37:17.393482   19704 notify.go:220] Checking for updates...
I0309 21:37:17.394014   19704 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0309 21:37:17.394014   19704 driver.go:394] Setting default libvirt URI to qemu:///system
I0309 21:37:17.525371   19704 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0309 21:37:17.526971   19704 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0309 21:37:17.945023   19704 info.go:266] docker info: {ID:124bad20-26ca-4f63-8793-479b3e34dce0 Containers:54 ContainersRunning:42 ContainersPaused:0 ContainersStopped:12 Images:39 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:197 OomKillDisable:true NGoroutines:198 SystemTime:2025-03-09 18:37:17.92805872 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:19 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16465084416 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\dm\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\dm\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\dm\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\dm\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\dm\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\dm\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\dm\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\dm\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\dm\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\dm\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\dm\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0309 21:37:17.946120   19704 out.go:177] ‚ú®  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥—Ä–∞–π–≤–µ—Ä docker –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
I0309 21:37:17.947222   19704 start.go:297] selected driver: docker
I0309 21:37:17.947222   19704 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\dm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0309 21:37:17.947222   19704 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0309 21:37:17.952782   19704 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0309 21:37:18.307141   19704 info.go:266] docker info: {ID:124bad20-26ca-4f63-8793-479b3e34dce0 Containers:54 ContainersRunning:42 ContainersPaused:0 ContainersStopped:12 Images:39 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:199 OomKillDisable:true NGoroutines:198 SystemTime:2025-03-09 18:37:18.294342166 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:19 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16465084416 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\dm\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\dm\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\dm\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\dm\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\dm\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\dm\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\dm\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\dm\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\dm\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\dm\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\dm\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0309 21:37:18.402987   19704 cni.go:84] Creating CNI manager for ""
I0309 21:37:18.402987   19704 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0309 21:37:18.402987   19704 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\dm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0309 21:37:18.404053   19704 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0309 21:37:18.405131   19704 cache.go:121] Beginning downloading kic base image for docker with docker
I0309 21:37:18.406193   19704 out.go:177] üöú  Pulling base image v0.0.46 ...
I0309 21:37:18.406715   19704 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0309 21:37:18.406715   19704 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0309 21:37:18.407246   19704 preload.go:146] Found local preload: C:\Users\dm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0309 21:37:18.407246   19704 cache.go:56] Caching tarball of preloaded images
I0309 21:37:18.407246   19704 preload.go:172] Found C:\Users\dm\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0309 21:37:18.407246   19704 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0309 21:37:18.407776   19704 profile.go:143] Saving config to C:\Users\dm\.minikube\profiles\minikube\config.json ...
I0309 21:37:18.644744   19704 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0309 21:37:18.644744   19704 localpath.go:146] windows sanitize: C:\Users\dm\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\dm\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0309 21:37:18.645273   19704 localpath.go:146] windows sanitize: C:\Users\dm\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\dm\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0309 21:37:18.645273   19704 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0309 21:37:18.645273   19704 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0309 21:37:18.645273   19704 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0309 21:37:18.645273   19704 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0309 21:37:18.645273   19704 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0309 21:37:18.645273   19704 localpath.go:146] windows sanitize: C:\Users\dm\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\dm\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0309 21:37:38.384191   19704 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0309 21:37:38.385334   19704 cache.go:227] Successfully downloaded all kic artifacts
I0309 21:37:38.386351   19704 start.go:360] acquireMachinesLock for minikube: {Name:mk72523a84e77c2e09da0051af9818b3457b0c81 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0309 21:37:38.386351   19704 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0309 21:37:38.386861   19704 start.go:96] Skipping create...Using existing machine configuration
I0309 21:37:38.386861   19704 fix.go:54] fixHost starting: 
I0309 21:37:38.394244   19704 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0309 21:37:38.481227   19704 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0309 21:37:38.481227   19704 fix.go:138] unexpected machine state, will restart: <nil>
I0309 21:37:38.482903   19704 out.go:177] üèÉ  –û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è —Ä–∞–±–æ—Ç–∞—é—â–∏–π docker "minikube" container ...
I0309 21:37:38.483980   19704 machine.go:93] provisionDockerMachine start ...
I0309 21:37:38.487293   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:38.591261   19704 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0309 21:37:38.592374   19704 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xee5360] 0xee7ea0 <nil>  [] 0s} 127.0.0.1 45166 <nil> <nil>}
I0309 21:37:38.592374   19704 main.go:141] libmachine: About to run SSH command:
hostname
I0309 21:37:38.782815   19704 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0309 21:37:38.782815   19704 ubuntu.go:169] provisioning hostname "minikube"
I0309 21:37:38.785046   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:38.855613   19704 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0309 21:37:38.856144   19704 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xee5360] 0xee7ea0 <nil>  [] 0s} 127.0.0.1 45166 <nil> <nil>}
I0309 21:37:38.856144   19704 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0309 21:37:39.034696   19704 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0309 21:37:39.036303   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:39.089864   19704 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0309 21:37:39.090408   19704 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xee5360] 0xee7ea0 <nil>  [] 0s} 127.0.0.1 45166 <nil> <nil>}
I0309 21:37:39.090408   19704 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0309 21:37:39.240939   19704 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0309 21:37:39.240939   19704 ubuntu.go:175] set auth options {CertDir:C:\Users\dm\.minikube CaCertPath:C:\Users\dm\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\dm\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\dm\.minikube\machines\server.pem ServerKeyPath:C:\Users\dm\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\dm\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\dm\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\dm\.minikube}
I0309 21:37:39.240939   19704 ubuntu.go:177] setting up certificates
I0309 21:37:39.240939   19704 provision.go:84] configureAuth start
I0309 21:37:39.242542   19704 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0309 21:37:39.290425   19704 provision.go:143] copyHostCerts
I0309 21:37:39.291018   19704 exec_runner.go:144] found C:\Users\dm\.minikube/ca.pem, removing ...
I0309 21:37:39.291018   19704 exec_runner.go:203] rm: C:\Users\dm\.minikube\ca.pem
I0309 21:37:39.291018   19704 exec_runner.go:151] cp: C:\Users\dm\.minikube\certs\ca.pem --> C:\Users\dm\.minikube/ca.pem (1066 bytes)
I0309 21:37:39.292091   19704 exec_runner.go:144] found C:\Users\dm\.minikube/cert.pem, removing ...
I0309 21:37:39.292091   19704 exec_runner.go:203] rm: C:\Users\dm\.minikube\cert.pem
I0309 21:37:39.292091   19704 exec_runner.go:151] cp: C:\Users\dm\.minikube\certs\cert.pem --> C:\Users\dm\.minikube/cert.pem (1111 bytes)
I0309 21:37:39.293163   19704 exec_runner.go:144] found C:\Users\dm\.minikube/key.pem, removing ...
I0309 21:37:39.293163   19704 exec_runner.go:203] rm: C:\Users\dm\.minikube\key.pem
I0309 21:37:39.293163   19704 exec_runner.go:151] cp: C:\Users\dm\.minikube\certs\key.pem --> C:\Users\dm\.minikube/key.pem (1679 bytes)
I0309 21:37:39.293702   19704 provision.go:117] generating server cert: C:\Users\dm\.minikube\machines\server.pem ca-key=C:\Users\dm\.minikube\certs\ca.pem private-key=C:\Users\dm\.minikube\certs\ca-key.pem org=dm.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0309 21:37:39.392496   19704 provision.go:177] copyRemoteCerts
I0309 21:37:39.392496   19704 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0309 21:37:39.394134   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:39.451868   19704 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:45166 SSHKeyPath:C:\Users\dm\.minikube\machines\minikube\id_rsa Username:docker}
I0309 21:37:39.567961   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\machines\server.pem --> /etc/docker/server.pem (1168 bytes)
I0309 21:37:39.620447   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0309 21:37:39.653448   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1066 bytes)
I0309 21:37:39.690239   19704 provision.go:87] duration metric: took 449.3001ms to configureAuth
I0309 21:37:39.690239   19704 ubuntu.go:193] setting minikube options for container-runtime
I0309 21:37:39.690799   19704 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0309 21:37:39.692599   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:39.764859   19704 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0309 21:37:39.765410   19704 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xee5360] 0xee7ea0 <nil>  [] 0s} 127.0.0.1 45166 <nil> <nil>}
I0309 21:37:39.765410   19704 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0309 21:37:39.921895   19704 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0309 21:37:39.921895   19704 ubuntu.go:71] root file system type: overlay
I0309 21:37:39.921895   19704 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0309 21:37:39.923559   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:40.037551   19704 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0309 21:37:40.038090   19704 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xee5360] 0xee7ea0 <nil>  [] 0s} 127.0.0.1 45166 <nil> <nil>}
I0309 21:37:40.038090   19704 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0309 21:37:40.199201   19704 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0309 21:37:40.200799   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:40.315871   19704 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0309 21:37:40.315871   19704 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xee5360] 0xee7ea0 <nil>  [] 0s} 127.0.0.1 45166 <nil> <nil>}
I0309 21:37:40.315871   19704 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0309 21:37:40.455673   19704 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0309 21:37:40.455673   19704 machine.go:96] duration metric: took 1.9716935s to provisionDockerMachine
I0309 21:37:40.455673   19704 start.go:293] postStartSetup for "minikube" (driver="docker")
I0309 21:37:40.455673   19704 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0309 21:37:40.456183   19704 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0309 21:37:40.457268   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:40.563649   19704 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:45166 SSHKeyPath:C:\Users\dm\.minikube\machines\minikube\id_rsa Username:docker}
I0309 21:37:40.679537   19704 ssh_runner.go:195] Run: cat /etc/os-release
I0309 21:37:40.684520   19704 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0309 21:37:40.684520   19704 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0309 21:37:40.684520   19704 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0309 21:37:40.684520   19704 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0309 21:37:40.684520   19704 filesync.go:126] Scanning C:\Users\dm\.minikube\addons for local assets ...
I0309 21:37:40.684520   19704 filesync.go:126] Scanning C:\Users\dm\.minikube\files for local assets ...
I0309 21:37:40.685025   19704 start.go:296] duration metric: took 229.3516ms for postStartSetup
I0309 21:37:40.689926   19704 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0309 21:37:40.691528   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:40.797139   19704 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:45166 SSHKeyPath:C:\Users\dm\.minikube\machines\minikube\id_rsa Username:docker}
I0309 21:37:40.895694   19704 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0309 21:37:40.902089   19704 fix.go:56] duration metric: took 2.5152283s for fixHost
I0309 21:37:40.902089   19704 start.go:83] releasing machines lock for "minikube", held for 2.5157384s
I0309 21:37:40.903724   19704 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0309 21:37:41.009902   19704 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0309 21:37:41.012032   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:41.014146   19704 ssh_runner.go:195] Run: cat /version.json
I0309 21:37:41.015746   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:37:41.122728   19704 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:45166 SSHKeyPath:C:\Users\dm\.minikube\machines\minikube\id_rsa Username:docker}
I0309 21:37:41.128712   19704 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:45166 SSHKeyPath:C:\Users\dm\.minikube\machines\minikube\id_rsa Username:docker}
W0309 21:37:41.229635   19704 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0309 21:37:41.245525   19704 ssh_runner.go:195] Run: systemctl --version
I0309 21:37:41.260494   19704 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0309 21:37:41.268988   19704 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0309 21:37:41.281178   19704 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0309 21:37:41.281726   19704 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0309 21:37:41.294375   19704 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0309 21:37:41.294375   19704 start.go:495] detecting cgroup driver to use...
I0309 21:37:41.294375   19704 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0309 21:37:41.294924   19704 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0309 21:37:41.324766   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0309 21:37:41.343614   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0309 21:37:41.358799   19704 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0309 21:37:41.364196   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0309 21:37:41.382100   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0309 21:37:41.397992   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0309 21:37:41.413465   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0309 21:37:41.429190   19704 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0309 21:37:41.444784   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0309 21:37:41.461529   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0309 21:37:41.477600   19704 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0309 21:37:41.490965   19704 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0309 21:37:41.501996   19704 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0309 21:37:41.512786   19704 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0309 21:37:41.649747   19704 ssh_runner.go:195] Run: sudo systemctl restart containerd
W0309 21:37:41.856667   19704 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0309 21:37:41.857215   19704 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0309 21:37:52.062999   19704 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.4132525s)
I0309 21:37:52.062999   19704 start.go:495] detecting cgroup driver to use...
I0309 21:37:52.062999   19704 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0309 21:37:52.063508   19704 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0309 21:37:52.089314   19704 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0309 21:37:52.089820   19704 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0309 21:37:52.120578   19704 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0309 21:37:52.172705   19704 ssh_runner.go:195] Run: which cri-dockerd
I0309 21:37:52.178065   19704 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0309 21:37:52.207318   19704 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0309 21:37:52.289898   19704 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0309 21:37:52.420903   19704 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0309 21:37:52.525231   19704 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0309 21:37:52.525231   19704 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0309 21:37:52.548013   19704 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0309 21:37:52.664665   19704 ssh_runner.go:195] Run: sudo systemctl restart docker
I0309 21:38:04.333415   19704 ssh_runner.go:235] Completed: sudo systemctl restart docker: (11.6687496s)
I0309 21:38:04.333963   19704 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0309 21:38:04.348815   19704 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0309 21:38:04.376302   19704 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0309 21:38:04.390569   19704 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0309 21:38:04.492973   19704 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0309 21:38:04.634837   19704 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0309 21:38:04.743136   19704 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0309 21:38:04.770274   19704 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0309 21:38:04.783861   19704 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0309 21:38:04.911767   19704 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0309 21:38:05.024746   19704 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0309 21:38:05.028399   19704 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0309 21:38:05.033782   19704 start.go:563] Will wait 60s for crictl version
I0309 21:38:05.037451   19704 ssh_runner.go:195] Run: which crictl
I0309 21:38:05.043183   19704 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0309 21:38:05.085788   19704 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0309 21:38:05.087931   19704 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0309 21:38:05.124260   19704 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0309 21:38:05.152742   19704 out.go:235] üê≥  –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è Kubernetes v1.32.0 –Ω–∞ Docker 27.4.1 ...
I0309 21:38:05.154899   19704 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0309 21:38:05.307016   19704 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0309 21:38:05.311253   19704 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0309 21:38:05.317680   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0309 21:38:05.434738   19704 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\dm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0309 21:38:05.434738   19704 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0309 21:38:05.436308   19704 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0309 21:38:05.501318   19704 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0309 21:38:05.501318   19704 docker.go:619] Images already preloaded, skipping extraction
I0309 21:38:05.503024   19704 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0309 21:38:05.526573   19704 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0309 21:38:05.526573   19704 cache_images.go:84] Images are preloaded, skipping loading
I0309 21:38:05.526573   19704 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0309 21:38:05.526573   19704 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0309 21:38:05.528765   19704 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0309 21:38:05.698710   19704 cni.go:84] Creating CNI manager for ""
I0309 21:38:05.698710   19704 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0309 21:38:05.698710   19704 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0309 21:38:05.698710   19704 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0309 21:38:05.699262   19704 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0309 21:38:05.699817   19704 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0309 21:38:05.785586   19704 binaries.go:44] Found k8s binaries, skipping transfer
I0309 21:38:05.786113   19704 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0309 21:38:05.883594   19704 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0309 21:38:05.991917   19704 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0309 21:38:06.102828   19704 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0309 21:38:06.206500   19704 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0309 21:38:06.212060   19704 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0309 21:38:06.695561   19704 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0309 21:38:06.790969   19704 certs.go:68] Setting up C:\Users\dm\.minikube\profiles\minikube for IP: 192.168.49.2
I0309 21:38:06.790969   19704 certs.go:194] generating shared ca certs ...
I0309 21:38:06.790969   19704 certs.go:226] acquiring lock for ca certs: {Name:mk2537639f97daa197d94588ab8c2d39f412989f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0309 21:38:06.792046   19704 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\dm\.minikube\ca.key
I0309 21:38:06.792046   19704 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\dm\.minikube\proxy-client-ca.key
I0309 21:38:06.792046   19704 certs.go:256] generating profile certs ...
I0309 21:38:06.793101   19704 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\dm\.minikube\profiles\minikube\client.key
I0309 21:38:06.793101   19704 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\dm\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0309 21:38:06.793101   19704 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\dm\.minikube\profiles\minikube\proxy-client.key
I0309 21:38:06.793635   19704 certs.go:484] found cert: C:\Users\dm\.minikube\certs\ca-key.pem (1675 bytes)
I0309 21:38:06.794149   19704 certs.go:484] found cert: C:\Users\dm\.minikube\certs\ca.pem (1066 bytes)
I0309 21:38:06.794149   19704 certs.go:484] found cert: C:\Users\dm\.minikube\certs\cert.pem (1111 bytes)
I0309 21:38:06.794149   19704 certs.go:484] found cert: C:\Users\dm\.minikube\certs\key.pem (1679 bytes)
I0309 21:38:06.794681   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0309 21:38:07.084499   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0309 21:38:07.289362   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0309 21:38:07.702187   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0309 21:38:07.904220   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0309 21:38:08.005986   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0309 21:38:08.177293   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0309 21:38:08.247512   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0309 21:38:08.617448   19704 ssh_runner.go:362] scp C:\Users\dm\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0309 21:38:08.689201   19704 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0309 21:38:08.746354   19704 ssh_runner.go:195] Run: openssl version
I0309 21:38:08.752680   19704 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0309 21:38:08.784591   19704 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0309 21:38:08.789955   19704 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Mar  9 18:25 /usr/share/ca-certificates/minikubeCA.pem
I0309 21:38:08.794231   19704 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0309 21:38:08.802377   19704 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0309 21:38:08.829829   19704 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0309 21:38:08.838476   19704 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0309 21:38:08.851046   19704 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0309 21:38:08.861803   19704 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0309 21:38:08.875321   19704 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0309 21:38:08.886776   19704 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0309 21:38:08.898655   19704 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0309 21:38:08.905611   19704 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:8000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\dm:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0309 21:38:08.907462   19704 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0309 21:38:08.932479   19704 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0309 21:38:08.957449   19704 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0309 21:38:08.957449   19704 kubeadm.go:593] restartPrimaryControlPlane start ...
I0309 21:38:08.957449   19704 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0309 21:38:09.336998   19704 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0309 21:38:09.338574   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0309 21:38:09.383820   19704 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:45704"
I0309 21:38:09.386539   19704 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0309 21:38:09.398990   19704 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0309 21:38:09.398990   19704 kubeadm.go:597] duration metric: took 441.5414ms to restartPrimaryControlPlane
I0309 21:38:09.398990   19704 kubeadm.go:394] duration metric: took 493.3791ms to StartCluster
I0309 21:38:09.398990   19704 settings.go:142] acquiring lock: {Name:mk8bb356b06b194f94675673fedaa79d9adf9f1e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0309 21:38:09.398990   19704 settings.go:150] Updating kubeconfig:  C:\Users\dm\.kube\config
I0309 21:38:09.400204   19704 lock.go:35] WriteFile acquiring C:\Users\dm\.kube\config: {Name:mk00340bc49c27ce80122fcdf7257207d77f018a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0309 21:38:09.401281   19704 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0309 21:38:09.401281   19704 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0309 21:38:09.401281   19704 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0309 21:38:09.401281   19704 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0309 21:38:09.401281   19704 addons.go:247] addon storage-provisioner should already be in state true
I0309 21:38:09.401281   19704 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0309 21:38:09.401281   19704 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0309 21:38:09.401812   19704 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0309 21:38:09.401812   19704 host.go:66] Checking if "minikube" exists ...
I0309 21:38:09.402350   19704 out.go:177] üîé  –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Kubernetes –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è ...
I0309 21:38:09.403427   19704 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0309 21:38:09.406611   19704 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0309 21:38:09.408231   19704 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0309 21:38:09.460184   19704 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0309 21:38:09.460184   19704 addons.go:247] addon default-storageclass should already be in state true
I0309 21:38:09.460709   19704 host.go:66] Checking if "minikube" exists ...
I0309 21:38:09.461240   19704 out.go:177]     ‚ñ™ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—Ä–∞–∑ gcr.io/k8s-minikube/storage-provisioner:v5
I0309 21:38:09.461804   19704 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0309 21:38:09.461804   19704 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0309 21:38:09.463910   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:38:09.464439   19704 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0309 21:38:09.517212   19704 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0309 21:38:09.517212   19704 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0309 21:38:09.519417   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0309 21:38:09.534362   19704 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:45166 SSHKeyPath:C:\Users\dm\.minikube\machines\minikube\id_rsa Username:docker}
I0309 21:38:09.567628   19704 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:45166 SSHKeyPath:C:\Users\dm\.minikube\machines\minikube\id_rsa Username:docker}
I0309 21:38:09.798449   19704 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0309 21:38:09.875453   19704 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0309 21:38:09.885998   19704 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0309 21:38:09.888205   19704 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0309 21:38:09.994559   19704 api_server.go:52] waiting for apiserver process to appear ...
I0309 21:38:09.995086   19704 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0309 21:38:11.300195   19704 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.4247422s)
I0309 21:38:12.011519   19704 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.0164331s)
I0309 21:38:12.011519   19704 api_server.go:72] duration metric: took 2.6102375s to wait for apiserver process to appear ...
I0309 21:38:12.011519   19704 api_server.go:88] waiting for apiserver healthz status ...
I0309 21:38:12.011519   19704 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.1255206s)
I0309 21:38:12.011519   19704 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:45704/healthz ...
I0309 21:38:12.012572   19704 out.go:177] üåü  –í–∫–ª—é—á–µ–Ω–Ω—ã–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è: default-storageclass, storage-provisioner
I0309 21:38:12.013107   19704 addons.go:514] duration metric: took 2.6118259s for enable addons: enabled=[default-storageclass storage-provisioner]
I0309 21:38:12.017636   19704 api_server.go:279] https://127.0.0.1:45704/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0309 21:38:12.017636   19704 api_server.go:103] status: https://127.0.0.1:45704/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0309 21:38:12.511961   19704 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:45704/healthz ...
I0309 21:38:12.517345   19704 api_server.go:279] https://127.0.0.1:45704/healthz returned 200:
ok
I0309 21:38:12.519138   19704 api_server.go:141] control plane version: v1.32.0
I0309 21:38:12.519138   19704 api_server.go:131] duration metric: took 507.6188ms to wait for apiserver health ...
I0309 21:38:12.519138   19704 system_pods.go:43] waiting for kube-system pods to appear ...
I0309 21:38:12.526433   19704 system_pods.go:59] 7 kube-system pods found
I0309 21:38:12.526433   19704 system_pods.go:61] "coredns-668d6bf9bc-8j7v6" [efad975a-a038-42d7-bbfd-2802f8e9854b] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0309 21:38:12.526433   19704 system_pods.go:61] "etcd-minikube" [866a0b40-59c5-410d-aaf4-416d9f7920c3] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0309 21:38:12.526433   19704 system_pods.go:61] "kube-apiserver-minikube" [4c592ecd-7b29-4486-af4f-5f408e0d6741] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0309 21:38:12.526433   19704 system_pods.go:61] "kube-controller-manager-minikube" [cf937d47-198a-4c00-8c44-faff1e419cfc] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0309 21:38:12.526433   19704 system_pods.go:61] "kube-proxy-gq9bg" [246dc0cc-45c7-4be4-bf7b-f77e61564326] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0309 21:38:12.526433   19704 system_pods.go:61] "kube-scheduler-minikube" [c253668f-c428-4413-ba16-bf267287b9ab] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0309 21:38:12.526433   19704 system_pods.go:61] "storage-provisioner" [f14a37c0-a36e-43c7-851b-03ff322d6e2e] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0309 21:38:12.526433   19704 system_pods.go:74] duration metric: took 7.2948ms to wait for pod list to return data ...
I0309 21:38:12.526433   19704 kubeadm.go:582] duration metric: took 3.1251511s to wait for: map[apiserver:true system_pods:true]
I0309 21:38:12.526433   19704 node_conditions.go:102] verifying NodePressure condition ...
I0309 21:38:12.531303   19704 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0309 21:38:12.531303   19704 node_conditions.go:123] node cpu capacity is 16
I0309 21:38:12.531303   19704 node_conditions.go:105] duration metric: took 4.8708ms to run NodePressure ...
I0309 21:38:12.531303   19704 start.go:241] waiting for startup goroutines ...
I0309 21:38:12.531303   19704 start.go:246] waiting for cluster config update ...
I0309 21:38:12.531303   19704 start.go:255] writing updated cluster config ...
I0309 21:38:12.536790   19704 ssh_runner.go:195] Run: rm -f paused
I0309 21:38:12.693153   19704 start.go:600] kubectl: 1.31.4, cluster: 1.32.0 (minor skew: 1)
I0309 21:38:12.694186   19704 out.go:177] üèÑ  –ì–æ—Ç–æ–≤–æ! kubectl –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ "minikube" –∏ "default" –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–º—ë–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é


==> Docker <==
Mar 09 18:38:29 minikube dockerd[9949]: time="2025-03-09T18:38:29.439995808Z" level=info msg="ignoring event" container=d98b7675ba061abebe67ded34c9f22fe0d2b57e762baf853653d5c6497cc2bc7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:29 minikube cri-dockerd[10278]: time="2025-03-09T18:38:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0edcfc60129ab371bc62e287b378e399560748c2093b835ba06bd3b4d6ae8f03/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:29 minikube dockerd[9949]: time="2025-03-09T18:38:29.739239129Z" level=info msg="ignoring event" container=f1a27324c46ac2bfd56f56489634cce886939baa95f2ecbb2edd9010e6801758 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:30 minikube cri-dockerd[10278]: time="2025-03-09T18:38:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9c5c3483aeb7c456524ca91a8d503f013b2be01c961fa5b3ec4127a0a110cc39/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:32 minikube dockerd[9949]: time="2025-03-09T18:38:32.508096624Z" level=info msg="ignoring event" container=430e3a69f1c5bda44b995e60d5b976b41d1f8ec9579ef5ab6a31bd880f7e8193 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:32 minikube dockerd[9949]: time="2025-03-09T18:38:32.640460706Z" level=info msg="ignoring event" container=0edcfc60129ab371bc62e287b378e399560748c2093b835ba06bd3b4d6ae8f03 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:32 minikube cri-dockerd[10278]: time="2025-03-09T18:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/520cfeabceb3eee52885d8a77d4d5e585e086434681c2b755be1c5afd3a4f2f6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:32 minikube cri-dockerd[10278]: time="2025-03-09T18:38:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc1edf4c93d6939a3c6e4fee54cbca07f70b3d1240d32ee986464852e1f7e664/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:35 minikube dockerd[9949]: time="2025-03-09T18:38:35.057514623Z" level=info msg="ignoring event" container=9c5c3483aeb7c456524ca91a8d503f013b2be01c961fa5b3ec4127a0a110cc39 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:35 minikube cri-dockerd[10278]: time="2025-03-09T18:38:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3695c910f540cf604155321a4c1bca0f809a53a7091b026d50c8af23bce3e502/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:36 minikube dockerd[9949]: time="2025-03-09T18:38:36.826783749Z" level=info msg="ignoring event" container=520cfeabceb3eee52885d8a77d4d5e585e086434681c2b755be1c5afd3a4f2f6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:37 minikube dockerd[9949]: time="2025-03-09T18:38:37.045616922Z" level=info msg="ignoring event" container=cc1edf4c93d6939a3c6e4fee54cbca07f70b3d1240d32ee986464852e1f7e664 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:37 minikube cri-dockerd[10278]: time="2025-03-09T18:38:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dd16053241aebe995b5dcb6e3caa2815b0d5dd3b455e4d19e3ca0caa324926f4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:37 minikube cri-dockerd[10278]: time="2025-03-09T18:38:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d59c5a4280710eab7af7895cf8d85b12bbfbd451b163276319df585bad74aa92/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:39 minikube dockerd[9949]: time="2025-03-09T18:38:39.426389089Z" level=info msg="ignoring event" container=3695c910f540cf604155321a4c1bca0f809a53a7091b026d50c8af23bce3e502 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:39 minikube cri-dockerd[10278]: time="2025-03-09T18:38:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a999f838607fc3049ed9086479783f762b8660d50029682cdbfa57a1d1b41c4b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:40 minikube dockerd[9949]: time="2025-03-09T18:38:40.618437596Z" level=info msg="ignoring event" container=dd16053241aebe995b5dcb6e3caa2815b0d5dd3b455e4d19e3ca0caa324926f4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:40 minikube cri-dockerd[10278]: time="2025-03-09T18:38:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0af0448a62f03edf7ce394028e9e1cf076f25b718a99ace6dba47a1ebdffcb4f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:40 minikube dockerd[9949]: time="2025-03-09T18:38:40.929786057Z" level=info msg="ignoring event" container=d59c5a4280710eab7af7895cf8d85b12bbfbd451b163276319df585bad74aa92 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:41 minikube cri-dockerd[10278]: time="2025-03-09T18:38:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5b2ab2d27b365a2be916eb7d6985c10f0dec97339c82bf20ffdade6b8f976c34/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:44 minikube dockerd[9949]: time="2025-03-09T18:38:44.818430308Z" level=info msg="ignoring event" container=5b2ab2d27b365a2be916eb7d6985c10f0dec97339c82bf20ffdade6b8f976c34 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:44 minikube dockerd[9949]: time="2025-03-09T18:38:44.886472080Z" level=info msg="ignoring event" container=0af0448a62f03edf7ce394028e9e1cf076f25b718a99ace6dba47a1ebdffcb4f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:45 minikube dockerd[9949]: time="2025-03-09T18:38:45.114558003Z" level=info msg="ignoring event" container=a999f838607fc3049ed9086479783f762b8660d50029682cdbfa57a1d1b41c4b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:45 minikube cri-dockerd[10278]: time="2025-03-09T18:38:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a3c4f3cd4cf8fcf398a8c64e6f3f77daab66fc493653444d1bd1b86c6575353/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:45 minikube cri-dockerd[10278]: time="2025-03-09T18:38:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6bbf10202410bf05a13bf021af08dbc0bf8010838ace0f0680bad637bb416494/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:45 minikube cri-dockerd[10278]: time="2025-03-09T18:38:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7813640b6cab93e7c891062f28e9189c7665f33e186b652690f26f93a3d64dac/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:48 minikube dockerd[9949]: time="2025-03-09T18:38:48.927898708Z" level=info msg="ignoring event" container=7813640b6cab93e7c891062f28e9189c7665f33e186b652690f26f93a3d64dac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:49 minikube cri-dockerd[10278]: time="2025-03-09T18:38:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e3a73ea22f37b440a3987f23474dbce3e529e6b0d79247d3e6c040202a122629/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:49 minikube dockerd[9949]: time="2025-03-09T18:38:49.158003135Z" level=info msg="ignoring event" container=2a3c4f3cd4cf8fcf398a8c64e6f3f77daab66fc493653444d1bd1b86c6575353 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:49 minikube cri-dockerd[10278]: time="2025-03-09T18:38:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/348e0df206ddae9dc81ff22ccdc2e04f20979557af71638a361884802ca81502/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:51 minikube dockerd[9949]: time="2025-03-09T18:38:51.282626848Z" level=info msg="ignoring event" container=6bbf10202410bf05a13bf021af08dbc0bf8010838ace0f0680bad637bb416494 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:51 minikube cri-dockerd[10278]: time="2025-03-09T18:38:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/46625adc91f6e910d73a0725816d5cc785a0fbf2a8f88ada069b251faa52eb75/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:53 minikube dockerd[9949]: time="2025-03-09T18:38:53.168282347Z" level=info msg="ignoring event" container=348e0df206ddae9dc81ff22ccdc2e04f20979557af71638a361884802ca81502 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:53 minikube dockerd[9949]: time="2025-03-09T18:38:53.479527429Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 09 18:38:53 minikube dockerd[9949]: time="2025-03-09T18:38:53.479602603Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 09 18:38:53 minikube cri-dockerd[10278]: time="2025-03-09T18:38:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/44a213cf0cd7ebae614270fe7f0c9413c16240fce2fb72a48ad30968f952678a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:54 minikube dockerd[9949]: time="2025-03-09T18:38:54.978886092Z" level=info msg="ignoring event" container=e3a73ea22f37b440a3987f23474dbce3e529e6b0d79247d3e6c040202a122629 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:55 minikube cri-dockerd[10278]: time="2025-03-09T18:38:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8e227efd900eb1c821f93f8c7d1cfb74b84eaf5b2888fd0ba50a8b367c68b1b4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:55 minikube dockerd[9949]: time="2025-03-09T18:38:55.314978632Z" level=info msg="ignoring event" container=46625adc91f6e910d73a0725816d5cc785a0fbf2a8f88ada069b251faa52eb75 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:55 minikube cri-dockerd[10278]: time="2025-03-09T18:38:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/16a0bf986b0b516091be03e93b8b2475d7a1bbf1807761610ceacaf7b3c6d33d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:56 minikube dockerd[9949]: time="2025-03-09T18:38:56.936182072Z" level=info msg="ignoring event" container=44a213cf0cd7ebae614270fe7f0c9413c16240fce2fb72a48ad30968f952678a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:57 minikube cri-dockerd[10278]: time="2025-03-09T18:38:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1dcca2cd30c26e837d36d772a47941204f67028660a3c3ca2d2448eb1a8a840b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:38:59 minikube dockerd[9949]: time="2025-03-09T18:38:59.337455558Z" level=info msg="ignoring event" container=8e227efd900eb1c821f93f8c7d1cfb74b84eaf5b2888fd0ba50a8b367c68b1b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:38:59 minikube cri-dockerd[10278]: time="2025-03-09T18:38:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f317d8288d3d7ad7a234109bb7db8a1159e8c76918b40defd45c858f49b4e040/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:39:00 minikube dockerd[9949]: time="2025-03-09T18:39:00.049655308Z" level=info msg="ignoring event" container=16a0bf986b0b516091be03e93b8b2475d7a1bbf1807761610ceacaf7b3c6d33d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:39:00 minikube cri-dockerd[10278]: time="2025-03-09T18:39:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/36350e3e974596126e6b8e196ade6289f6ce4894f07887108f58c3d069135a61/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:39:01 minikube dockerd[9949]: time="2025-03-09T18:39:01.690777066Z" level=info msg="ignoring event" container=1dcca2cd30c26e837d36d772a47941204f67028660a3c3ca2d2448eb1a8a840b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:39:02 minikube cri-dockerd[10278]: time="2025-03-09T18:39:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c0f8e78bd5fe4fbf0a6484e8d182ba9107c4753d8fef96bed10624d172057e1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:39:03 minikube dockerd[9949]: time="2025-03-09T18:39:03.087638635Z" level=info msg="ignoring event" container=f317d8288d3d7ad7a234109bb7db8a1159e8c76918b40defd45c858f49b4e040 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 09 18:39:03 minikube cri-dockerd[10278]: time="2025-03-09T18:39:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d0dffe75c7c7306cc08d6e9b2ef8516532f13d7e871f1c621a66042e71e18c5f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 09 18:39:06 minikube dockerd[9949]: time="2025-03-09T18:39:06.248307668Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 09 18:39:06 minikube dockerd[9949]: time="2025-03-09T18:39:06.248358595Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 09 18:39:09 minikube dockerd[9949]: time="2025-03-09T18:39:09.073862315Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 09 18:39:09 minikube dockerd[9949]: time="2025-03-09T18:39:09.073937418Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 09 18:41:40 minikube dockerd[9949]: time="2025-03-09T18:41:40.162952316Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 09 18:41:40 minikube dockerd[9949]: time="2025-03-09T18:41:40.163016547Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 09 18:41:54 minikube dockerd[9949]: time="2025-03-09T18:41:54.920436163Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 09 18:41:54 minikube dockerd[9949]: time="2025-03-09T18:41:54.920502691Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 09 18:41:57 minikube dockerd[9949]: time="2025-03-09T18:41:57.521978638Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 09 18:41:57 minikube dockerd[9949]: time="2025-03-09T18:41:57.522061968Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
6be363f11753f       6e38f40d628db       5 minutes ago       Running             storage-provisioner       3                   40a64e2299605       storage-provisioner
418148587a1dd       040f9f8aac8cd       5 minutes ago       Running             kube-proxy                1                   05783b5b596f3       kube-proxy-gq9bg
d8b0758d69a12       c69fa2e9cbf5f       5 minutes ago       Running             coredns                   1                   344264b05fa4a       coredns-668d6bf9bc-8j7v6
ecb0e6bb487e2       a389e107f4ff1       5 minutes ago       Running             kube-scheduler            1                   00af245bd71ea       kube-scheduler-minikube
ef75476dad549       a9e7e6b294baf       5 minutes ago       Running             etcd                      1                   1cff4a4408c28       etcd-minikube
b29a5db56671d       6e38f40d628db       5 minutes ago       Exited              storage-provisioner       2                   40a64e2299605       storage-provisioner
f612e2be7efba       c2e17b8d0f4a3       5 minutes ago       Running             kube-apiserver            1                   828f7be632ef8       kube-apiserver-minikube
fd1f4078f62ae       8cab3d2a8bd0f       5 minutes ago       Running             kube-controller-manager   1                   85795c80787ee       kube-controller-manager-minikube
4a7d9213e997e       c69fa2e9cbf5f       17 minutes ago      Exited              coredns                   0                   b64743ce923ba       coredns-668d6bf9bc-8j7v6
ddffff171d068       040f9f8aac8cd       17 minutes ago      Exited              kube-proxy                0                   5a4b98bff06bd       kube-proxy-gq9bg
4b05a66983781       a389e107f4ff1       17 minutes ago      Exited              kube-scheduler            0                   36d362652b762       kube-scheduler-minikube
99d7ce1513d40       8cab3d2a8bd0f       17 minutes ago      Exited              kube-controller-manager   0                   2e8c25abb6602       kube-controller-manager-minikube
ae29cc61e803f       a9e7e6b294baf       17 minutes ago      Exited              etcd                      0                   fe32afc07bdbb       etcd-minikube
6655acae5b7e6       c2e17b8d0f4a3       17 minutes ago      Exited              kube-apiserver            0                   f256c15869ff7       kube-apiserver-minikube


==> coredns [4a7d9213e997] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:34184 - 47601 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 6.002897588s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:55291->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:53152 - 4566 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 6.003479834s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:54393->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:57328 - 6378 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 4.001483932s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:60583->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:43066 - 14232 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 2.000671406s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:35680->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:59198 - 19640 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 2.001047852s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:55436->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:33982 - 59719 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 2.001233899s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:44860->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:34785 - 34043 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 2.001307894s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:54010->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:33441 - 61348 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 2.000635888s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:53941->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:50394 - 43259 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 2.000654006s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:51750->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:58846 - 2916 "HINFO IN 7562962014978153426.6866418612702974046. udp 57 false 512" - - 0 2.001199114s
[ERROR] plugin/errors: 2 7562962014978153426.6866418612702974046. HINFO: read udp 10.244.0.2:45239->192.168.65.254:53: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [d8b0758d69a1] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:51218 - 51728 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 6.002312006s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:41354->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:42560 - 2268 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 6.002099074s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:59629->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:37069 - 18545 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 4.002278002s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:53199->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:60211 - 20574 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 2.000296149s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:46325->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:36121 - 7629 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 2.000406085s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:57525->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:44432 - 35234 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 2.000912298s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:38963->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:38739 - 17828 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 2.000457345s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:51386->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:48778 - 6485 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 2.000656137s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:49028->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:47123 - 33892 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 2.000432966s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:60508->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:37192 - 1960 "HINFO IN 4559337838526844333.2414035830909901263. udp 57 false 512" - - 0 2.00033915s
[ERROR] plugin/errors: 2 4559337838526844333.2414035830909901263. HINFO: read udp 10.244.0.23:57011->192.168.65.254:53: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_09T21_26_03_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 09 Mar 2025 18:26:00 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 09 Mar 2025 18:43:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 09 Mar 2025 18:38:14 +0000   Sun, 09 Mar 2025 18:25:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 09 Mar 2025 18:38:14 +0000   Sun, 09 Mar 2025 18:25:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 09 Mar 2025 18:38:14 +0000   Sun, 09 Mar 2025 18:25:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 09 Mar 2025 18:38:14 +0000   Sun, 09 Mar 2025 18:26:00 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16079184Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16079184Ki
  pods:               110
System Info:
  Machine ID:                 9d7fb6d4061d46c0a599e7a700a221f6
  System UUID:                9d7fb6d4061d46c0a599e7a700a221f6
  Boot ID:                    48bb61b5-1b49-497e-96e0-6ab5de914011
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     app-python-5d55899f8d-2fx58         0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m31s
  default                     app-python-5d55899f8d-gzpvj         0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m31s
  default                     app-python-5d55899f8d-hb9dg         0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m31s
  kube-system                 coredns-668d6bf9bc-8j7v6            100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     17m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         17m
  kube-system                 kube-apiserver-minikube             250m (1%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-proxy-gq9bg                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           17m                kube-proxy       
  Normal   Starting                           5m2s               kube-proxy       
  Normal   NodeAllocatableEnforced            17m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            17m (x8 over 17m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              17m (x8 over 17m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               17m (x7 over 17m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  17m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           17m                kubelet          Starting kubelet.
  Warning  CgroupV1                           17m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            17m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            17m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              17m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               17m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     17m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     5m7s               node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.349194] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.214137] FS-Cache: Duplicate cookie detected
[  +0.000475] FS-Cache: O-cookie c=00000022 [p=00000002 fl=222 nc=0 na=1]
[  +0.000685] FS-Cache: O-cookie d=00000000d04907ac{9P.session} n=0000000067506482
[  +0.000745] FS-Cache: O-key=[10] '34323934393337383430'
[  +0.000655] FS-Cache: N-cookie c=00000023 [p=00000002 fl=2 nc=0 na=1]
[  +0.000565] FS-Cache: N-cookie d=00000000d04907ac{9P.session} n=00000000399f582f
[  +0.000663] FS-Cache: N-key=[10] '34323934393337383430'
[  +0.007778] FS-Cache: Duplicate cookie detected
[  +0.000531] FS-Cache: O-cookie c=00000024 [p=00000002 fl=222 nc=0 na=1]
[  +0.000411] FS-Cache: O-cookie d=00000000d04907ac{9P.session} n=000000000481031b
[  +0.000588] FS-Cache: O-key=[10] '34323934393337383431'
[  +0.000444] FS-Cache: N-cookie c=00000025 [p=00000002 fl=2 nc=0 na=1]
[  +0.000576] FS-Cache: N-cookie d=00000000d04907ac{9P.session} n=00000000807a55ca
[  +0.000512] FS-Cache: N-key=[10] '34323934393337383431'
[  +0.150668] WSL (1 - init(Ubuntu)) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.014182] FS-Cache: Duplicate cookie detected
[  +0.000632] FS-Cache: O-cookie c=0000002b [p=00000002 fl=222 nc=0 na=1]
[  +0.000603] FS-Cache: O-cookie d=00000000d04907ac{9P.session} n=00000000cc4230da
[  +0.000565] FS-Cache: O-key=[10] '34323934393337383538'
[  +0.000579] FS-Cache: N-cookie c=0000002c [p=00000002 fl=2 nc=0 na=1]
[  +0.000605] FS-Cache: N-cookie d=00000000d04907ac{9P.session} n=0000000067627bc8
[  +0.000697] FS-Cache: N-key=[10] '34323934393337383538'
[  +0.001286] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.021262] WSL (1 - init(Ubuntu)) WARNING: /etc/resolv.conf updating disabled in /etc/wsl.conf
[  +0.072200] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000431] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000598] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000942] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000407] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000853] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000980] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001379] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004298] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000116] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001329] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000746] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000624] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001020] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001024] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002217] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.681083] netlink: 'init': attribute type 4 has an invalid length.
[  +1.051277] WSL (210) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.123547] WSL (210) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.084582] WSL (210) ERROR: CheckConnection: connect() failed: 101
[  +6.014177] CPU: 7 PID: 1815 Comm: exe Not tainted 5.15.167.4-microsoft-standard-WSL2 #1
[  +0.000958] RIP: 0033:0x7f08c1716a50
[  +0.000412] Code: Unable to access opcode bytes at RIP 0x7f08c1716a26.
[  +0.000613] RSP: 002b:00007ffd994e6bb0 EFLAGS: 00000200 ORIG_RAX: 000000000000003b
[  +0.000542] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000
[  +0.000740] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
[  +0.000584] RBP: 0000000000000000 R08: 0000000000000000 R09: 0000000000000000
[  +0.000603] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000
[  +0.000694] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
[  +0.000559] FS:  0000000000000000 GS:  0000000000000000
[  +1.646997] tmpfs: Unknown parameter 'noswap'
[  +3.569936] tmpfs: Unknown parameter 'noswap'
[Mar 9 18:25] tmpfs: Unknown parameter 'noswap'
[Mar 9 18:26] tmpfs: Unknown parameter 'noswap'
[Mar 9 18:36] hrtimer: interrupt took 11926702 ns


==> etcd [ae29cc61e803] <==
{"level":"info","ts":"2025-03-09T18:25:57.636995Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-09T18:25:57.637020Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-09T18:25:57.656653Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-03-09T18:25:57.707679Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-03-09T18:25:57.715023Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"5.16079ms"}
{"level":"info","ts":"2025-03-09T18:25:57.724950Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-03-09T18:25:57.725085Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-03-09T18:25:57.725121Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-03-09T18:25:57.725129Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-03-09T18:25:57.725140Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-03-09T18:25:57.725171Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-03-09T18:25:57.732994Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-03-09T18:25:57.743723Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-03-09T18:25:57.746330Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-03-09T18:25:57.749772Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-03-09T18:25:57.750204Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-03-09T18:25:57.750527Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-09T18:25:57.750622Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-09T18:25:57.750633Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-09T18:25:57.750773Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-09T18:25:57.753797Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-09T18:25:57.754037Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-09T18:25:57.754102Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-09T18:25:57.754467Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-03-09T18:25:57.754530Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-03-09T18:25:57.755195Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-03-09T18:25:57.755279Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-09T18:25:58.226288Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-03-09T18:25:58.226360Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-03-09T18:25:58.226378Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-03-09T18:25:58.226389Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-03-09T18:25:58.226533Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-09T18:25:58.226552Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-03-09T18:25:58.226565Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-09T18:25:58.227818Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-09T18:25:58.228926Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-03-09T18:25:58.228987Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-09T18:25:58.229085Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-09T18:25:58.229369Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-03-09T18:25:58.229415Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-03-09T18:25:58.229891Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-09T18:25:58.230017Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-09T18:25:58.230013Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-09T18:25:58.230057Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-09T18:25:58.230750Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-03-09T18:25:58.232768Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-09T18:25:58.234146Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-03-09T18:35:58.223844Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":740}
{"level":"info","ts":"2025-03-09T18:35:58.229861Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":740,"took":"5.719077ms","hash":4025414743,"current-db-size-bytes":1871872,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1871872,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-03-09T18:35:58.229923Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4025414743,"revision":740,"compact-revision":-1}
{"level":"info","ts":"2025-03-09T18:37:41.678684Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-03-09T18:37:41.678893Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-03-09T18:37:41.679041Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-09T18:37:41.679147Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-09T18:37:41.783055Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-09T18:37:41.783121Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-03-09T18:37:41.783250Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-03-09T18:37:41.794911Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-09T18:37:41.795111Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-09T18:37:41.795158Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [ef75476dad54] <==
{"level":"warn","ts":"2025-03-09T18:38:07.485900Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-09T18:38:07.486024Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2025-03-09T18:38:07.486109Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2025-03-09T18:38:07.486155Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-09T18:38:07.486172Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-09T18:38:07.486212Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-09T18:38:07.505966Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-03-09T18:38:07.546264Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-03-09T18:38:07.575272Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"28.70652ms"}
{"level":"info","ts":"2025-03-09T18:38:07.579929Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-03-09T18:38:07.586465Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":1420}
{"level":"info","ts":"2025-03-09T18:38:07.586636Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-03-09T18:38:07.586681Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-03-09T18:38:07.586695Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 1420, applied: 0, lastindex: 1420, lastterm: 2]"}
{"level":"warn","ts":"2025-03-09T18:38:07.588238Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-03-09T18:38:07.589600Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":740}
{"level":"info","ts":"2025-03-09T18:38:07.591908Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1257}
{"level":"info","ts":"2025-03-09T18:38:07.593477Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-03-09T18:38:07.595228Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-03-09T18:38:07.595541Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-03-09T18:38:07.595585Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-03-09T18:38:07.595746Z","caller":"etcdserver/server.go:773","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-03-09T18:38:07.595898Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-09T18:38:07.596050Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-03-09T18:38:07.596066Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-09T18:38:07.596091Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-09T18:38:07.596121Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-09T18:38:07.596238Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-09T18:38:07.596302Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-09T18:38:07.596405Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-09T18:38:07.598874Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-09T18:38:07.598993Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-09T18:38:07.599009Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-09T18:38:07.617307Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-03-09T18:38:07.617420Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-03-09T18:38:09.387654Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-03-09T18:38:09.387711Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-03-09T18:38:09.387732Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-09T18:38:09.387742Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-03-09T18:38:09.387746Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-03-09T18:38:09.387752Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-03-09T18:38:09.387758Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-03-09T18:38:09.389432Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-03-09T18:38:09.389432Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-09T18:38:09.389489Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-09T18:38:09.389684Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-03-09T18:38:09.389732Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-03-09T18:38:09.390172Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-09T18:38:09.391925Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-03-09T18:38:09.392015Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-09T18:38:09.392668Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}


==> kernel <==
 18:43:21 up  1:45,  0 users,  load average: 0.81, 1.67, 1.71
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [6655acae5b7e] <==
W0309 18:37:47.249185       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:47.279006       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:47.300516       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:47.327651       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:47.337074       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:47.338629       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:47.362531       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:47.405070       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:47.409635       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:49.675051       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:49.917901       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:49.933827       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.045119       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.181486       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.275413       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.280995       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.337477       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.360562       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.376214       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.420771       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.429267       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.450840       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.470354       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.475935       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.516305       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.539894       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.544523       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.557308       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.599305       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.713839       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.727431       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.746992       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.749558       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.790028       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.819958       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.838926       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.848920       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.869934       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.923498       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.964809       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.973187       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.973191       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:50.993615       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.043868       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.054768       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.059335       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.165781       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.165798       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.243608       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.262098       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.288057       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.326534       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.356139       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.370640       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.421162       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.424991       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.521693       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.560937       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.682100       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0309 18:37:51.690785       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [f612e2be7efb] <==
W0309 18:38:10.356367       1 genericapiserver.go:767] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0309 18:38:10.809594       1 secure_serving.go:213] Serving securely on [::]:8443
I0309 18:38:10.809944       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0309 18:38:10.810117       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0309 18:38:10.873584       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0309 18:38:10.873640       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0309 18:38:10.873651       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0309 18:38:10.873666       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0309 18:38:10.873591       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0309 18:38:10.873755       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0309 18:38:10.873826       1 controller.go:78] Starting OpenAPI AggregationController
I0309 18:38:10.873853       1 controller.go:119] Starting legacy_token_tracking_controller
I0309 18:38:10.873865       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0309 18:38:10.873890       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0309 18:38:10.873986       1 aggregator.go:169] waiting for initial CRD sync...
I0309 18:38:10.874018       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0309 18:38:10.874026       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0309 18:38:10.874095       1 local_available_controller.go:156] Starting LocalAvailability controller
I0309 18:38:10.874105       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0309 18:38:10.874104       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0309 18:38:10.873616       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0309 18:38:10.874165       1 controller.go:142] Starting OpenAPI controller
I0309 18:38:10.874193       1 controller.go:90] Starting OpenAPI V3 controller
I0309 18:38:10.874217       1 naming_controller.go:294] Starting NamingConditionController
I0309 18:38:10.874236       1 establishing_controller.go:81] Starting EstablishingController
I0309 18:38:10.874255       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0309 18:38:10.874265       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0309 18:38:10.874278       1 crd_finalizer.go:269] Starting CRDFinalizer
I0309 18:38:10.883742       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0309 18:38:10.883846       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0309 18:38:10.884689       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0309 18:38:10.913789       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0309 18:38:10.913851       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0309 18:38:10.913882       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0309 18:38:10.913966       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0309 18:38:10.974551       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0309 18:38:10.974603       1 aggregator.go:171] initial CRD sync complete...
I0309 18:38:10.974614       1 autoregister_controller.go:144] Starting autoregister controller
I0309 18:38:10.974623       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0309 18:38:10.983974       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0309 18:38:11.073553       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0309 18:38:10.993872       1 shared_informer.go:320] Caches are synced for node_authorizer
I0309 18:38:11.073550       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0309 18:38:11.073617       1 policy_source.go:240] refreshing policies
I0309 18:38:11.073773       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0309 18:38:11.073899       1 shared_informer.go:320] Caches are synced for configmaps
I0309 18:38:11.074103       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0309 18:38:11.074138       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0309 18:38:11.074784       1 cache.go:39] Caches are synced for autoregister controller
I0309 18:38:11.074915       1 cache.go:39] Caches are synced for LocalAvailability controller
I0309 18:38:11.079706       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0309 18:38:11.081420       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E0309 18:38:11.088312       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0309 18:38:11.879046       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0309 18:38:11.891701       1 controller.go:615] quota admission added evaluator for: endpoints
I0309 18:38:13.007044       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0309 18:38:14.236457       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0309 18:38:14.485434       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0309 18:38:14.635485       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0309 18:38:14.735137       1 controller.go:615] quota admission added evaluator for: deployments.apps


==> kube-controller-manager [99d7ce1513d4] <==
I0309 18:26:06.680946       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0309 18:26:06.689148       1 shared_informer.go:320] Caches are synced for garbage collector
I0309 18:26:07.818362       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="193.667822ms"
I0309 18:26:07.836965       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="18.445588ms"
I0309 18:26:07.837127       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="84.451¬µs"
I0309 18:26:07.907980       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="76.826¬µs"
I0309 18:26:11.134384       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="63.36¬µs"
I0309 18:26:12.162117       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="7.864306ms"
I0309 18:26:12.162218       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="30.849¬µs"
I0309 18:26:13.442616       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0309 18:27:52.970451       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="16.370028ms"
I0309 18:27:52.975503       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="4.96453ms"
I0309 18:27:52.975715       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="134.537¬µs"
I0309 18:27:52.978716       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="34.286¬µs"
I0309 18:27:57.715701       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="48.213¬µs"
I0309 18:27:59.858960       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="55.005¬µs"
I0309 18:28:30.286557       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="155.857¬µs"
I0309 18:28:45.285608       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="76.947¬µs"
I0309 18:28:49.539327       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7f488d9f5" duration="7.254¬µs"
I0309 18:28:51.678948       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="14.662443ms"
I0309 18:28:51.685258       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="6.161632ms"
I0309 18:28:51.685361       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="31.09¬µs"
I0309 18:28:51.691271       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="38.433¬µs"
I0309 18:28:56.294416       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="46.268¬µs"
I0309 18:29:11.286225       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="52.049¬µs"
I0309 18:29:29.280871       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="100.642¬µs"
I0309 18:29:41.281598       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="158.673¬µs"
I0309 18:29:55.284651       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="94.55¬µs"
I0309 18:30:09.280291       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="44.886¬µs"
I0309 18:30:52.278614       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="56.448¬µs"
I0309 18:31:05.278839       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="94.229¬µs"
I0309 18:32:15.274669       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="58.963¬µs"
I0309 18:32:26.273094       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="40.066¬µs"
I0309 18:32:51.782016       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0309 18:33:18.153888       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-7d7db9cdcc" duration="9.648¬µs"
I0309 18:35:50.452583       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="30.693892ms"
I0309 18:35:50.485779       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="33.11382ms"
I0309 18:35:50.485902       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="52.67¬µs"
I0309 18:35:50.495951       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="52.651¬µs"
I0309 18:35:50.505068       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="39.044¬µs"
I0309 18:35:56.578130       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="51.539¬µs"
I0309 18:35:58.601847       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="72.238¬µs"
I0309 18:35:59.692763       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="63.451¬µs"
I0309 18:36:01.718135       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="63.882¬µs"
I0309 18:36:02.726077       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="44.385¬µs"
I0309 18:36:06.025133       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="53.623¬µs"
I0309 18:36:13.202734       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="69.753¬µs"
I0309 18:36:15.283804       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="46.428¬µs"
I0309 18:36:18.303727       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="57.97¬µs"
I0309 18:36:20.328429       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="72.989¬µs"
I0309 18:36:20.340056       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="68.741¬µs"
I0309 18:36:25.557636       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="59.383¬µs"
I0309 18:36:31.263492       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="59.704¬µs"
I0309 18:36:57.265596       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="76.797¬µs"
I0309 18:37:00.262045       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="39.225¬µs"
I0309 18:37:00.272501       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="44.896¬µs"
I0309 18:37:11.263299       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="60.545¬µs"
I0309 18:37:11.276118       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="126.802¬µs"
I0309 18:37:13.265596       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="48.463¬µs"
I0309 18:37:41.262377       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="79.641¬µs"


==> kube-controller-manager [fd1f4078f62a] <==
I0309 18:38:14.181943       1 shared_informer.go:320] Caches are synced for expand
I0309 18:38:14.183489       1 shared_informer.go:320] Caches are synced for TTL after finished
I0309 18:38:14.186226       1 shared_informer.go:320] Caches are synced for namespace
I0309 18:38:14.189081       1 shared_informer.go:320] Caches are synced for ephemeral
I0309 18:38:14.189145       1 shared_informer.go:320] Caches are synced for HPA
I0309 18:38:14.192378       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0309 18:38:14.192430       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0309 18:38:14.192483       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0309 18:38:14.192486       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0309 18:38:14.195873       1 shared_informer.go:320] Caches are synced for stateful set
I0309 18:38:14.198237       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0309 18:38:14.199552       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0309 18:38:14.201841       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0309 18:38:14.204154       1 shared_informer.go:320] Caches are synced for daemon sets
I0309 18:38:14.207281       1 shared_informer.go:320] Caches are synced for ReplicationController
I0309 18:38:14.209543       1 shared_informer.go:320] Caches are synced for cronjob
I0309 18:38:14.211962       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0309 18:38:14.214444       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0309 18:38:14.217762       1 shared_informer.go:320] Caches are synced for node
I0309 18:38:14.217817       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0309 18:38:14.217835       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0309 18:38:14.217839       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0309 18:38:14.217843       1 shared_informer.go:320] Caches are synced for cidrallocator
I0309 18:38:14.217929       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0309 18:38:14.218060       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0309 18:38:14.222241       1 shared_informer.go:320] Caches are synced for attach detach
I0309 18:38:14.230969       1 shared_informer.go:320] Caches are synced for garbage collector
I0309 18:38:14.231012       1 shared_informer.go:320] Caches are synced for PV protection
I0309 18:38:14.231024       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0309 18:38:14.231038       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0309 18:38:14.230984       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0309 18:38:14.231311       1 shared_informer.go:320] Caches are synced for persistent volume
I0309 18:38:14.231393       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0309 18:38:14.231664       1 shared_informer.go:320] Caches are synced for endpoint
I0309 18:38:14.231674       1 shared_informer.go:320] Caches are synced for GC
I0309 18:38:14.231757       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="215.792¬µs"
I0309 18:38:14.234935       1 shared_informer.go:320] Caches are synced for resource quota
I0309 18:38:14.239240       1 shared_informer.go:320] Caches are synced for resource quota
I0309 18:38:14.252043       1 shared_informer.go:320] Caches are synced for garbage collector
I0309 18:38:14.521617       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0309 18:38:14.641983       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="410.285418ms"
I0309 18:38:14.642080       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="41.68¬µs"
I0309 18:38:18.938477       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="43.584¬µs"
I0309 18:38:20.820631       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="8.157972ms"
I0309 18:38:20.820752       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="47.47¬µs"
I0309 18:38:23.037265       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="66.437¬µs"
I0309 18:38:53.810625       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="39.636¬µs"
I0309 18:38:57.913969       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="45.016¬µs"
I0309 18:39:05.110626       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="37.14¬µs"
I0309 18:39:07.197615       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="55.516¬µs"
I0309 18:39:09.210633       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="31.099¬µs"
I0309 18:39:20.254370       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="33.584¬µs"
I0309 18:39:20.262571       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="55.967¬µs"
I0309 18:41:52.248971       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="89.361¬µs"
I0309 18:42:03.249292       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="93.348¬µs"
I0309 18:42:06.246866       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="40.307¬µs"
I0309 18:42:11.247051       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="34.667¬µs"
I0309 18:42:17.247991       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="41.239¬µs"
I0309 18:42:26.245049       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/app-python-5d55899f8d" duration="66.087¬µs"
I0309 18:43:21.535651       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [418148587a1d] <==
I0309 18:38:17.582350       1 server_linux.go:66] "Using iptables proxy"
I0309 18:38:17.863426       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0309 18:38:17.863509       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0309 18:38:18.500846       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0309 18:38:18.500927       1 server_linux.go:170] "Using iptables Proxier"
I0309 18:38:18.541319       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0309 18:38:18.568857       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0309 18:38:18.650553       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0309 18:38:18.650648       1 server.go:497] "Version info" version="v1.32.0"
I0309 18:38:18.650667       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0309 18:38:18.730699       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0309 18:38:18.749585       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0309 18:38:18.752096       1 config.go:105] "Starting endpoint slice config controller"
I0309 18:38:18.752153       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0309 18:38:18.752165       1 config.go:329] "Starting node config controller"
I0309 18:38:18.752179       1 shared_informer.go:313] Waiting for caches to sync for node config
I0309 18:38:18.752207       1 config.go:199] "Starting service config controller"
I0309 18:38:18.752219       1 shared_informer.go:313] Waiting for caches to sync for service config
I0309 18:38:18.853146       1 shared_informer.go:320] Caches are synced for service config
I0309 18:38:18.853186       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0309 18:38:18.853198       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [ddffff171d06] <==
I0309 18:26:08.231106       1 server_linux.go:66] "Using iptables proxy"
I0309 18:26:08.543723       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0309 18:26:08.543817       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0309 18:26:08.686846       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0309 18:26:08.686938       1 server_linux.go:170] "Using iptables Proxier"
I0309 18:26:08.727329       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0309 18:26:08.754392       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0309 18:26:08.834106       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0309 18:26:08.834230       1 server.go:497] "Version info" version="v1.32.0"
I0309 18:26:08.834265       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0309 18:26:08.938595       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0309 18:26:08.955422       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0309 18:26:08.956813       1 config.go:105] "Starting endpoint slice config controller"
I0309 18:26:08.956862       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0309 18:26:08.956872       1 config.go:329] "Starting node config controller"
I0309 18:26:08.956879       1 shared_informer.go:313] Waiting for caches to sync for node config
I0309 18:26:08.956862       1 config.go:199] "Starting service config controller"
I0309 18:26:08.956921       1 shared_informer.go:313] Waiting for caches to sync for service config
I0309 18:26:09.057567       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0309 18:26:09.057681       1 shared_informer.go:320] Caches are synced for service config
I0309 18:26:09.057699       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [4b05a6698378] <==
I0309 18:25:58.209146       1 serving.go:386] Generated self-signed cert in-memory
W0309 18:26:00.113955       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0309 18:26:00.114263       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0309 18:26:00.114539       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0309 18:26:00.114820       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0309 18:26:00.223013       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0309 18:26:00.223117       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0309 18:26:00.226098       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0309 18:26:00.226192       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0309 18:26:00.226327       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0309 18:26:00.226433       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0309 18:26:00.228188       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0309 18:26:00.228374       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0309 18:26:00.228663       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0309 18:26:00.228742       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.230671       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0309 18:26:00.230759       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.231280       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0309 18:26:00.231376       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.313312       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0309 18:26:00.313355       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0309 18:26:00.313391       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0309 18:26:00.313399       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0309 18:26:00.313401       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.313410       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0309 18:26:00.313312       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0309 18:26:00.313446       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0309 18:26:00.313447       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.313458       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
W0309 18:26:00.313312       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0309 18:26:00.313420       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.313475       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0309 18:26:00.313482       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0309 18:26:00.313486       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0309 18:26:00.313495       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.313486       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0309 18:26:00.313574       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.313359       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0309 18:26:00.313631       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.313364       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0309 18:26:00.313681       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0309 18:26:00.313539       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0309 18:26:00.313731       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0309 18:26:01.190102       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0309 18:26:01.190156       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0309 18:26:01.193899       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0309 18:26:01.193957       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0309 18:26:01.236503       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0309 18:26:01.236566       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0309 18:26:01.241859       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0309 18:26:01.241954       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0309 18:26:01.290716       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0309 18:26:01.290788       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0309 18:26:01.784968       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0309 18:26:01.785039       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0309 18:26:03.727339       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0309 18:37:41.685217       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0309 18:37:41.685409       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0309 18:37:41.685820       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0309 18:37:41.686323       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [ecb0e6bb487e] <==
I0309 18:38:07.690579       1 serving.go:386] Generated self-signed cert in-memory
W0309 18:38:10.925145       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0309 18:38:10.925179       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0309 18:38:10.925188       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0309 18:38:10.925194       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0309 18:38:11.086581       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0309 18:38:11.086870       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0309 18:38:11.091689       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0309 18:38:11.091774       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0309 18:38:11.092101       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0309 18:38:11.092194       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0309 18:38:11.192921       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Mar 09 18:39:44 minikube kubelet[2598]: E0309 18:39:44.244370    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:39:47 minikube kubelet[2598]: E0309 18:39:47.243995    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:39:49 minikube kubelet[2598]: E0309 18:39:49.244604    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:39:56 minikube kubelet[2598]: E0309 18:39:56.244522    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:40:00 minikube kubelet[2598]: E0309 18:40:00.244382    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:40:02 minikube kubelet[2598]: E0309 18:40:02.243859    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:40:07 minikube kubelet[2598]: E0309 18:40:07.243461    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:40:13 minikube kubelet[2598]: E0309 18:40:13.242745    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:40:14 minikube kubelet[2598]: E0309 18:40:14.242573    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:40:20 minikube kubelet[2598]: E0309 18:40:20.242991    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:40:26 minikube kubelet[2598]: E0309 18:40:26.242470    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:40:28 minikube kubelet[2598]: E0309 18:40:28.242559    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:40:33 minikube kubelet[2598]: E0309 18:40:33.242894    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:40:39 minikube kubelet[2598]: E0309 18:40:39.242302    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:40:43 minikube kubelet[2598]: E0309 18:40:43.247260    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:40:48 minikube kubelet[2598]: E0309 18:40:48.241585    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:40:51 minikube kubelet[2598]: E0309 18:40:51.241794    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:40:54 minikube kubelet[2598]: E0309 18:40:54.241953    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:41:01 minikube kubelet[2598]: E0309 18:41:01.241592    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:41:05 minikube kubelet[2598]: E0309 18:41:05.241559    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:41:08 minikube kubelet[2598]: E0309 18:41:08.240461    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:41:16 minikube kubelet[2598]: E0309 18:41:16.240173    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:41:16 minikube kubelet[2598]: E0309 18:41:16.240266    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:41:22 minikube kubelet[2598]: E0309 18:41:22.240005    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:41:27 minikube kubelet[2598]: E0309 18:41:27.240268    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:41:28 minikube kubelet[2598]: E0309 18:41:28.240605    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:41:39 minikube kubelet[2598]: E0309 18:41:39.238174    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:41:40 minikube kubelet[2598]: E0309 18:41:40.166501    2598 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-python:latest"
Mar 09 18:41:40 minikube kubelet[2598]: E0309 18:41:40.166587    2598 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-python:latest"
Mar 09 18:41:40 minikube kubelet[2598]: E0309 18:41:40.166692    2598 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:app-python,Image:app-python:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6f2xc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod app-python-5d55899f8d-gzpvj_default(26cfaccf-d1e5-4134-8cd4-00874ab9e32e): ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Mar 09 18:41:40 minikube kubelet[2598]: E0309 18:41:40.167964    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ErrImagePull: \"Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:41:41 minikube kubelet[2598]: E0309 18:41:41.238863    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:41:52 minikube kubelet[2598]: E0309 18:41:52.238630    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:41:54 minikube kubelet[2598]: E0309 18:41:54.923922    2598 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-python:latest"
Mar 09 18:41:54 minikube kubelet[2598]: E0309 18:41:54.924005    2598 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-python:latest"
Mar 09 18:41:54 minikube kubelet[2598]: E0309 18:41:54.924232    2598 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:app-python,Image:app-python:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95npp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod app-python-5d55899f8d-2fx58_default(75a39efb-3249-47c6-9cd8-9f7c18735f2c): ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Mar 09 18:41:54 minikube kubelet[2598]: E0309 18:41:54.926067    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ErrImagePull: \"Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:41:57 minikube kubelet[2598]: E0309 18:41:57.526599    2598 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-python:latest"
Mar 09 18:41:57 minikube kubelet[2598]: E0309 18:41:57.526673    2598 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="app-python:latest"
Mar 09 18:41:57 minikube kubelet[2598]: E0309 18:41:57.526831    2598 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:app-python,Image:app-python:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vt5mb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod app-python-5d55899f8d-hb9dg_default(112c478c-0126-4a6e-a6bb-7482a91bdcfd): ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Mar 09 18:41:57 minikube kubelet[2598]: E0309 18:41:57.528137    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ErrImagePull: \"Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:42:03 minikube kubelet[2598]: E0309 18:42:03.239458    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:42:06 minikube kubelet[2598]: E0309 18:42:06.237581    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:42:11 minikube kubelet[2598]: E0309 18:42:11.237295    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:42:15 minikube kubelet[2598]: E0309 18:42:15.237733    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:42:17 minikube kubelet[2598]: E0309 18:42:17.237837    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:42:26 minikube kubelet[2598]: E0309 18:42:26.237020    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:42:26 minikube kubelet[2598]: E0309 18:42:26.237039    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:42:31 minikube kubelet[2598]: E0309 18:42:31.237712    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:42:37 minikube kubelet[2598]: E0309 18:42:37.235941    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:42:38 minikube kubelet[2598]: E0309 18:42:38.236569    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:42:43 minikube kubelet[2598]: E0309 18:42:43.241556    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:42:49 minikube kubelet[2598]: E0309 18:42:49.235658    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:42:49 minikube kubelet[2598]: E0309 18:42:49.235765    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:42:57 minikube kubelet[2598]: E0309 18:42:57.236365    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:43:00 minikube kubelet[2598]: E0309 18:43:00.236319    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:43:03 minikube kubelet[2598]: E0309 18:43:03.235874    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"
Mar 09 18:43:12 minikube kubelet[2598]: E0309 18:43:12.234941    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-2fx58" podUID="75a39efb-3249-47c6-9cd8-9f7c18735f2c"
Mar 09 18:43:14 minikube kubelet[2598]: E0309 18:43:14.235278    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-gzpvj" podUID="26cfaccf-d1e5-4134-8cd4-00874ab9e32e"
Mar 09 18:43:16 minikube kubelet[2598]: E0309 18:43:16.235144    2598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"app-python\" with ImagePullBackOff: \"Back-off pulling image \\\"app-python:latest\\\": ErrImagePull: Error response from daemon: pull access denied for app-python, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/app-python-5d55899f8d-hb9dg" podUID="112c478c-0126-4a6e-a6bb-7482a91bdcfd"


==> storage-provisioner [6be363f11753] <==
I0309 18:38:20.383141       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0309 18:38:20.390016       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0309 18:38:20.390119       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0309 18:38:37.788685       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0309 18:38:37.788793       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"dcca967c-b844-4678-bf5a-561608fb133d", APIVersion:"v1", ResourceVersion:"1473", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_3407107b-832c-4a2b-a20e-0d2d9e96d70d became leader
I0309 18:38:37.788841       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_3407107b-832c-4a2b-a20e-0d2d9e96d70d!
I0309 18:38:37.888975       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_3407107b-832c-4a2b-a20e-0d2d9e96d70d!


==> storage-provisioner [b29a5db56671] <==
I0309 18:38:07.082542       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0309 18:38:07.088458       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

