# Lab 14: Kubernetes StatefulSet Implementation

## Task 1: StatefulSet Implementation in Helm Chart

### Understanding StatefulSets
StatefulSets are designed to manage stateful applications with guarantees about the ordering and uniqueness of Pods. Unlike Deployments, StatefulSets:
- Provide stable, unique network identifiers
- Stable, persistent storage
- Ordered, graceful deployment and scaling
- Ordered, automated rolling updates

The primary features that distinguish StatefulSets from Deployments are:

1. **Stable, Unique Network Identifiers**: Pods in a StatefulSet have persistent identifiers that are maintained upon rescheduling. The pattern follows: `$(statefulset name)-$(ordinal)`.

2. **Stable, Persistent Storage**: StatefulSet can use volumeClaimTemplates to provide persistent storage for each pod. When a pod is rescheduled, the same PVC is reattached.

3. **Ordered Deployment and Scaling**: StatefulSets provide guarantees about the ordering of pod creation, scaling, and deletion.

### Implementation
I've converted the existing Deployment to a StatefulSet by:
1. Renaming `deployment.yaml` to `statefulset.yaml`
2. Changing the Kind from `Deployment` to `StatefulSet`
3. Adding required StatefulSet-specific fields like `serviceName`
4. Implementing persistence with PVC templates
5. Updating the application to store state in the persistent volume mount at `/data`

## Task 2: StatefulSet Exploration and Optimization

### Command Outputs
```
# Output of kubectl get po,sts,svc,pvc will be added here after deployment
```

### Persistent Storage Validation
When a pod in a StatefulSet is deleted, the PVC associated with it persists. When the pod is recreated, it reattaches to the same PVC, ensuring data persistence.

```
# Results of pod deletion and verification will be added here
```

### Headless Service Access
A headless service (with clusterIP: None) allows direct DNS lookup for individual pods in the StatefulSet. This is essential for applications that need to communicate directly with specific pods.

```
# DNS resolution test results will be added here
```

### Monitoring & Alerts
I've implemented liveness and readiness probes in the StatefulSet to ensure pod health.

For stateful applications, health probes are critical because:
1. They ensure the application is ready to serve requests before traffic is directed to it
2. They detect when a pod becomes unhealthy and needs to be restarted
3. With stateful applications, proper health checking prevents data corruption or inconsistency
4. Unlike stateless applications, a malfunctioning stateful pod can't simply be terminated and replaced without considering data consistency

The probes I've implemented:
- **Liveness Probe**: Checks if the application is running. If it fails, Kubernetes restarts the pod.
- **Readiness Probe**: Checks if the application is ready to receive traffic. If it fails, the pod is removed from service endpoints.

### Ordering Guarantee and Parallel Operations
Our application doesn't strictly require ordering guarantees because:
- Each pod instance operates independently
- The application doesn't rely on inter-pod communication in a specific order
- There's no leader/follower relationship that requires sequential startup

I've implemented parallel pod operations by setting `.spec.podManagementPolicy: Parallel` in the StatefulSet definition to instruct the controller to launch or terminate pods in parallel. This is ideal for our application because:

1. It speeds up the deployment and scaling operations
2. Each pod operates independently with its own state
3. There's no dependency between pods that requires ordered startup

This optimization significantly improves the deployment and scaling time for our application, while still maintaining the unique identities and persistent storage benefits of StatefulSets.

## Differences Between Pods in the StatefulSet

StatefulSet pods maintain individual state. In our application, each pod maintains its own visit counter file at `/data/visits`. This means:

1. When you access a specific pod, it increments only its own counter
2. Due to the way Kubernetes services work, requests are load-balanced across pods
3. Each pod's counter reflects only the visits it has personally served

This behavior is different from a shared database where all pods would see the same counter value. In a production environment, we would typically use a database service for truly shared state, but this example demonstrates the per-pod persistence capability of StatefulSets. 